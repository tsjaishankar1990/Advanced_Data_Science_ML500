{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:00:44.299477609Z",
     "start_time": "2023-06-09T21:00:43.644624286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<!-- Many of the styles here are inspired by: \n    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n       \n    \n    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n    and at the request of participants, I have added it to this common import-file here.\n\n    -->\n\n<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n\n<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n<style>\n\n\n#ipython_notebook::before{\n content:\"Neural Architectures\";\n        color: white;\n        font-weight: bold;\n        text-transform: uppercase;\n        font-family: 'Lora',serif;\n        font-size:16pt;\n        margin-bottom:15px;\n        margin-top:15px;\n           \n}\nbody > #header {\n    #background: #D15555;\n    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n    opacity: 0.8;\n\n}\n\n\n.navbar-default .navbar-nav > li > a, #kernel_indicator {\n    color: white;\n    transition: all 0.25s;\n    font-size:10pt;\n    font-family: sans-serif;\n    font-weight:normal;\n}\n.navbar-default {\n    padding-left:100px;\n    background: none;\n    border: none;\n}\n\n\nbody > menubar-container {\n    background-color: wheat;\n}\n#ipython_notebook img{                                                                                        \n    display:block; \n    \n    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n    background-size: contain;\n   \n    padding-left: 600px;\n    padding-right: 100px;\n    \n    -moz-box-sizing: border-box;\n    box-sizing: border-box;\n}\n\n\n\nbody {\n #font-family:  'Literata', serif;\n    font-family:'Lora', san-serif;\n    text-align: justify;\n    font-weight: 400;\n    font-size: 12pt;\n}\n\niframe{\n    width:100%;\n    min-height:600px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n# font-family: 'Montserrat', sans-serif;\n font-family:'Lora', serif;\n font-weight: 200;\n text-transform: uppercase;\n color: #EC7063 ;\n}\n\nh2 {\n    color: #000080;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n#notebook_name {\n    font-weight: 600;\n    font-size:20pt;\n    text-variant:uppercase;\n    color: wheat; \n    margin-right:20px;\n    margin-left:-500px;\n}\n#notebook_name:hover {\nbackground-color: salmon;\n}\n\n\n.dataframe { /* dataframe atau table */\n    background: white;\n    box-shadow: 0px 1px 2px #bbb;\n}\n.dataframe thead th, .dataframe tbody td {\n    text-align: center;\n    padding: 1em;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n.output {\n    align-items: center;\n}\n\ndiv.cell {\n    transition: all 0.25s;\n    border: none;\n    position: relative;\n    top: 0;\n}\ndiv.cell.selected, div.cell.selected.jupyter-soft-selected {\n    border: none;\n    background: transparent;\n    box-shadow: 0 6px 18px #aaa;\n    z-index: 10;\n    top: -10px;\n}\n.CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n    font-family: 'Hack' , serif; \n    font-weight: 500;\n    font-size: 14pt;\n}\n\n\n\n</style>    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "\n\n<center><img src=\"https://d4x5p7s4.rocketcdn.me/wp-content/uploads/2016/03/logo-poster-smaller.png\"/> </center>\n<div style=\"color:#aaa;font-size:8pt\">\n<hr/>\n&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \nOnly the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n\n<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n </blockquote>\n <hr/>\n</div>\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with CIFAR-10\n",
    "\n",
    "We will start today with one of the known and famous datasets, the CIFAR: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "In particular, we will take `CIFAR-10`, which is a dataset with:\n",
    "\n",
    "* Only ten classes as the target variable value: i.e., the images are one of the these: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
    "* 60,000 color images of size 32x32 pixels\n",
    "* 50,000 of these images for the training set\n",
    "* 10,000, the remaining images, form the test set\n",
    "\n",
    "There many neural architectures we can use to classify images into the target classes. For example, we could use the simple Feed-forward network (fully connected dense network). Or we could use another neural architecture called **Convolutional** neural network (CNN). Or we could use transformers.\n",
    "\n",
    "In this particular case, we will use Convolutional neural networks. Since we are yet to study the actual theoretical details of convolutional networks in a subsequent session, for today:\n",
    "\n",
    "* we have provided you with a simple network, which you can feed into a classifier\n",
    "* and also provided a simple classifier that will run the training and evaluation loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:01:21.270264474Z",
     "start_time": "2023-06-09T21:01:12.773661957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device on which the computations will happen is: cuda. Hopefully CUDA?\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:05<00:00, 33150830.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/cifar-10-python.tar.gz to /tmp/data\n",
      "Files already downloaded and verified\n",
      "\n",
      "A Cifar10 Classifier, with the underlying neural-net as: \n",
      "SimpleCifarNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from svlearn.datasets.cifar10 import Cifar10Classifier, SimpleCifarNet\n",
    "    \n",
    "# Create the classifier, and show its neural-net\n",
    "classifier = Cifar10Classifier(SimpleCifarNet(), learning_rate=0.1)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:01:55.931098591Z",
     "start_time": "2023-06-09T21:01:24.481581078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 20.775\n",
      "[1,   200] loss: 2.321\n",
      "[1,   300] loss: 2.316\n",
      "[1,   400] loss: 2.317\n",
      "[1,   500] loss: 2.325\n",
      "[1,   600] loss: 2.322\n",
      "[1,   700] loss: 2.318\n",
      "[1,   800] loss: 2.325\n",
      "[1,   900] loss: 2.316\n",
      "[1,  1000] loss: 2.317\n",
      "[1,  1100] loss: 2.320\n",
      "[1,  1200] loss: 2.318\n",
      "[1,  1300] loss: 2.328\n",
      "[1,  1400] loss: 2.323\n",
      "[1,  1500] loss: 2.321\n",
      "[1,  1600] loss: 2.329\n",
      "[1,  1700] loss: 2.322\n",
      "[1,  1800] loss: 2.321\n",
      "[1,  1900] loss: 2.321\n",
      "[1,  2000] loss: 2.321\n",
      "[1,  2100] loss: 2.320\n",
      "[1,  2200] loss: 2.316\n",
      "[1,  2300] loss: 2.317\n",
      "[1,  2400] loss: 2.319\n",
      "[1,  2500] loss: 2.322\n",
      "[1,  2600] loss: 2.323\n",
      "[1,  2700] loss: 2.327\n",
      "[1,  2800] loss: 2.317\n",
      "[1,  2900] loss: 2.321\n",
      "[1,  3000] loss: 2.320\n",
      "[1,  3100] loss: 2.321\n",
      "[2,   100] loss: 2.320\n",
      "[2,   200] loss: 2.318\n",
      "[2,   300] loss: 2.326\n",
      "[2,   400] loss: 2.315\n",
      "[2,   500] loss: 2.311\n",
      "[2,   600] loss: 2.313\n",
      "[2,   700] loss: 2.320\n",
      "[2,   800] loss: 2.326\n",
      "[2,   900] loss: 2.323\n",
      "[2,  1000] loss: 2.322\n",
      "[2,  1100] loss: 2.324\n",
      "[2,  1200] loss: 2.317\n",
      "[2,  1300] loss: 2.319\n",
      "[2,  1400] loss: 2.324\n",
      "[2,  1500] loss: 2.325\n",
      "[2,  1600] loss: 2.315\n",
      "[2,  1700] loss: 2.326\n",
      "[2,  1800] loss: 2.321\n",
      "[2,  1900] loss: 2.318\n",
      "[2,  2000] loss: 2.320\n",
      "[2,  2100] loss: 2.325\n",
      "[2,  2200] loss: 2.325\n",
      "[2,  2300] loss: 2.321\n",
      "[2,  2400] loss: 2.317\n",
      "[2,  2500] loss: 2.319\n",
      "[2,  2600] loss: 2.321\n",
      "[2,  2700] loss: 2.322\n",
      "[2,  2800] loss: 2.322\n",
      "[2,  2900] loss: 2.329\n",
      "[2,  3000] loss: 2.323\n",
      "[2,  3100] loss: 2.319\n",
      "[3,   100] loss: 2.319\n",
      "[3,   200] loss: 2.320\n",
      "[3,   300] loss: 2.326\n",
      "[3,   400] loss: 2.315\n",
      "[3,   500] loss: 2.318\n",
      "[3,   600] loss: 2.326\n",
      "[3,   700] loss: 2.326\n",
      "[3,   800] loss: 2.321\n",
      "[3,   900] loss: 2.319\n",
      "[3,  1000] loss: 2.317\n",
      "[3,  1100] loss: 2.322\n",
      "[3,  1200] loss: 2.315\n",
      "[3,  1300] loss: 2.323\n",
      "[3,  1400] loss: 2.328\n",
      "[3,  1500] loss: 2.318\n",
      "[3,  1600] loss: 2.322\n",
      "[3,  1700] loss: 2.320\n",
      "[3,  1800] loss: 2.318\n",
      "[3,  1900] loss: 2.321\n",
      "[3,  2000] loss: 2.315\n",
      "[3,  2100] loss: 2.327\n",
      "[3,  2200] loss: 2.325\n",
      "[3,  2300] loss: 2.328\n",
      "[3,  2400] loss: 2.325\n",
      "[3,  2500] loss: 2.319\n",
      "[3,  2600] loss: 2.320\n",
      "[3,  2700] loss: 2.315\n",
      "[3,  2800] loss: 2.313\n",
      "[3,  2900] loss: 2.313\n",
      "[3,  3000] loss: 2.319\n",
      "[3,  3100] loss: 2.316\n",
      "[4,   100] loss: 2.319\n",
      "[4,   200] loss: 2.321\n",
      "[4,   300] loss: 2.318\n",
      "[4,   400] loss: 2.323\n",
      "[4,   500] loss: 2.321\n",
      "[4,   600] loss: 2.321\n",
      "[4,   700] loss: 2.325\n",
      "[4,   800] loss: 2.324\n",
      "[4,   900] loss: 2.315\n",
      "[4,  1000] loss: 2.320\n",
      "[4,  1100] loss: 2.325\n",
      "[4,  1200] loss: 2.322\n",
      "[4,  1300] loss: 2.317\n",
      "[4,  1400] loss: 2.320\n",
      "[4,  1500] loss: 2.321\n",
      "[4,  1600] loss: 2.320\n",
      "[4,  1700] loss: 2.322\n",
      "[4,  1800] loss: 2.315\n",
      "[4,  1900] loss: 2.319\n",
      "[4,  2000] loss: 2.319\n",
      "[4,  2100] loss: 2.317\n",
      "[4,  2200] loss: 2.325\n",
      "[4,  2300] loss: 2.319\n",
      "[4,  2400] loss: 2.321\n",
      "[4,  2500] loss: 2.322\n",
      "[4,  2600] loss: 2.321\n",
      "[4,  2700] loss: 2.317\n",
      "[4,  2800] loss: 2.318\n",
      "[4,  2900] loss: 2.324\n",
      "[4,  3000] loss: 2.314\n",
      "[4,  3100] loss: 2.321\n",
      "[5,   100] loss: 2.323\n",
      "[5,   200] loss: 2.318\n",
      "[5,   300] loss: 2.318\n",
      "[5,   400] loss: 2.315\n",
      "[5,   500] loss: 2.317\n",
      "[5,   600] loss: 2.317\n",
      "[5,   700] loss: 2.318\n",
      "[5,   800] loss: 2.318\n",
      "[5,   900] loss: 2.319\n",
      "[5,  1000] loss: 2.327\n",
      "[5,  1100] loss: 2.319\n",
      "[5,  1200] loss: 2.323\n",
      "[5,  1300] loss: 2.321\n",
      "[5,  1400] loss: 2.321\n",
      "[5,  1500] loss: 2.329\n",
      "[5,  1600] loss: 2.309\n",
      "[5,  1700] loss: 2.320\n",
      "[5,  1800] loss: 2.321\n",
      "[5,  1900] loss: 2.319\n",
      "[5,  2000] loss: 2.315\n",
      "[5,  2100] loss: 2.327\n",
      "[5,  2200] loss: 2.325\n",
      "[5,  2300] loss: 2.321\n",
      "[5,  2400] loss: 2.317\n",
      "[5,  2500] loss: 2.320\n",
      "[5,  2600] loss: 2.315\n",
      "[5,  2700] loss: 2.317\n",
      "[5,  2800] loss: 2.320\n",
      "[5,  2900] loss: 2.322\n",
      "[5,  3000] loss: 2.324\n",
      "[5,  3100] loss: 2.332\n",
      "[6,   100] loss: 2.318\n",
      "[6,   200] loss: 2.321\n",
      "[6,   300] loss: 2.316\n",
      "[6,   400] loss: 2.322\n",
      "[6,   500] loss: 2.322\n",
      "[6,   600] loss: 2.329\n",
      "[6,   700] loss: 2.314\n",
      "[6,   800] loss: 2.322\n",
      "[6,   900] loss: 2.320\n",
      "[6,  1000] loss: 2.314\n",
      "[6,  1100] loss: 2.320\n",
      "[6,  1200] loss: 2.320\n",
      "[6,  1300] loss: 2.323\n",
      "[6,  1400] loss: 2.321\n",
      "[6,  1500] loss: 2.319\n",
      "[6,  1600] loss: 2.316\n",
      "[6,  1700] loss: 2.325\n",
      "[6,  1800] loss: 2.322\n",
      "[6,  1900] loss: 2.328\n",
      "[6,  2000] loss: 2.325\n",
      "[6,  2100] loss: 2.319\n",
      "[6,  2200] loss: 2.323\n",
      "[6,  2300] loss: 2.318\n",
      "[6,  2400] loss: 2.316\n",
      "[6,  2500] loss: 2.314\n",
      "[6,  2600] loss: 2.327\n",
      "[6,  2700] loss: 2.317\n",
      "[6,  2800] loss: 2.316\n",
      "[6,  2900] loss: 2.328\n",
      "[6,  3000] loss: 2.316\n",
      "[6,  3100] loss: 2.314\n",
      "[7,   100] loss: 2.320\n",
      "[7,   200] loss: 2.314\n",
      "[7,   300] loss: 2.320\n",
      "[7,   400] loss: 2.320\n",
      "[7,   500] loss: 2.320\n",
      "[7,   600] loss: 2.323\n",
      "[7,   700] loss: 2.316\n",
      "[7,   800] loss: 2.320\n",
      "[7,   900] loss: 2.320\n",
      "[7,  1000] loss: 2.323\n",
      "[7,  1100] loss: 2.321\n",
      "[7,  1200] loss: 2.321\n",
      "[7,  1300] loss: 2.319\n",
      "[7,  1400] loss: 2.323\n",
      "[7,  1500] loss: 2.315\n",
      "[7,  1600] loss: 2.320\n",
      "[7,  1700] loss: 2.316\n",
      "[7,  1800] loss: 2.324\n",
      "[7,  1900] loss: 2.326\n",
      "[7,  2000] loss: 2.318\n",
      "[7,  2100] loss: 2.318\n",
      "[7,  2200] loss: 2.328\n",
      "[7,  2300] loss: 2.323\n",
      "[7,  2400] loss: 2.325\n",
      "[7,  2500] loss: 2.319\n",
      "[7,  2600] loss: 2.316\n",
      "[7,  2700] loss: 2.320\n",
      "[7,  2800] loss: 2.320\n",
      "[7,  2900] loss: 2.321\n",
      "[7,  3000] loss: 2.315\n",
      "[7,  3100] loss: 2.320\n",
      "[8,   100] loss: 2.321\n",
      "[8,   200] loss: 2.322\n",
      "[8,   300] loss: 2.322\n",
      "[8,   400] loss: 2.323\n",
      "[8,   500] loss: 2.327\n",
      "[8,   600] loss: 2.319\n",
      "[8,   700] loss: 2.325\n",
      "[8,   800] loss: 2.321\n",
      "[8,   900] loss: 2.324\n",
      "[8,  1000] loss: 2.315\n",
      "[8,  1100] loss: 2.314\n",
      "[8,  1200] loss: 2.318\n",
      "[8,  1300] loss: 2.320\n",
      "[8,  1400] loss: 2.324\n",
      "[8,  1500] loss: 2.323\n",
      "[8,  1600] loss: 2.321\n",
      "[8,  1700] loss: 2.314\n",
      "[8,  1800] loss: 2.326\n",
      "[8,  1900] loss: 2.318\n",
      "[8,  2000] loss: 2.321\n",
      "[8,  2100] loss: 2.316\n",
      "[8,  2200] loss: 2.322\n",
      "[8,  2300] loss: 2.317\n",
      "[8,  2400] loss: 2.317\n",
      "[8,  2500] loss: 2.314\n",
      "[8,  2600] loss: 2.322\n",
      "[8,  2700] loss: 2.323\n",
      "[8,  2800] loss: 2.317\n",
      "[8,  2900] loss: 2.317\n",
      "[8,  3000] loss: 2.327\n",
      "[8,  3100] loss: 2.317\n",
      "[9,   100] loss: 2.322\n",
      "[9,   200] loss: 2.320\n",
      "[9,   300] loss: 2.323\n",
      "[9,   400] loss: 2.314\n",
      "[9,   500] loss: 2.325\n",
      "[9,   600] loss: 2.326\n",
      "[9,   700] loss: 2.316\n",
      "[9,   800] loss: 2.327\n",
      "[9,   900] loss: 2.322\n",
      "[9,  1000] loss: 2.323\n",
      "[9,  1100] loss: 2.330\n",
      "[9,  1200] loss: 2.324\n",
      "[9,  1300] loss: 2.329\n",
      "[9,  1400] loss: 2.320\n",
      "[9,  1500] loss: 2.317\n",
      "[9,  1600] loss: 2.326\n",
      "[9,  1700] loss: 2.321\n",
      "[9,  1800] loss: 2.313\n",
      "[9,  1900] loss: 2.322\n",
      "[9,  2000] loss: 2.321\n",
      "[9,  2100] loss: 2.322\n",
      "[9,  2200] loss: 2.321\n",
      "[9,  2300] loss: 2.318\n",
      "[9,  2400] loss: 2.315\n",
      "[9,  2500] loss: 2.316\n",
      "[9,  2600] loss: 2.322\n",
      "[9,  2700] loss: 2.322\n",
      "[9,  2800] loss: 2.321\n",
      "[9,  2900] loss: 2.320\n",
      "[9,  3000] loss: 2.327\n",
      "[9,  3100] loss: 2.316\n",
      "[10,   100] loss: 2.317\n",
      "[10,   200] loss: 2.317\n",
      "[10,   300] loss: 2.317\n",
      "[10,   400] loss: 2.321\n",
      "[10,   500] loss: 2.320\n",
      "[10,   600] loss: 2.324\n",
      "[10,   700] loss: 2.323\n",
      "[10,   800] loss: 2.321\n",
      "[10,   900] loss: 2.318\n",
      "[10,  1000] loss: 2.318\n",
      "[10,  1100] loss: 2.315\n",
      "[10,  1200] loss: 2.315\n",
      "[10,  1300] loss: 2.313\n",
      "[10,  1400] loss: 2.324\n",
      "[10,  1500] loss: 2.321\n",
      "[10,  1600] loss: 2.320\n",
      "[10,  1700] loss: 2.322\n",
      "[10,  1800] loss: 2.320\n",
      "[10,  1900] loss: 2.316\n",
      "[10,  2000] loss: 2.317\n",
      "[10,  2100] loss: 2.322\n",
      "[10,  2200] loss: 2.322\n",
      "[10,  2300] loss: 2.323\n",
      "[10,  2400] loss: 2.316\n",
      "[10,  2500] loss: 2.325\n",
      "[10,  2600] loss: 2.323\n",
      "[10,  2700] loss: 2.318\n",
      "[10,  2800] loss: 2.321\n",
      "[10,  2900] loss: 2.313\n",
      "[10,  3000] loss: 2.320\n",
      "[10,  3100] loss: 2.317\n",
      "Finished training\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Seq:       1 Epoch:      0 Step:  0.00000 Loss: 2.318013906478882,\n Seq:       2 Epoch:      0 Step:  1.00000 Loss: 1048.362548828125,\n Seq:       3 Epoch:      0 Step:  2.00000 Loss: 692.7894287109375,\n Seq:       4 Epoch:      0 Step:  3.00000 Loss: 107.52269744873047,\n Seq:       5 Epoch:      0 Step:  4.00000 Loss: 6.08970832824707,\n Seq:       6 Epoch:      0 Step:  5.00000 Loss: 2.3050696849823,\n Seq:       7 Epoch:      0 Step:  6.00000 Loss: 2.2760863304138184,\n Seq:       8 Epoch:      0 Step:  7.00000 Loss: 2.2670044898986816,\n Seq:       9 Epoch:      0 Step:  8.00000 Loss: 2.2668256759643555,\n Seq:      10 Epoch:      0 Step:  9.00000 Loss: 2.2667934894561768,\n Seq:      11 Epoch:      0 Step:  10.00000 Loss: 2.3547203540802,\n Seq:      12 Epoch:      0 Step:  11.00000 Loss: 2.3413684368133545,\n Seq:      13 Epoch:      0 Step:  12.00000 Loss: 2.301520586013794,\n Seq:      14 Epoch:      0 Step:  13.00000 Loss: 2.2876970767974854,\n Seq:      15 Epoch:      0 Step:  14.00000 Loss: 2.303067684173584,\n Seq:      16 Epoch:      0 Step:  15.00000 Loss: 2.3538899421691895,\n Seq:      17 Epoch:      0 Step:  16.00000 Loss: 2.2926599979400635,\n Seq:      18 Epoch:      0 Step:  17.00000 Loss: 2.290853977203369,\n Seq:      19 Epoch:      0 Step:  18.00000 Loss: 2.236003875732422,\n Seq:      20 Epoch:      0 Step:  19.00000 Loss: 2.3950235843658447,\n Seq:      21 Epoch:      0 Step:  20.00000 Loss: 2.2645254135131836,\n Seq:      22 Epoch:      0 Step:  21.00000 Loss: 2.3436341285705566,\n Seq:      23 Epoch:      0 Step:  22.00000 Loss: 2.3442342281341553,\n Seq:      24 Epoch:      0 Step:  23.00000 Loss: 2.303194522857666,\n Seq:      25 Epoch:      0 Step:  24.00000 Loss: 2.2158596515655518,\n Seq:      26 Epoch:      0 Step:  25.00000 Loss: 2.5321178436279297,\n Seq:      27 Epoch:      0 Step:  26.00000 Loss: 2.4137184619903564,\n Seq:      28 Epoch:      0 Step:  27.00000 Loss: 2.3572230339050293,\n Seq:      29 Epoch:      0 Step:  28.00000 Loss: 2.341301918029785,\n Seq:      30 Epoch:      0 Step:  29.00000 Loss: 2.3893933296203613,\n Seq:      31 Epoch:      0 Step:  30.00000 Loss: 2.226043462753296,\n Seq:      32 Epoch:      0 Step:  31.00000 Loss: 2.4196200370788574,\n Seq:      33 Epoch:      0 Step:  32.00000 Loss: 2.394411563873291,\n Seq:      34 Epoch:      0 Step:  33.00000 Loss: 2.4243509769439697,\n Seq:      35 Epoch:      0 Step:  34.00000 Loss: 2.380540370941162,\n Seq:      36 Epoch:      0 Step:  35.00000 Loss: 2.3500821590423584,\n Seq:      37 Epoch:      0 Step:  36.00000 Loss: 2.286184310913086,\n Seq:      38 Epoch:      0 Step:  37.00000 Loss: 2.4388070106506348,\n Seq:      39 Epoch:      0 Step:  38.00000 Loss: 2.2635953426361084,\n Seq:      40 Epoch:      0 Step:  39.00000 Loss: 2.3144712448120117,\n Seq:      41 Epoch:      0 Step:  40.00000 Loss: 2.3026137351989746,\n Seq:      42 Epoch:      0 Step:  41.00000 Loss: 2.339207887649536,\n Seq:      43 Epoch:      0 Step:  42.00000 Loss: 2.2884445190429688,\n Seq:      44 Epoch:      0 Step:  43.00000 Loss: 2.291452646255493,\n Seq:      45 Epoch:      0 Step:  44.00000 Loss: 2.3528435230255127,\n Seq:      46 Epoch:      0 Step:  45.00000 Loss: 2.2358553409576416,\n Seq:      47 Epoch:      0 Step:  46.00000 Loss: 2.287454128265381,\n Seq:      48 Epoch:      0 Step:  47.00000 Loss: 2.2858169078826904,\n Seq:      49 Epoch:      0 Step:  48.00000 Loss: 2.339552879333496,\n Seq:      50 Epoch:      0 Step:  49.00000 Loss: 2.3590171337127686,\n Seq:      51 Epoch:      0 Step:  50.00000 Loss: 2.2681076526641846,\n Seq:      52 Epoch:      0 Step:  51.00000 Loss: 2.2851853370666504,\n Seq:      53 Epoch:      0 Step:  52.00000 Loss: 2.323424816131592,\n Seq:      54 Epoch:      0 Step:  53.00000 Loss: 2.225602865219116,\n Seq:      55 Epoch:      0 Step:  54.00000 Loss: 2.329280138015747,\n Seq:      56 Epoch:      0 Step:  55.00000 Loss: 2.3423709869384766,\n Seq:      57 Epoch:      0 Step:  56.00000 Loss: 2.2524967193603516,\n Seq:      58 Epoch:      0 Step:  57.00000 Loss: 2.2737367153167725,\n Seq:      59 Epoch:      0 Step:  58.00000 Loss: 2.312044143676758,\n Seq:      60 Epoch:      0 Step:  59.00000 Loss: 2.335271120071411,\n Seq:      61 Epoch:      0 Step:  60.00000 Loss: 2.254288911819458,\n Seq:      62 Epoch:      0 Step:  61.00000 Loss: 2.291426658630371,\n Seq:      63 Epoch:      0 Step:  62.00000 Loss: 2.29825496673584,\n Seq:      64 Epoch:      0 Step:  63.00000 Loss: 2.2572805881500244,\n Seq:      65 Epoch:      0 Step:  64.00000 Loss: 2.282668352127075,\n Seq:      66 Epoch:      0 Step:  65.00000 Loss: 2.2915492057800293,\n Seq:      67 Epoch:      0 Step:  66.00000 Loss: 2.4461963176727295,\n Seq:      68 Epoch:      0 Step:  67.00000 Loss: 2.3818564414978027,\n Seq:      69 Epoch:      0 Step:  68.00000 Loss: 2.3870978355407715,\n Seq:      70 Epoch:      0 Step:  69.00000 Loss: 2.3582699298858643,\n Seq:      71 Epoch:      0 Step:  70.00000 Loss: 2.3289835453033447,\n Seq:      72 Epoch:      0 Step:  71.00000 Loss: 2.3256261348724365,\n Seq:      73 Epoch:      0 Step:  72.00000 Loss: 2.4442954063415527,\n Seq:      74 Epoch:      0 Step:  73.00000 Loss: 2.3915324211120605,\n Seq:      75 Epoch:      0 Step:  74.00000 Loss: 2.360713243484497,\n Seq:      76 Epoch:      0 Step:  75.00000 Loss: 2.360410451889038,\n Seq:      77 Epoch:      0 Step:  76.00000 Loss: 2.2959771156311035,\n Seq:      78 Epoch:      0 Step:  77.00000 Loss: 2.2931478023529053,\n Seq:      79 Epoch:      0 Step:  78.00000 Loss: 2.3052351474761963,\n Seq:      80 Epoch:      0 Step:  79.00000 Loss: 2.321991443634033,\n Seq:      81 Epoch:      0 Step:  80.00000 Loss: 2.254972219467163,\n Seq:      82 Epoch:      0 Step:  81.00000 Loss: 2.369953155517578,\n Seq:      83 Epoch:      0 Step:  82.00000 Loss: 2.2987465858459473,\n Seq:      84 Epoch:      0 Step:  83.00000 Loss: 2.2924320697784424,\n Seq:      85 Epoch:      0 Step:  84.00000 Loss: 2.351015090942383,\n Seq:      86 Epoch:      0 Step:  85.00000 Loss: 2.352128267288208,\n Seq:      87 Epoch:      0 Step:  86.00000 Loss: 2.3442206382751465,\n Seq:      88 Epoch:      0 Step:  87.00000 Loss: 2.332611322402954,\n Seq:      89 Epoch:      0 Step:  88.00000 Loss: 2.2556023597717285,\n Seq:      90 Epoch:      0 Step:  89.00000 Loss: 2.2804133892059326,\n Seq:      91 Epoch:      0 Step:  90.00000 Loss: 2.3005690574645996,\n Seq:      92 Epoch:      0 Step:  91.00000 Loss: 2.2409191131591797,\n Seq:      93 Epoch:      0 Step:  92.00000 Loss: 2.2823073863983154,\n Seq:      94 Epoch:      0 Step:  93.00000 Loss: 2.3323516845703125,\n Seq:      95 Epoch:      0 Step:  94.00000 Loss: 2.3965508937835693,\n Seq:      96 Epoch:      0 Step:  95.00000 Loss: 2.2861082553863525,\n Seq:      97 Epoch:      0 Step:  96.00000 Loss: 2.284417152404785,\n Seq:      98 Epoch:      0 Step:  97.00000 Loss: 2.317302942276001,\n Seq:      99 Epoch:      0 Step:  98.00000 Loss: 2.3290932178497314,\n Seq:     100 Epoch:      0 Step:  99.00000 Loss: 2.2696211338043213,\n Seq:     101 Epoch:      0 Step:  100.00000 Loss: 2.3155088424682617,\n Seq:     102 Epoch:      0 Step:  101.00000 Loss: 2.174067974090576,\n Seq:     103 Epoch:      0 Step:  102.00000 Loss: 2.314704179763794,\n Seq:     104 Epoch:      0 Step:  103.00000 Loss: 2.7956390380859375,\n Seq:     105 Epoch:      0 Step:  104.00000 Loss: 2.3947057723999023,\n Seq:     106 Epoch:      0 Step:  105.00000 Loss: 2.311461925506592,\n Seq:     107 Epoch:      0 Step:  106.00000 Loss: 2.3314061164855957,\n Seq:     108 Epoch:      0 Step:  107.00000 Loss: 2.27909779548645,\n Seq:     109 Epoch:      0 Step:  108.00000 Loss: 2.2054615020751953,\n Seq:     110 Epoch:      0 Step:  109.00000 Loss: 2.3148694038391113,\n Seq:     111 Epoch:      0 Step:  110.00000 Loss: 2.3783414363861084,\n Seq:     112 Epoch:      0 Step:  111.00000 Loss: 2.2990493774414062,\n Seq:     113 Epoch:      0 Step:  112.00000 Loss: 2.366567373275757,\n Seq:     114 Epoch:      0 Step:  113.00000 Loss: 2.3768362998962402,\n Seq:     115 Epoch:      0 Step:  114.00000 Loss: 2.362692356109619,\n Seq:     116 Epoch:      0 Step:  115.00000 Loss: 2.365727424621582,\n Seq:     117 Epoch:      0 Step:  116.00000 Loss: 2.2733278274536133,\n Seq:     118 Epoch:      0 Step:  117.00000 Loss: 2.300736427307129,\n Seq:     119 Epoch:      0 Step:  118.00000 Loss: 2.3250961303710938,\n Seq:     120 Epoch:      0 Step:  119.00000 Loss: 2.349917411804199,\n Seq:     121 Epoch:      0 Step:  120.00000 Loss: 2.3011913299560547,\n Seq:     122 Epoch:      0 Step:  121.00000 Loss: 2.3266983032226562,\n Seq:     123 Epoch:      0 Step:  122.00000 Loss: 2.295607566833496,\n Seq:     124 Epoch:      0 Step:  123.00000 Loss: 2.322582960128784,\n Seq:     125 Epoch:      0 Step:  124.00000 Loss: 2.271575450897217,\n Seq:     126 Epoch:      0 Step:  125.00000 Loss: 2.319260835647583,\n Seq:     127 Epoch:      0 Step:  126.00000 Loss: 2.3126120567321777,\n Seq:     128 Epoch:      0 Step:  127.00000 Loss: 2.3420019149780273,\n Seq:     129 Epoch:      0 Step:  128.00000 Loss: 2.332932710647583,\n Seq:     130 Epoch:      0 Step:  129.00000 Loss: 2.323763847351074,\n Seq:     131 Epoch:      0 Step:  130.00000 Loss: 2.352447986602783,\n Seq:     132 Epoch:      0 Step:  131.00000 Loss: 2.3058059215545654,\n Seq:     133 Epoch:      0 Step:  132.00000 Loss: 2.2694618701934814,\n Seq:     134 Epoch:      0 Step:  133.00000 Loss: 2.2991113662719727,\n Seq:     135 Epoch:      0 Step:  134.00000 Loss: 2.29475474357605,\n Seq:     136 Epoch:      0 Step:  135.00000 Loss: 2.3336472511291504,\n Seq:     137 Epoch:      0 Step:  136.00000 Loss: 2.307281732559204,\n Seq:     138 Epoch:      0 Step:  137.00000 Loss: 2.3131024837493896,\n Seq:     139 Epoch:      0 Step:  138.00000 Loss: 2.33396315574646,\n Seq:     140 Epoch:      0 Step:  139.00000 Loss: 2.293037176132202,\n Seq:     141 Epoch:      0 Step:  140.00000 Loss: 2.310922384262085,\n Seq:     142 Epoch:      0 Step:  141.00000 Loss: 2.3564200401306152,\n Seq:     143 Epoch:      0 Step:  142.00000 Loss: 2.2990705966949463,\n Seq:     144 Epoch:      0 Step:  143.00000 Loss: 2.3101131916046143,\n Seq:     145 Epoch:      0 Step:  144.00000 Loss: 2.3032782077789307,\n Seq:     146 Epoch:      0 Step:  145.00000 Loss: 2.360238790512085,\n Seq:     147 Epoch:      0 Step:  146.00000 Loss: 2.3046603202819824,\n Seq:     148 Epoch:      0 Step:  147.00000 Loss: 2.2746219635009766,\n Seq:     149 Epoch:      0 Step:  148.00000 Loss: 2.3187975883483887,\n Seq:     150 Epoch:      0 Step:  149.00000 Loss: 2.2600412368774414,\n Seq:     151 Epoch:      0 Step:  150.00000 Loss: 2.3279170989990234,\n Seq:     152 Epoch:      0 Step:  151.00000 Loss: 2.3499789237976074,\n Seq:     153 Epoch:      0 Step:  152.00000 Loss: 2.394468069076538,\n Seq:     154 Epoch:      0 Step:  153.00000 Loss: 2.357428789138794,\n Seq:     155 Epoch:      0 Step:  154.00000 Loss: 2.368075370788574,\n Seq:     156 Epoch:      0 Step:  155.00000 Loss: 2.321852445602417,\n Seq:     157 Epoch:      0 Step:  156.00000 Loss: 2.3195924758911133,\n Seq:     158 Epoch:      0 Step:  157.00000 Loss: 2.2956724166870117,\n Seq:     159 Epoch:      0 Step:  158.00000 Loss: 2.3087961673736572,\n Seq:     160 Epoch:      0 Step:  159.00000 Loss: 2.302647829055786,\n Seq:     161 Epoch:      0 Step:  160.00000 Loss: 2.246403932571411,\n Seq:     162 Epoch:      0 Step:  161.00000 Loss: 2.3297853469848633,\n Seq:     163 Epoch:      0 Step:  162.00000 Loss: 2.346705198287964,\n Seq:     164 Epoch:      0 Step:  163.00000 Loss: 2.3090155124664307,\n Seq:     165 Epoch:      0 Step:  164.00000 Loss: 2.333984851837158,\n Seq:     166 Epoch:      0 Step:  165.00000 Loss: 2.3049933910369873,\n Seq:     167 Epoch:      0 Step:  166.00000 Loss: 2.3071393966674805,\n Seq:     168 Epoch:      0 Step:  167.00000 Loss: 2.3379178047180176,\n Seq:     169 Epoch:      0 Step:  168.00000 Loss: 2.292415142059326,\n Seq:     170 Epoch:      0 Step:  169.00000 Loss: 2.3305180072784424,\n Seq:     171 Epoch:      0 Step:  170.00000 Loss: 2.3032758235931396,\n Seq:     172 Epoch:      0 Step:  171.00000 Loss: 2.240668773651123,\n Seq:     173 Epoch:      0 Step:  172.00000 Loss: 2.388820171356201,\n Seq:     174 Epoch:      0 Step:  173.00000 Loss: 2.3218069076538086,\n Seq:     175 Epoch:      0 Step:  174.00000 Loss: 2.3388805389404297,\n Seq:     176 Epoch:      0 Step:  175.00000 Loss: 2.2179038524627686,\n Seq:     177 Epoch:      0 Step:  176.00000 Loss: 2.3115122318267822,\n Seq:     178 Epoch:      0 Step:  177.00000 Loss: 2.2706336975097656,\n Seq:     179 Epoch:      0 Step:  178.00000 Loss: 2.293875217437744,\n Seq:     180 Epoch:      0 Step:  179.00000 Loss: 2.328524589538574,\n Seq:     181 Epoch:      0 Step:  180.00000 Loss: 2.356278657913208,\n Seq:     182 Epoch:      0 Step:  181.00000 Loss: 2.3608856201171875,\n Seq:     183 Epoch:      0 Step:  182.00000 Loss: 2.3346290588378906,\n Seq:     184 Epoch:      0 Step:  183.00000 Loss: 2.356611728668213,\n Seq:     185 Epoch:      0 Step:  184.00000 Loss: 2.275832176208496,\n Seq:     186 Epoch:      0 Step:  185.00000 Loss: 2.296929121017456,\n Seq:     187 Epoch:      0 Step:  186.00000 Loss: 2.3097023963928223,\n Seq:     188 Epoch:      0 Step:  187.00000 Loss: 2.3521387577056885,\n Seq:     189 Epoch:      0 Step:  188.00000 Loss: 2.3427867889404297,\n Seq:     190 Epoch:      0 Step:  189.00000 Loss: 2.312155246734619,\n Seq:     191 Epoch:      0 Step:  190.00000 Loss: 2.2814157009124756,\n Seq:     192 Epoch:      0 Step:  191.00000 Loss: 2.311579465866089,\n Seq:     193 Epoch:      0 Step:  192.00000 Loss: 2.325529098510742,\n Seq:     194 Epoch:      0 Step:  193.00000 Loss: 2.3356974124908447,\n Seq:     195 Epoch:      0 Step:  194.00000 Loss: 2.3227009773254395,\n Seq:     196 Epoch:      0 Step:  195.00000 Loss: 2.2928574085235596,\n Seq:     197 Epoch:      0 Step:  196.00000 Loss: 2.3712029457092285,\n Seq:     198 Epoch:      0 Step:  197.00000 Loss: 2.2352981567382812,\n Seq:     199 Epoch:      0 Step:  198.00000 Loss: 2.2936887741088867,\n Seq:     200 Epoch:      0 Step:  199.00000 Loss: 2.334043264389038,\n Seq:     201 Epoch:      0 Step:  200.00000 Loss: 2.303447961807251,\n Seq:     202 Epoch:      0 Step:  201.00000 Loss: 2.3216371536254883,\n Seq:     203 Epoch:      0 Step:  202.00000 Loss: 2.312941312789917,\n Seq:     204 Epoch:      0 Step:  203.00000 Loss: 2.3846402168273926,\n Seq:     205 Epoch:      0 Step:  204.00000 Loss: 2.323702335357666,\n Seq:     206 Epoch:      0 Step:  205.00000 Loss: 2.31839919090271,\n Seq:     207 Epoch:      0 Step:  206.00000 Loss: 2.3139243125915527,\n Seq:     208 Epoch:      0 Step:  207.00000 Loss: 2.308136463165283,\n Seq:     209 Epoch:      0 Step:  208.00000 Loss: 2.316882371902466,\n Seq:     210 Epoch:      0 Step:  209.00000 Loss: 2.3400981426239014,\n Seq:     211 Epoch:      0 Step:  210.00000 Loss: 2.310478925704956,\n Seq:     212 Epoch:      0 Step:  211.00000 Loss: 2.360320568084717,\n Seq:     213 Epoch:      0 Step:  212.00000 Loss: 2.2650554180145264,\n Seq:     214 Epoch:      0 Step:  213.00000 Loss: 2.328599691390991,\n Seq:     215 Epoch:      0 Step:  214.00000 Loss: 2.2745559215545654,\n Seq:     216 Epoch:      0 Step:  215.00000 Loss: 2.324878692626953,\n Seq:     217 Epoch:      0 Step:  216.00000 Loss: 2.344693899154663,\n Seq:     218 Epoch:      0 Step:  217.00000 Loss: 2.295299530029297,\n Seq:     219 Epoch:      0 Step:  218.00000 Loss: 2.298063278198242,\n Seq:     220 Epoch:      0 Step:  219.00000 Loss: 2.2828989028930664,\n Seq:     221 Epoch:      0 Step:  220.00000 Loss: 2.2555181980133057,\n Seq:     222 Epoch:      0 Step:  221.00000 Loss: 2.3253917694091797,\n Seq:     223 Epoch:      0 Step:  222.00000 Loss: 2.2788426876068115,\n Seq:     224 Epoch:      0 Step:  223.00000 Loss: 2.364989995956421,\n Seq:     225 Epoch:      0 Step:  224.00000 Loss: 2.2984695434570312,\n Seq:     226 Epoch:      0 Step:  225.00000 Loss: 2.2887203693389893,\n Seq:     227 Epoch:      0 Step:  226.00000 Loss: 2.346006155014038,\n Seq:     228 Epoch:      0 Step:  227.00000 Loss: 2.264636278152466,\n Seq:     229 Epoch:      0 Step:  228.00000 Loss: 2.3235745429992676,\n Seq:     230 Epoch:      0 Step:  229.00000 Loss: 2.2877140045166016,\n Seq:     231 Epoch:      0 Step:  230.00000 Loss: 2.225569248199463,\n Seq:     232 Epoch:      0 Step:  231.00000 Loss: 2.3246078491210938,\n Seq:     233 Epoch:      0 Step:  232.00000 Loss: 2.326443672180176,\n Seq:     234 Epoch:      0 Step:  233.00000 Loss: 2.2713842391967773,\n Seq:     235 Epoch:      0 Step:  234.00000 Loss: 2.268862247467041,\n Seq:     236 Epoch:      0 Step:  235.00000 Loss: 2.261357069015503,\n Seq:     237 Epoch:      0 Step:  236.00000 Loss: 2.20304799079895,\n Seq:     238 Epoch:      0 Step:  237.00000 Loss: 2.326620101928711,\n Seq:     239 Epoch:      0 Step:  238.00000 Loss: 2.3496031761169434,\n Seq:     240 Epoch:      0 Step:  239.00000 Loss: 2.2895493507385254,\n Seq:     241 Epoch:      0 Step:  240.00000 Loss: 2.2792227268218994,\n Seq:     242 Epoch:      0 Step:  241.00000 Loss: 2.41597843170166,\n Seq:     243 Epoch:      0 Step:  242.00000 Loss: 2.4137890338897705,\n Seq:     244 Epoch:      0 Step:  243.00000 Loss: 2.2829251289367676,\n Seq:     245 Epoch:      0 Step:  244.00000 Loss: 2.279698610305786,\n Seq:     246 Epoch:      0 Step:  245.00000 Loss: 2.347231864929199,\n Seq:     247 Epoch:      0 Step:  246.00000 Loss: 2.3378849029541016,\n Seq:     248 Epoch:      0 Step:  247.00000 Loss: 2.253560781478882,\n Seq:     249 Epoch:      0 Step:  248.00000 Loss: 2.251065254211426,\n Seq:     250 Epoch:      0 Step:  249.00000 Loss: 2.3036935329437256,\n Seq:     251 Epoch:      0 Step:  250.00000 Loss: 2.237316846847534,\n Seq:     252 Epoch:      0 Step:  251.00000 Loss: 2.3658180236816406,\n Seq:     253 Epoch:      0 Step:  252.00000 Loss: 2.371154546737671,\n Seq:     254 Epoch:      0 Step:  253.00000 Loss: 2.4156129360198975,\n Seq:     255 Epoch:      0 Step:  254.00000 Loss: 2.1970913410186768,\n Seq:     256 Epoch:      0 Step:  255.00000 Loss: 2.328124523162842,\n Seq:     257 Epoch:      0 Step:  256.00000 Loss: 2.385488748550415,\n Seq:     258 Epoch:      0 Step:  257.00000 Loss: 2.2688324451446533,\n Seq:     259 Epoch:      0 Step:  258.00000 Loss: 2.3061468601226807,\n Seq:     260 Epoch:      0 Step:  259.00000 Loss: 2.3768646717071533,\n Seq:     261 Epoch:      0 Step:  260.00000 Loss: 2.3422691822052,\n Seq:     262 Epoch:      0 Step:  261.00000 Loss: 2.453502655029297,\n Seq:     263 Epoch:      0 Step:  262.00000 Loss: 2.155648708343506,\n Seq:     264 Epoch:      0 Step:  263.00000 Loss: 2.3846166133880615,\n Seq:     265 Epoch:      0 Step:  264.00000 Loss: 2.319169282913208,\n Seq:     266 Epoch:      0 Step:  265.00000 Loss: 2.4015207290649414,\n Seq:     267 Epoch:      0 Step:  266.00000 Loss: 2.3906636238098145,\n Seq:     268 Epoch:      0 Step:  267.00000 Loss: 2.2193710803985596,\n Seq:     269 Epoch:      0 Step:  268.00000 Loss: 2.4290239810943604,\n Seq:     270 Epoch:      0 Step:  269.00000 Loss: 2.3260035514831543,\n Seq:     271 Epoch:      0 Step:  270.00000 Loss: 2.322441816329956,\n Seq:     272 Epoch:      0 Step:  271.00000 Loss: 2.316922187805176,\n Seq:     273 Epoch:      0 Step:  272.00000 Loss: 2.3148162364959717,\n Seq:     274 Epoch:      0 Step:  273.00000 Loss: 2.346447467803955,\n Seq:     275 Epoch:      0 Step:  274.00000 Loss: 2.2992331981658936,\n Seq:     276 Epoch:      0 Step:  275.00000 Loss: 2.3142764568328857,\n Seq:     277 Epoch:      0 Step:  276.00000 Loss: 2.2693047523498535,\n Seq:     278 Epoch:      0 Step:  277.00000 Loss: 2.321228504180908,\n Seq:     279 Epoch:      0 Step:  278.00000 Loss: 2.3301830291748047,\n Seq:     280 Epoch:      0 Step:  279.00000 Loss: 2.350802183151245,\n Seq:     281 Epoch:      0 Step:  280.00000 Loss: 2.277796506881714,\n Seq:     282 Epoch:      0 Step:  281.00000 Loss: 2.3086013793945312,\n Seq:     283 Epoch:      0 Step:  282.00000 Loss: 2.288165330886841,\n Seq:     284 Epoch:      0 Step:  283.00000 Loss: 2.3321785926818848,\n Seq:     285 Epoch:      0 Step:  284.00000 Loss: 2.3233108520507812,\n Seq:     286 Epoch:      0 Step:  285.00000 Loss: 2.3208954334259033,\n Seq:     287 Epoch:      0 Step:  286.00000 Loss: 2.295652389526367,\n Seq:     288 Epoch:      0 Step:  287.00000 Loss: 2.358942985534668,\n Seq:     289 Epoch:      0 Step:  288.00000 Loss: 2.3046655654907227,\n Seq:     290 Epoch:      0 Step:  289.00000 Loss: 2.342616081237793,\n Seq:     291 Epoch:      0 Step:  290.00000 Loss: 2.2696280479431152,\n Seq:     292 Epoch:      0 Step:  291.00000 Loss: 2.3270962238311768,\n Seq:     293 Epoch:      0 Step:  292.00000 Loss: 2.3059258460998535,\n Seq:     294 Epoch:      0 Step:  293.00000 Loss: 2.344918966293335,\n Seq:     295 Epoch:      0 Step:  294.00000 Loss: 2.3781373500823975,\n Seq:     296 Epoch:      0 Step:  295.00000 Loss: 2.301215887069702,\n Seq:     297 Epoch:      0 Step:  296.00000 Loss: 2.336120843887329,\n Seq:     298 Epoch:      0 Step:  297.00000 Loss: 2.378293752670288,\n Seq:     299 Epoch:      0 Step:  298.00000 Loss: 2.2748031616210938,\n Seq:     300 Epoch:      0 Step:  299.00000 Loss: 2.2554287910461426,\n Seq:     301 Epoch:      0 Step:  300.00000 Loss: 2.3353376388549805,\n Seq:     302 Epoch:      0 Step:  301.00000 Loss: 2.2289249897003174,\n Seq:     303 Epoch:      0 Step:  302.00000 Loss: 2.2813472747802734,\n Seq:     304 Epoch:      0 Step:  303.00000 Loss: 2.335221767425537,\n Seq:     305 Epoch:      0 Step:  304.00000 Loss: 2.303093194961548,\n Seq:     306 Epoch:      0 Step:  305.00000 Loss: 2.39516019821167,\n Seq:     307 Epoch:      0 Step:  306.00000 Loss: 2.301867723464966,\n Seq:     308 Epoch:      0 Step:  307.00000 Loss: 2.4014124870300293,\n Seq:     309 Epoch:      0 Step:  308.00000 Loss: 2.289454221725464,\n Seq:     310 Epoch:      0 Step:  309.00000 Loss: 2.3593344688415527,\n Seq:     311 Epoch:      0 Step:  310.00000 Loss: 2.335960626602173,\n Seq:     312 Epoch:      0 Step:  311.00000 Loss: 2.323786497116089,\n Seq:     313 Epoch:      0 Step:  312.00000 Loss: 2.3981966972351074,\n Seq:     314 Epoch:      0 Step:  313.00000 Loss: 2.3296523094177246,\n Seq:     315 Epoch:      0 Step:  314.00000 Loss: 2.3473961353302,\n Seq:     316 Epoch:      0 Step:  315.00000 Loss: 2.3191981315612793,\n Seq:     317 Epoch:      0 Step:  316.00000 Loss: 2.2968995571136475,\n Seq:     318 Epoch:      0 Step:  317.00000 Loss: 2.298429489135742,\n Seq:     319 Epoch:      0 Step:  318.00000 Loss: 2.261653184890747,\n Seq:     320 Epoch:      0 Step:  319.00000 Loss: 2.3214528560638428,\n Seq:     321 Epoch:      0 Step:  320.00000 Loss: 2.349153518676758,\n Seq:     322 Epoch:      0 Step:  321.00000 Loss: 2.295703649520874,\n Seq:     323 Epoch:      0 Step:  322.00000 Loss: 2.3257815837860107,\n Seq:     324 Epoch:      0 Step:  323.00000 Loss: 2.3435850143432617,\n Seq:     325 Epoch:      0 Step:  324.00000 Loss: 2.2743940353393555,\n Seq:     326 Epoch:      0 Step:  325.00000 Loss: 2.270071029663086,\n Seq:     327 Epoch:      0 Step:  326.00000 Loss: 2.29453706741333,\n Seq:     328 Epoch:      0 Step:  327.00000 Loss: 2.2688755989074707,\n Seq:     329 Epoch:      0 Step:  328.00000 Loss: 2.288452386856079,\n Seq:     330 Epoch:      0 Step:  329.00000 Loss: 2.396486759185791,\n Seq:     331 Epoch:      0 Step:  330.00000 Loss: 2.302088737487793,\n Seq:     332 Epoch:      0 Step:  331.00000 Loss: 2.1838037967681885,\n Seq:     333 Epoch:      0 Step:  332.00000 Loss: 2.271129608154297,\n Seq:     334 Epoch:      0 Step:  333.00000 Loss: 2.3334615230560303,\n Seq:     335 Epoch:      0 Step:  334.00000 Loss: 2.2585370540618896,\n Seq:     336 Epoch:      0 Step:  335.00000 Loss: 2.3677146434783936,\n Seq:     337 Epoch:      0 Step:  336.00000 Loss: 2.2802734375,\n Seq:     338 Epoch:      0 Step:  337.00000 Loss: 2.4969658851623535,\n Seq:     339 Epoch:      0 Step:  338.00000 Loss: 2.4917588233947754,\n Seq:     340 Epoch:      0 Step:  339.00000 Loss: 2.3337182998657227,\n Seq:     341 Epoch:      0 Step:  340.00000 Loss: 2.2905821800231934,\n Seq:     342 Epoch:      0 Step:  341.00000 Loss: 2.3385989665985107,\n Seq:     343 Epoch:      0 Step:  342.00000 Loss: 2.2916009426116943,\n Seq:     344 Epoch:      0 Step:  343.00000 Loss: 2.3343608379364014,\n Seq:     345 Epoch:      0 Step:  344.00000 Loss: 2.3438007831573486,\n Seq:     346 Epoch:      0 Step:  345.00000 Loss: 2.2579691410064697,\n Seq:     347 Epoch:      0 Step:  346.00000 Loss: 2.3205292224884033,\n Seq:     348 Epoch:      0 Step:  347.00000 Loss: 2.272413730621338,\n Seq:     349 Epoch:      0 Step:  348.00000 Loss: 2.411616802215576,\n Seq:     350 Epoch:      0 Step:  349.00000 Loss: 2.226062774658203,\n Seq:     351 Epoch:      0 Step:  350.00000 Loss: 2.2684285640716553,\n Seq:     352 Epoch:      0 Step:  351.00000 Loss: 2.2319092750549316,\n Seq:     353 Epoch:      0 Step:  352.00000 Loss: 2.26845645904541,\n Seq:     354 Epoch:      0 Step:  353.00000 Loss: 2.346501111984253,\n Seq:     355 Epoch:      0 Step:  354.00000 Loss: 2.4033043384552,\n Seq:     356 Epoch:      0 Step:  355.00000 Loss: 2.3571693897247314,\n Seq:     357 Epoch:      0 Step:  356.00000 Loss: 2.3526504039764404,\n Seq:     358 Epoch:      0 Step:  357.00000 Loss: 2.276628017425537,\n Seq:     359 Epoch:      0 Step:  358.00000 Loss: 2.29492449760437,\n Seq:     360 Epoch:      0 Step:  359.00000 Loss: 2.315770149230957,\n Seq:     361 Epoch:      0 Step:  360.00000 Loss: 2.258788585662842,\n Seq:     362 Epoch:      0 Step:  361.00000 Loss: 2.274566173553467,\n Seq:     363 Epoch:      0 Step:  362.00000 Loss: 2.3412740230560303,\n Seq:     364 Epoch:      0 Step:  363.00000 Loss: 2.217602014541626,\n Seq:     365 Epoch:      0 Step:  364.00000 Loss: 2.322108507156372,\n Seq:     366 Epoch:      0 Step:  365.00000 Loss: 2.362740993499756,\n Seq:     367 Epoch:      0 Step:  366.00000 Loss: 2.4162039756774902,\n Seq:     368 Epoch:      0 Step:  367.00000 Loss: 2.3532419204711914,\n Seq:     369 Epoch:      0 Step:  368.00000 Loss: 2.2859344482421875,\n Seq:     370 Epoch:      0 Step:  369.00000 Loss: 2.317046642303467,\n Seq:     371 Epoch:      0 Step:  370.00000 Loss: 2.3067851066589355,\n Seq:     372 Epoch:      0 Step:  371.00000 Loss: 2.4317188262939453,\n Seq:     373 Epoch:      0 Step:  372.00000 Loss: 2.2528440952301025,\n Seq:     374 Epoch:      0 Step:  373.00000 Loss: 2.424618721008301,\n Seq:     375 Epoch:      0 Step:  374.00000 Loss: 2.373044967651367,\n Seq:     376 Epoch:      0 Step:  375.00000 Loss: 2.3214704990386963,\n Seq:     377 Epoch:      0 Step:  376.00000 Loss: 2.350081205368042,\n Seq:     378 Epoch:      0 Step:  377.00000 Loss: 2.2185590267181396,\n Seq:     379 Epoch:      0 Step:  378.00000 Loss: 2.37092924118042,\n Seq:     380 Epoch:      0 Step:  379.00000 Loss: 2.3110573291778564,\n Seq:     381 Epoch:      0 Step:  380.00000 Loss: 2.297229290008545,\n Seq:     382 Epoch:      0 Step:  381.00000 Loss: 2.27014422416687,\n Seq:     383 Epoch:      0 Step:  382.00000 Loss: 2.283518075942993,\n Seq:     384 Epoch:      0 Step:  383.00000 Loss: 2.257258415222168,\n Seq:     385 Epoch:      0 Step:  384.00000 Loss: 2.3935346603393555,\n Seq:     386 Epoch:      0 Step:  385.00000 Loss: 2.2816531658172607,\n Seq:     387 Epoch:      0 Step:  386.00000 Loss: 2.3498694896698,\n Seq:     388 Epoch:      0 Step:  387.00000 Loss: 2.2788567543029785,\n Seq:     389 Epoch:      0 Step:  388.00000 Loss: 2.293487071990967,\n Seq:     390 Epoch:      0 Step:  389.00000 Loss: 2.278304100036621,\n Seq:     391 Epoch:      0 Step:  390.00000 Loss: 2.3717498779296875,\n Seq:     392 Epoch:      0 Step:  391.00000 Loss: 2.347557544708252,\n Seq:     393 Epoch:      0 Step:  392.00000 Loss: 2.2477238178253174,\n Seq:     394 Epoch:      0 Step:  393.00000 Loss: 2.3356258869171143,\n Seq:     395 Epoch:      0 Step:  394.00000 Loss: 2.2403857707977295,\n Seq:     396 Epoch:      0 Step:  395.00000 Loss: 2.2963380813598633,\n Seq:     397 Epoch:      0 Step:  396.00000 Loss: 2.2984697818756104,\n Seq:     398 Epoch:      0 Step:  397.00000 Loss: 2.3063528537750244,\n Seq:     399 Epoch:      0 Step:  398.00000 Loss: 2.3385939598083496,\n Seq:     400 Epoch:      0 Step:  399.00000 Loss: 2.365360975265503,\n Seq:     401 Epoch:      0 Step:  400.00000 Loss: 2.4200632572174072,\n Seq:     402 Epoch:      0 Step:  401.00000 Loss: 2.3462512493133545,\n Seq:     403 Epoch:      0 Step:  402.00000 Loss: 2.2597053050994873,\n Seq:     404 Epoch:      0 Step:  403.00000 Loss: 2.3715262413024902,\n Seq:     405 Epoch:      0 Step:  404.00000 Loss: 2.358660936355591,\n Seq:     406 Epoch:      0 Step:  405.00000 Loss: 2.334775447845459,\n Seq:     407 Epoch:      0 Step:  406.00000 Loss: 2.228806972503662,\n Seq:     408 Epoch:      0 Step:  407.00000 Loss: 2.4465441703796387,\n Seq:     409 Epoch:      0 Step:  408.00000 Loss: 2.33903169631958,\n Seq:     410 Epoch:      0 Step:  409.00000 Loss: 2.3101730346679688,\n Seq:     411 Epoch:      0 Step:  410.00000 Loss: 2.304157018661499,\n Seq:     412 Epoch:      0 Step:  411.00000 Loss: 2.361544609069824,\n Seq:     413 Epoch:      0 Step:  412.00000 Loss: 2.4063680171966553,\n Seq:     414 Epoch:      0 Step:  413.00000 Loss: 2.4041244983673096,\n Seq:     415 Epoch:      0 Step:  414.00000 Loss: 2.2986011505126953,\n Seq:     416 Epoch:      0 Step:  415.00000 Loss: 2.3957176208496094,\n Seq:     417 Epoch:      0 Step:  416.00000 Loss: 2.2778873443603516,\n Seq:     418 Epoch:      0 Step:  417.00000 Loss: 2.3569254875183105,\n Seq:     419 Epoch:      0 Step:  418.00000 Loss: 2.318248987197876,\n Seq:     420 Epoch:      0 Step:  419.00000 Loss: 2.2883381843566895,\n Seq:     421 Epoch:      0 Step:  420.00000 Loss: 2.3064019680023193,\n Seq:     422 Epoch:      0 Step:  421.00000 Loss: 2.3635752201080322,\n Seq:     423 Epoch:      0 Step:  422.00000 Loss: 2.3303792476654053,\n Seq:     424 Epoch:      0 Step:  423.00000 Loss: 2.2990617752075195,\n Seq:     425 Epoch:      0 Step:  424.00000 Loss: 2.295954704284668,\n Seq:     426 Epoch:      0 Step:  425.00000 Loss: 2.3434722423553467,\n Seq:     427 Epoch:      0 Step:  426.00000 Loss: 2.3195266723632812,\n Seq:     428 Epoch:      0 Step:  427.00000 Loss: 2.303807497024536,\n Seq:     429 Epoch:      0 Step:  428.00000 Loss: 2.2840211391448975,\n Seq:     430 Epoch:      0 Step:  429.00000 Loss: 2.2932469844818115,\n Seq:     431 Epoch:      0 Step:  430.00000 Loss: 2.3689804077148438,\n Seq:     432 Epoch:      0 Step:  431.00000 Loss: 2.2423834800720215,\n Seq:     433 Epoch:      0 Step:  432.00000 Loss: 2.2619540691375732,\n Seq:     434 Epoch:      0 Step:  433.00000 Loss: 2.236504316329956,\n Seq:     435 Epoch:      0 Step:  434.00000 Loss: 2.223053455352783,\n Seq:     436 Epoch:      0 Step:  435.00000 Loss: 2.404879093170166,\n Seq:     437 Epoch:      0 Step:  436.00000 Loss: 2.375404119491577,\n Seq:     438 Epoch:      0 Step:  437.00000 Loss: 2.357712507247925,\n Seq:     439 Epoch:      0 Step:  438.00000 Loss: 2.3902854919433594,\n Seq:     440 Epoch:      0 Step:  439.00000 Loss: 2.2901580333709717,\n Seq:     441 Epoch:      0 Step:  440.00000 Loss: 2.4120359420776367,\n Seq:     442 Epoch:      0 Step:  441.00000 Loss: 2.2824814319610596,\n Seq:     443 Epoch:      0 Step:  442.00000 Loss: 2.2968029975891113,\n Seq:     444 Epoch:      0 Step:  443.00000 Loss: 2.173008441925049,\n Seq:     445 Epoch:      0 Step:  444.00000 Loss: 2.443586587905884,\n Seq:     446 Epoch:      0 Step:  445.00000 Loss: 2.357076406478882,\n Seq:     447 Epoch:      0 Step:  446.00000 Loss: 2.3221588134765625,\n Seq:     448 Epoch:      0 Step:  447.00000 Loss: 2.265385866165161,\n Seq:     449 Epoch:      0 Step:  448.00000 Loss: 2.230381965637207,\n Seq:     450 Epoch:      0 Step:  449.00000 Loss: 2.3562119007110596,\n Seq:     451 Epoch:      0 Step:  450.00000 Loss: 2.3757662773132324,\n Seq:     452 Epoch:      0 Step:  451.00000 Loss: 2.3155570030212402,\n Seq:     453 Epoch:      0 Step:  452.00000 Loss: 2.2981948852539062,\n Seq:     454 Epoch:      0 Step:  453.00000 Loss: 2.364980697631836,\n Seq:     455 Epoch:      0 Step:  454.00000 Loss: 2.3716089725494385,\n Seq:     456 Epoch:      0 Step:  455.00000 Loss: 2.2372539043426514,\n Seq:     457 Epoch:      0 Step:  456.00000 Loss: 2.4211935997009277,\n Seq:     458 Epoch:      0 Step:  457.00000 Loss: 2.336667060852051,\n Seq:     459 Epoch:      0 Step:  458.00000 Loss: 2.3215415477752686,\n Seq:     460 Epoch:      0 Step:  459.00000 Loss: 2.358266592025757,\n Seq:     461 Epoch:      0 Step:  460.00000 Loss: 2.356381893157959,\n Seq:     462 Epoch:      0 Step:  461.00000 Loss: 2.3231399059295654,\n Seq:     463 Epoch:      0 Step:  462.00000 Loss: 2.404381275177002,\n Seq:     464 Epoch:      0 Step:  463.00000 Loss: 2.3337929248809814,\n Seq:     465 Epoch:      0 Step:  464.00000 Loss: 2.3141915798187256,\n Seq:     466 Epoch:      0 Step:  465.00000 Loss: 2.319091558456421,\n Seq:     467 Epoch:      0 Step:  466.00000 Loss: 2.373950242996216,\n Seq:     468 Epoch:      0 Step:  467.00000 Loss: 2.3105313777923584,\n Seq:     469 Epoch:      0 Step:  468.00000 Loss: 2.3029091358184814,\n Seq:     470 Epoch:      0 Step:  469.00000 Loss: 2.294877290725708,\n Seq:     471 Epoch:      0 Step:  470.00000 Loss: 2.3040308952331543,\n Seq:     472 Epoch:      0 Step:  471.00000 Loss: 2.324889659881592,\n Seq:     473 Epoch:      0 Step:  472.00000 Loss: 2.3725383281707764,\n Seq:     474 Epoch:      0 Step:  473.00000 Loss: 2.2959423065185547,\n Seq:     475 Epoch:      0 Step:  474.00000 Loss: 2.309556245803833,\n Seq:     476 Epoch:      0 Step:  475.00000 Loss: 2.3823113441467285,\n Seq:     477 Epoch:      0 Step:  476.00000 Loss: 2.282299518585205,\n Seq:     478 Epoch:      0 Step:  477.00000 Loss: 2.26910662651062,\n Seq:     479 Epoch:      0 Step:  478.00000 Loss: 2.362061023712158,\n Seq:     480 Epoch:      0 Step:  479.00000 Loss: 2.25370717048645,\n Seq:     481 Epoch:      0 Step:  480.00000 Loss: 2.32006573677063,\n Seq:     482 Epoch:      0 Step:  481.00000 Loss: 2.283473253250122,\n Seq:     483 Epoch:      0 Step:  482.00000 Loss: 2.2334816455841064,\n Seq:     484 Epoch:      0 Step:  483.00000 Loss: 2.3114984035491943,\n Seq:     485 Epoch:      0 Step:  484.00000 Loss: 2.337798833847046,\n Seq:     486 Epoch:      0 Step:  485.00000 Loss: 2.3580939769744873,\n Seq:     487 Epoch:      0 Step:  486.00000 Loss: 2.3190126419067383,\n Seq:     488 Epoch:      0 Step:  487.00000 Loss: 2.3089799880981445,\n Seq:     489 Epoch:      0 Step:  488.00000 Loss: 2.2539925575256348,\n Seq:     490 Epoch:      0 Step:  489.00000 Loss: 2.239096164703369,\n Seq:     491 Epoch:      0 Step:  490.00000 Loss: 2.424072504043579,\n Seq:     492 Epoch:      0 Step:  491.00000 Loss: 2.2213118076324463,\n Seq:     493 Epoch:      0 Step:  492.00000 Loss: 2.329029083251953,\n Seq:     494 Epoch:      0 Step:  493.00000 Loss: 2.3630216121673584,\n Seq:     495 Epoch:      0 Step:  494.00000 Loss: 2.333735227584839,\n Seq:     496 Epoch:      0 Step:  495.00000 Loss: 2.37447190284729,\n Seq:     497 Epoch:      0 Step:  496.00000 Loss: 2.3484349250793457,\n Seq:     498 Epoch:      0 Step:  497.00000 Loss: 2.2735133171081543,\n Seq:     499 Epoch:      0 Step:  498.00000 Loss: 2.401606321334839,\n Seq:     500 Epoch:      0 Step:  499.00000 Loss: 2.272162914276123,\n Seq:     501 Epoch:      0 Step:  500.00000 Loss: 2.311379909515381,\n Seq:     502 Epoch:      0 Step:  501.00000 Loss: 2.3575005531311035,\n Seq:     503 Epoch:      0 Step:  502.00000 Loss: 2.2511730194091797,\n Seq:     504 Epoch:      0 Step:  503.00000 Loss: 2.3172671794891357,\n Seq:     505 Epoch:      0 Step:  504.00000 Loss: 2.419400691986084,\n Seq:     506 Epoch:      0 Step:  505.00000 Loss: 2.3884902000427246,\n Seq:     507 Epoch:      0 Step:  506.00000 Loss: 2.247075080871582,\n Seq:     508 Epoch:      0 Step:  507.00000 Loss: 2.344844341278076,\n Seq:     509 Epoch:      0 Step:  508.00000 Loss: 2.374392509460449,\n Seq:     510 Epoch:      0 Step:  509.00000 Loss: 2.337198257446289,\n Seq:     511 Epoch:      0 Step:  510.00000 Loss: 2.304396867752075,\n Seq:     512 Epoch:      0 Step:  511.00000 Loss: 2.323976755142212,\n Seq:     513 Epoch:      0 Step:  512.00000 Loss: 2.3402011394500732,\n Seq:     514 Epoch:      0 Step:  513.00000 Loss: 2.3207924365997314,\n Seq:     515 Epoch:      0 Step:  514.00000 Loss: 2.303927421569824,\n Seq:     516 Epoch:      0 Step:  515.00000 Loss: 2.302539587020874,\n Seq:     517 Epoch:      0 Step:  516.00000 Loss: 2.2700142860412598,\n Seq:     518 Epoch:      0 Step:  517.00000 Loss: 2.2815380096435547,\n Seq:     519 Epoch:      0 Step:  518.00000 Loss: 2.2891626358032227,\n Seq:     520 Epoch:      0 Step:  519.00000 Loss: 2.3559718132019043,\n Seq:     521 Epoch:      0 Step:  520.00000 Loss: 2.2850749492645264,\n Seq:     522 Epoch:      0 Step:  521.00000 Loss: 2.301414966583252,\n Seq:     523 Epoch:      0 Step:  522.00000 Loss: 2.306520700454712,\n Seq:     524 Epoch:      0 Step:  523.00000 Loss: 2.298090934753418,\n Seq:     525 Epoch:      0 Step:  524.00000 Loss: 2.267430067062378,\n Seq:     526 Epoch:      0 Step:  525.00000 Loss: 2.317793369293213,\n Seq:     527 Epoch:      0 Step:  526.00000 Loss: 2.336571455001831,\n Seq:     528 Epoch:      0 Step:  527.00000 Loss: 2.240582227706909,\n Seq:     529 Epoch:      0 Step:  528.00000 Loss: 2.367053747177124,\n Seq:     530 Epoch:      0 Step:  529.00000 Loss: 2.3031721115112305,\n Seq:     531 Epoch:      0 Step:  530.00000 Loss: 2.31937313079834,\n Seq:     532 Epoch:      0 Step:  531.00000 Loss: 2.4044220447540283,\n Seq:     533 Epoch:      0 Step:  532.00000 Loss: 2.3771121501922607,\n Seq:     534 Epoch:      0 Step:  533.00000 Loss: 2.3354732990264893,\n Seq:     535 Epoch:      0 Step:  534.00000 Loss: 2.286316394805908,\n Seq:     536 Epoch:      0 Step:  535.00000 Loss: 2.3310208320617676,\n Seq:     537 Epoch:      0 Step:  536.00000 Loss: 2.283254623413086,\n Seq:     538 Epoch:      0 Step:  537.00000 Loss: 2.376183032989502,\n Seq:     539 Epoch:      0 Step:  538.00000 Loss: 2.2389023303985596,\n Seq:     540 Epoch:      0 Step:  539.00000 Loss: 2.2884559631347656,\n Seq:     541 Epoch:      0 Step:  540.00000 Loss: 2.381955862045288,\n Seq:     542 Epoch:      0 Step:  541.00000 Loss: 2.309551239013672,\n Seq:     543 Epoch:      0 Step:  542.00000 Loss: 2.3375823497772217,\n Seq:     544 Epoch:      0 Step:  543.00000 Loss: 2.4351563453674316,\n Seq:     545 Epoch:      0 Step:  544.00000 Loss: 2.3742854595184326,\n Seq:     546 Epoch:      0 Step:  545.00000 Loss: 2.3321692943573,\n Seq:     547 Epoch:      0 Step:  546.00000 Loss: 2.317209243774414,\n Seq:     548 Epoch:      0 Step:  547.00000 Loss: 2.402087688446045,\n Seq:     549 Epoch:      0 Step:  548.00000 Loss: 2.2963006496429443,\n Seq:     550 Epoch:      0 Step:  549.00000 Loss: 2.334486484527588,\n Seq:     551 Epoch:      0 Step:  550.00000 Loss: 2.335634469985962,\n Seq:     552 Epoch:      0 Step:  551.00000 Loss: 2.2530596256256104,\n Seq:     553 Epoch:      0 Step:  552.00000 Loss: 2.3088724613189697,\n Seq:     554 Epoch:      0 Step:  553.00000 Loss: 2.3340871334075928,\n Seq:     555 Epoch:      0 Step:  554.00000 Loss: 2.308976888656616,\n Seq:     556 Epoch:      0 Step:  555.00000 Loss: 2.4044835567474365,\n Seq:     557 Epoch:      0 Step:  556.00000 Loss: 2.339257001876831,\n Seq:     558 Epoch:      0 Step:  557.00000 Loss: 2.3321681022644043,\n Seq:     559 Epoch:      0 Step:  558.00000 Loss: 2.2829833030700684,\n Seq:     560 Epoch:      0 Step:  559.00000 Loss: 2.3227999210357666,\n Seq:     561 Epoch:      0 Step:  560.00000 Loss: 2.290565252304077,\n Seq:     562 Epoch:      0 Step:  561.00000 Loss: 2.2913689613342285,\n Seq:     563 Epoch:      0 Step:  562.00000 Loss: 2.3173444271087646,\n Seq:     564 Epoch:      0 Step:  563.00000 Loss: 2.3158469200134277,\n Seq:     565 Epoch:      0 Step:  564.00000 Loss: 2.325927495956421,\n Seq:     566 Epoch:      0 Step:  565.00000 Loss: 2.2984814643859863,\n Seq:     567 Epoch:      0 Step:  566.00000 Loss: 2.364871025085449,\n Seq:     568 Epoch:      0 Step:  567.00000 Loss: 2.2646431922912598,\n Seq:     569 Epoch:      0 Step:  568.00000 Loss: 2.3025271892547607,\n Seq:     570 Epoch:      0 Step:  569.00000 Loss: 2.318679094314575,\n Seq:     571 Epoch:      0 Step:  570.00000 Loss: 2.3630118370056152,\n Seq:     572 Epoch:      0 Step:  571.00000 Loss: 2.3281779289245605,\n Seq:     573 Epoch:      0 Step:  572.00000 Loss: 2.3459084033966064,\n Seq:     574 Epoch:      0 Step:  573.00000 Loss: 2.3368213176727295,\n Seq:     575 Epoch:      0 Step:  574.00000 Loss: 2.2732977867126465,\n Seq:     576 Epoch:      0 Step:  575.00000 Loss: 2.3789329528808594,\n Seq:     577 Epoch:      0 Step:  576.00000 Loss: 2.31461238861084,\n Seq:     578 Epoch:      0 Step:  577.00000 Loss: 2.359997272491455,\n Seq:     579 Epoch:      0 Step:  578.00000 Loss: 2.3213770389556885,\n Seq:     580 Epoch:      0 Step:  579.00000 Loss: 2.3111798763275146,\n Seq:     581 Epoch:      0 Step:  580.00000 Loss: 2.3001346588134766,\n Seq:     582 Epoch:      0 Step:  581.00000 Loss: 2.3166913986206055,\n Seq:     583 Epoch:      0 Step:  582.00000 Loss: 2.3012664318084717,\n Seq:     584 Epoch:      0 Step:  583.00000 Loss: 2.320410966873169,\n Seq:     585 Epoch:      0 Step:  584.00000 Loss: 2.333071231842041,\n Seq:     586 Epoch:      0 Step:  585.00000 Loss: 2.3576931953430176,\n Seq:     587 Epoch:      0 Step:  586.00000 Loss: 2.2805824279785156,\n Seq:     588 Epoch:      0 Step:  587.00000 Loss: 2.3806569576263428,\n Seq:     589 Epoch:      0 Step:  588.00000 Loss: 2.2918684482574463,\n Seq:     590 Epoch:      0 Step:  589.00000 Loss: 2.2625491619110107,\n Seq:     591 Epoch:      0 Step:  590.00000 Loss: 2.2502927780151367,\n Seq:     592 Epoch:      0 Step:  591.00000 Loss: 2.257288694381714,\n Seq:     593 Epoch:      0 Step:  592.00000 Loss: 2.2498362064361572,\n Seq:     594 Epoch:      0 Step:  593.00000 Loss: 2.3349521160125732,\n Seq:     595 Epoch:      0 Step:  594.00000 Loss: 2.2938930988311768,\n Seq:     596 Epoch:      0 Step:  595.00000 Loss: 2.3923537731170654,\n Seq:     597 Epoch:      0 Step:  596.00000 Loss: 2.3467986583709717,\n Seq:     598 Epoch:      0 Step:  597.00000 Loss: 2.3792970180511475,\n Seq:     599 Epoch:      0 Step:  598.00000 Loss: 2.4340643882751465,\n Seq:     600 Epoch:      0 Step:  599.00000 Loss: 2.273212194442749,\n Seq:     601 Epoch:      0 Step:  600.00000 Loss: 2.4113667011260986,\n Seq:     602 Epoch:      0 Step:  601.00000 Loss: 2.3029568195343018,\n Seq:     603 Epoch:      0 Step:  602.00000 Loss: 2.2475786209106445,\n Seq:     604 Epoch:      0 Step:  603.00000 Loss: 2.313556432723999,\n Seq:     605 Epoch:      0 Step:  604.00000 Loss: 2.407296895980835,\n Seq:     606 Epoch:      0 Step:  605.00000 Loss: 2.356760263442993,\n Seq:     607 Epoch:      0 Step:  606.00000 Loss: 2.3728373050689697,\n Seq:     608 Epoch:      0 Step:  607.00000 Loss: 2.2630934715270996,\n Seq:     609 Epoch:      0 Step:  608.00000 Loss: 2.340778112411499,\n Seq:     610 Epoch:      0 Step:  609.00000 Loss: 2.350158929824829,\n Seq:     611 Epoch:      0 Step:  610.00000 Loss: 2.308035373687744,\n Seq:     612 Epoch:      0 Step:  611.00000 Loss: 2.3367555141448975,\n Seq:     613 Epoch:      0 Step:  612.00000 Loss: 2.2522215843200684,\n Seq:     614 Epoch:      0 Step:  613.00000 Loss: 2.277698040008545,\n Seq:     615 Epoch:      0 Step:  614.00000 Loss: 2.317049026489258,\n Seq:     616 Epoch:      0 Step:  615.00000 Loss: 2.3536462783813477,\n Seq:     617 Epoch:      0 Step:  616.00000 Loss: 2.36737322807312,\n Seq:     618 Epoch:      0 Step:  617.00000 Loss: 2.3074591159820557,\n Seq:     619 Epoch:      0 Step:  618.00000 Loss: 2.3506712913513184,\n Seq:     620 Epoch:      0 Step:  619.00000 Loss: 2.24985408782959,\n Seq:     621 Epoch:      0 Step:  620.00000 Loss: 2.310802698135376,\n Seq:     622 Epoch:      0 Step:  621.00000 Loss: 2.332376718521118,\n Seq:     623 Epoch:      0 Step:  622.00000 Loss: 2.378063201904297,\n Seq:     624 Epoch:      0 Step:  623.00000 Loss: 2.3820104598999023,\n Seq:     625 Epoch:      0 Step:  624.00000 Loss: 2.268812894821167,\n Seq:     626 Epoch:      0 Step:  625.00000 Loss: 2.3241498470306396,\n Seq:     627 Epoch:      0 Step:  626.00000 Loss: 2.266594886779785,\n Seq:     628 Epoch:      0 Step:  627.00000 Loss: 2.3510499000549316,\n Seq:     629 Epoch:      0 Step:  628.00000 Loss: 2.3642568588256836,\n Seq:     630 Epoch:      0 Step:  629.00000 Loss: 2.2354555130004883,\n Seq:     631 Epoch:      0 Step:  630.00000 Loss: 2.321213722229004,\n Seq:     632 Epoch:      0 Step:  631.00000 Loss: 2.318981409072876,\n Seq:     633 Epoch:      0 Step:  632.00000 Loss: 2.2778282165527344,\n Seq:     634 Epoch:      0 Step:  633.00000 Loss: 2.331899881362915,\n Seq:     635 Epoch:      0 Step:  634.00000 Loss: 2.2140519618988037,\n Seq:     636 Epoch:      0 Step:  635.00000 Loss: 2.2368688583374023,\n Seq:     637 Epoch:      0 Step:  636.00000 Loss: 2.443896770477295,\n Seq:     638 Epoch:      0 Step:  637.00000 Loss: 2.34163498878479,\n Seq:     639 Epoch:      0 Step:  638.00000 Loss: 2.3843400478363037,\n Seq:     640 Epoch:      0 Step:  639.00000 Loss: 2.3219518661499023,\n Seq:     641 Epoch:      0 Step:  640.00000 Loss: 2.2889974117279053,\n Seq:     642 Epoch:      0 Step:  641.00000 Loss: 2.293731212615967,\n Seq:     643 Epoch:      0 Step:  642.00000 Loss: 2.3313066959381104,\n Seq:     644 Epoch:      0 Step:  643.00000 Loss: 2.3772683143615723,\n Seq:     645 Epoch:      0 Step:  644.00000 Loss: 2.3033316135406494,\n Seq:     646 Epoch:      0 Step:  645.00000 Loss: 2.3506102561950684,\n Seq:     647 Epoch:      0 Step:  646.00000 Loss: 2.3494720458984375,\n Seq:     648 Epoch:      0 Step:  647.00000 Loss: 2.2411439418792725,\n Seq:     649 Epoch:      0 Step:  648.00000 Loss: 2.327633857727051,\n Seq:     650 Epoch:      0 Step:  649.00000 Loss: 2.3307082653045654,\n Seq:     651 Epoch:      0 Step:  650.00000 Loss: 2.2504255771636963,\n Seq:     652 Epoch:      0 Step:  651.00000 Loss: 2.3229711055755615,\n Seq:     653 Epoch:      0 Step:  652.00000 Loss: 2.309478759765625,\n Seq:     654 Epoch:      0 Step:  653.00000 Loss: 2.3212790489196777,\n Seq:     655 Epoch:      0 Step:  654.00000 Loss: 2.325782060623169,\n Seq:     656 Epoch:      0 Step:  655.00000 Loss: 2.3898208141326904,\n Seq:     657 Epoch:      0 Step:  656.00000 Loss: 2.3497347831726074,\n Seq:     658 Epoch:      0 Step:  657.00000 Loss: 2.2430713176727295,\n Seq:     659 Epoch:      0 Step:  658.00000 Loss: 2.259753942489624,\n Seq:     660 Epoch:      0 Step:  659.00000 Loss: 2.297274351119995,\n Seq:     661 Epoch:      0 Step:  660.00000 Loss: 2.3592488765716553,\n Seq:     662 Epoch:      0 Step:  661.00000 Loss: 2.271507978439331,\n Seq:     663 Epoch:      0 Step:  662.00000 Loss: 2.3886938095092773,\n Seq:     664 Epoch:      0 Step:  663.00000 Loss: 2.3076064586639404,\n Seq:     665 Epoch:      0 Step:  664.00000 Loss: 2.3576998710632324,\n Seq:     666 Epoch:      0 Step:  665.00000 Loss: 2.202969551086426,\n Seq:     667 Epoch:      0 Step:  666.00000 Loss: 2.3311893939971924,\n Seq:     668 Epoch:      0 Step:  667.00000 Loss: 2.4122138023376465,\n Seq:     669 Epoch:      0 Step:  668.00000 Loss: 2.1542112827301025,\n Seq:     670 Epoch:      0 Step:  669.00000 Loss: 2.398106336593628,\n Seq:     671 Epoch:      0 Step:  670.00000 Loss: 2.3194522857666016,\n Seq:     672 Epoch:      0 Step:  671.00000 Loss: 2.324700117111206,\n Seq:     673 Epoch:      0 Step:  672.00000 Loss: 2.402627468109131,\n Seq:     674 Epoch:      0 Step:  673.00000 Loss: 2.310946226119995,\n Seq:     675 Epoch:      0 Step:  674.00000 Loss: 2.233600616455078,\n Seq:     676 Epoch:      0 Step:  675.00000 Loss: 2.4268457889556885,\n Seq:     677 Epoch:      0 Step:  676.00000 Loss: 2.279269218444824,\n Seq:     678 Epoch:      0 Step:  677.00000 Loss: 2.2251484394073486,\n Seq:     679 Epoch:      0 Step:  678.00000 Loss: 2.3720688819885254,\n Seq:     680 Epoch:      0 Step:  679.00000 Loss: 2.4380624294281006,\n Seq:     681 Epoch:      0 Step:  680.00000 Loss: 2.3008198738098145,\n Seq:     682 Epoch:      0 Step:  681.00000 Loss: 2.2892062664031982,\n Seq:     683 Epoch:      0 Step:  682.00000 Loss: 2.3509416580200195,\n Seq:     684 Epoch:      0 Step:  683.00000 Loss: 2.3939404487609863,\n Seq:     685 Epoch:      0 Step:  684.00000 Loss: 2.3713643550872803,\n Seq:     686 Epoch:      0 Step:  685.00000 Loss: 2.196815252304077,\n Seq:     687 Epoch:      0 Step:  686.00000 Loss: 2.163590669631958,\n Seq:     688 Epoch:      0 Step:  687.00000 Loss: 2.2992234230041504,\n Seq:     689 Epoch:      0 Step:  688.00000 Loss: 2.3634583950042725,\n Seq:     690 Epoch:      0 Step:  689.00000 Loss: 2.356100082397461,\n Seq:     691 Epoch:      0 Step:  690.00000 Loss: 2.3014299869537354,\n Seq:     692 Epoch:      0 Step:  691.00000 Loss: 2.2723851203918457,\n Seq:     693 Epoch:      0 Step:  692.00000 Loss: 2.289198160171509,\n Seq:     694 Epoch:      0 Step:  693.00000 Loss: 2.316267490386963,\n Seq:     695 Epoch:      0 Step:  694.00000 Loss: 2.2972357273101807,\n Seq:     696 Epoch:      0 Step:  695.00000 Loss: 2.2783634662628174,\n Seq:     697 Epoch:      0 Step:  696.00000 Loss: 2.4230551719665527,\n Seq:     698 Epoch:      0 Step:  697.00000 Loss: 2.2354533672332764,\n Seq:     699 Epoch:      0 Step:  698.00000 Loss: 2.311762571334839,\n Seq:     700 Epoch:      0 Step:  699.00000 Loss: 2.2856974601745605,\n Seq:     701 Epoch:      0 Step:  700.00000 Loss: 2.3781843185424805,\n Seq:     702 Epoch:      0 Step:  701.00000 Loss: 2.3610873222351074,\n Seq:     703 Epoch:      0 Step:  702.00000 Loss: 2.2807250022888184,\n Seq:     704 Epoch:      0 Step:  703.00000 Loss: 2.28940749168396,\n Seq:     705 Epoch:      0 Step:  704.00000 Loss: 2.301412343978882,\n Seq:     706 Epoch:      0 Step:  705.00000 Loss: 2.3894643783569336,\n Seq:     707 Epoch:      0 Step:  706.00000 Loss: 2.3833372592926025,\n Seq:     708 Epoch:      0 Step:  707.00000 Loss: 2.3243963718414307,\n Seq:     709 Epoch:      0 Step:  708.00000 Loss: 2.267730951309204,\n Seq:     710 Epoch:      0 Step:  709.00000 Loss: 2.321230888366699,\n Seq:     711 Epoch:      0 Step:  710.00000 Loss: 2.278661012649536,\n Seq:     712 Epoch:      0 Step:  711.00000 Loss: 2.3833060264587402,\n Seq:     713 Epoch:      0 Step:  712.00000 Loss: 2.3194875717163086,\n Seq:     714 Epoch:      0 Step:  713.00000 Loss: 2.365993022918701,\n Seq:     715 Epoch:      0 Step:  714.00000 Loss: 2.224932909011841,\n Seq:     716 Epoch:      0 Step:  715.00000 Loss: 2.2062549591064453,\n Seq:     717 Epoch:      0 Step:  716.00000 Loss: 2.3291773796081543,\n Seq:     718 Epoch:      0 Step:  717.00000 Loss: 2.324244976043701,\n Seq:     719 Epoch:      0 Step:  718.00000 Loss: 2.2972238063812256,\n Seq:     720 Epoch:      0 Step:  719.00000 Loss: 2.264592409133911,\n Seq:     721 Epoch:      0 Step:  720.00000 Loss: 2.3406221866607666,\n Seq:     722 Epoch:      0 Step:  721.00000 Loss: 2.3727102279663086,\n Seq:     723 Epoch:      0 Step:  722.00000 Loss: 2.292067766189575,\n Seq:     724 Epoch:      0 Step:  723.00000 Loss: 2.1855502128601074,\n Seq:     725 Epoch:      0 Step:  724.00000 Loss: 2.4011294841766357,\n Seq:     726 Epoch:      0 Step:  725.00000 Loss: 2.3649775981903076,\n Seq:     727 Epoch:      0 Step:  726.00000 Loss: 2.3625152111053467,\n Seq:     728 Epoch:      0 Step:  727.00000 Loss: 2.4645984172821045,\n Seq:     729 Epoch:      0 Step:  728.00000 Loss: 2.4597599506378174,\n Seq:     730 Epoch:      0 Step:  729.00000 Loss: 2.2974839210510254,\n Seq:     731 Epoch:      0 Step:  730.00000 Loss: 2.3061411380767822,\n Seq:     732 Epoch:      0 Step:  731.00000 Loss: 2.3286755084991455,\n Seq:     733 Epoch:      0 Step:  732.00000 Loss: 2.2938232421875,\n Seq:     734 Epoch:      0 Step:  733.00000 Loss: 2.2803866863250732,\n Seq:     735 Epoch:      0 Step:  734.00000 Loss: 2.297252893447876,\n Seq:     736 Epoch:      0 Step:  735.00000 Loss: 2.260101795196533,\n Seq:     737 Epoch:      0 Step:  736.00000 Loss: 2.2702646255493164,\n Seq:     738 Epoch:      0 Step:  737.00000 Loss: 2.3436813354492188,\n Seq:     739 Epoch:      0 Step:  738.00000 Loss: 2.3695969581604004,\n Seq:     740 Epoch:      0 Step:  739.00000 Loss: 2.3346006870269775,\n Seq:     741 Epoch:      0 Step:  740.00000 Loss: 2.322385787963867,\n Seq:     742 Epoch:      0 Step:  741.00000 Loss: 2.368616819381714,\n Seq:     743 Epoch:      0 Step:  742.00000 Loss: 2.329843759536743,\n Seq:     744 Epoch:      0 Step:  743.00000 Loss: 2.2407116889953613,\n Seq:     745 Epoch:      0 Step:  744.00000 Loss: 2.3922860622406006,\n Seq:     746 Epoch:      0 Step:  745.00000 Loss: 2.222472667694092,\n Seq:     747 Epoch:      0 Step:  746.00000 Loss: 2.4467251300811768,\n Seq:     748 Epoch:      0 Step:  747.00000 Loss: 2.2525641918182373,\n Seq:     749 Epoch:      0 Step:  748.00000 Loss: 2.365666151046753,\n Seq:     750 Epoch:      0 Step:  749.00000 Loss: 2.3554868698120117,\n Seq:     751 Epoch:      0 Step:  750.00000 Loss: 2.304478406906128,\n Seq:     752 Epoch:      0 Step:  751.00000 Loss: 2.361461639404297,\n Seq:     753 Epoch:      0 Step:  752.00000 Loss: 2.3190932273864746,\n Seq:     754 Epoch:      0 Step:  753.00000 Loss: 2.2561757564544678,\n Seq:     755 Epoch:      0 Step:  754.00000 Loss: 2.3250865936279297,\n Seq:     756 Epoch:      0 Step:  755.00000 Loss: 2.34574294090271,\n Seq:     757 Epoch:      0 Step:  756.00000 Loss: 2.205000400543213,\n Seq:     758 Epoch:      0 Step:  757.00000 Loss: 2.296844959259033,\n Seq:     759 Epoch:      0 Step:  758.00000 Loss: 2.307234048843384,\n Seq:     760 Epoch:      0 Step:  759.00000 Loss: 2.4644203186035156,\n Seq:     761 Epoch:      0 Step:  760.00000 Loss: 2.3487818241119385,\n Seq:     762 Epoch:      0 Step:  761.00000 Loss: 2.449585437774658,\n Seq:     763 Epoch:      0 Step:  762.00000 Loss: 2.2424237728118896,\n Seq:     764 Epoch:      0 Step:  763.00000 Loss: 2.3457818031311035,\n Seq:     765 Epoch:      0 Step:  764.00000 Loss: 2.3208656311035156,\n Seq:     766 Epoch:      0 Step:  765.00000 Loss: 2.3640782833099365,\n Seq:     767 Epoch:      0 Step:  766.00000 Loss: 2.3027260303497314,\n Seq:     768 Epoch:      0 Step:  767.00000 Loss: 2.265874147415161,\n Seq:     769 Epoch:      0 Step:  768.00000 Loss: 2.2548818588256836,\n Seq:     770 Epoch:      0 Step:  769.00000 Loss: 2.3779261112213135,\n Seq:     771 Epoch:      0 Step:  770.00000 Loss: 2.305335521697998,\n Seq:     772 Epoch:      0 Step:  771.00000 Loss: 2.3618392944335938,\n Seq:     773 Epoch:      0 Step:  772.00000 Loss: 2.347015619277954,\n Seq:     774 Epoch:      0 Step:  773.00000 Loss: 2.286919593811035,\n Seq:     775 Epoch:      0 Step:  774.00000 Loss: 2.3537728786468506,\n Seq:     776 Epoch:      0 Step:  775.00000 Loss: 2.325620174407959,\n Seq:     777 Epoch:      0 Step:  776.00000 Loss: 2.2806568145751953,\n Seq:     778 Epoch:      0 Step:  777.00000 Loss: 2.3691301345825195,\n Seq:     779 Epoch:      0 Step:  778.00000 Loss: 2.311021327972412,\n Seq:     780 Epoch:      0 Step:  779.00000 Loss: 2.3475797176361084,\n Seq:     781 Epoch:      0 Step:  780.00000 Loss: 2.376826763153076,\n Seq:     782 Epoch:      0 Step:  781.00000 Loss: 2.346123218536377,\n Seq:     783 Epoch:      0 Step:  782.00000 Loss: 2.301800012588501,\n Seq:     784 Epoch:      0 Step:  783.00000 Loss: 2.400064706802368,\n Seq:     785 Epoch:      0 Step:  784.00000 Loss: 2.273674249649048,\n Seq:     786 Epoch:      0 Step:  785.00000 Loss: 2.321586847305298,\n Seq:     787 Epoch:      0 Step:  786.00000 Loss: 2.3238885402679443,\n Seq:     788 Epoch:      0 Step:  787.00000 Loss: 2.3049583435058594,\n Seq:     789 Epoch:      0 Step:  788.00000 Loss: 2.3889384269714355,\n Seq:     790 Epoch:      0 Step:  789.00000 Loss: 2.238844156265259,\n Seq:     791 Epoch:      0 Step:  790.00000 Loss: 2.2620439529418945,\n Seq:     792 Epoch:      0 Step:  791.00000 Loss: 2.2574894428253174,\n Seq:     793 Epoch:      0 Step:  792.00000 Loss: 2.4286959171295166,\n Seq:     794 Epoch:      0 Step:  793.00000 Loss: 2.274413824081421,\n Seq:     795 Epoch:      0 Step:  794.00000 Loss: 2.2945499420166016,\n Seq:     796 Epoch:      0 Step:  795.00000 Loss: 2.258948802947998,\n Seq:     797 Epoch:      0 Step:  796.00000 Loss: 2.3686461448669434,\n Seq:     798 Epoch:      0 Step:  797.00000 Loss: 2.4495935440063477,\n Seq:     799 Epoch:      0 Step:  798.00000 Loss: 2.312696695327759,\n Seq:     800 Epoch:      0 Step:  799.00000 Loss: 2.318452835083008,\n Seq:     801 Epoch:      0 Step:  800.00000 Loss: 2.2731926441192627,\n Seq:     802 Epoch:      0 Step:  801.00000 Loss: 2.3219611644744873,\n Seq:     803 Epoch:      0 Step:  802.00000 Loss: 2.3309824466705322,\n Seq:     804 Epoch:      0 Step:  803.00000 Loss: 2.325690984725952,\n Seq:     805 Epoch:      0 Step:  804.00000 Loss: 2.248471260070801,\n Seq:     806 Epoch:      0 Step:  805.00000 Loss: 2.428412914276123,\n Seq:     807 Epoch:      0 Step:  806.00000 Loss: 2.2489333152770996,\n Seq:     808 Epoch:      0 Step:  807.00000 Loss: 2.313539505004883,\n Seq:     809 Epoch:      0 Step:  808.00000 Loss: 2.2930614948272705,\n Seq:     810 Epoch:      0 Step:  809.00000 Loss: 2.360140323638916,\n Seq:     811 Epoch:      0 Step:  810.00000 Loss: 2.315772294998169,\n Seq:     812 Epoch:      0 Step:  811.00000 Loss: 2.311431407928467,\n Seq:     813 Epoch:      0 Step:  812.00000 Loss: 2.3218486309051514,\n Seq:     814 Epoch:      0 Step:  813.00000 Loss: 2.306762933731079,\n Seq:     815 Epoch:      0 Step:  814.00000 Loss: 2.310743808746338,\n Seq:     816 Epoch:      0 Step:  815.00000 Loss: 2.3967349529266357,\n Seq:     817 Epoch:      0 Step:  816.00000 Loss: 2.311882495880127,\n Seq:     818 Epoch:      0 Step:  817.00000 Loss: 2.2959344387054443,\n Seq:     819 Epoch:      0 Step:  818.00000 Loss: 2.335303783416748,\n Seq:     820 Epoch:      0 Step:  819.00000 Loss: 2.329216241836548,\n Seq:     821 Epoch:      0 Step:  820.00000 Loss: 2.283190965652466,\n Seq:     822 Epoch:      0 Step:  821.00000 Loss: 2.310218334197998,\n Seq:     823 Epoch:      0 Step:  822.00000 Loss: 2.3233695030212402,\n Seq:     824 Epoch:      0 Step:  823.00000 Loss: 2.2885823249816895,\n Seq:     825 Epoch:      0 Step:  824.00000 Loss: 2.356933116912842,\n Seq:     826 Epoch:      0 Step:  825.00000 Loss: 2.286302089691162,\n Seq:     827 Epoch:      0 Step:  826.00000 Loss: 2.2957053184509277,\n Seq:     828 Epoch:      0 Step:  827.00000 Loss: 2.294313669204712,\n Seq:     829 Epoch:      0 Step:  828.00000 Loss: 2.320068359375,\n Seq:     830 Epoch:      0 Step:  829.00000 Loss: 2.288982391357422,\n Seq:     831 Epoch:      0 Step:  830.00000 Loss: 2.277966022491455,\n Seq:     832 Epoch:      0 Step:  831.00000 Loss: 2.3049821853637695,\n Seq:     833 Epoch:      0 Step:  832.00000 Loss: 2.36318039894104,\n Seq:     834 Epoch:      0 Step:  833.00000 Loss: 2.2926485538482666,\n Seq:     835 Epoch:      0 Step:  834.00000 Loss: 2.286970615386963,\n Seq:     836 Epoch:      0 Step:  835.00000 Loss: 2.332367420196533,\n Seq:     837 Epoch:      0 Step:  836.00000 Loss: 2.249877452850342,\n Seq:     838 Epoch:      0 Step:  837.00000 Loss: 2.3420257568359375,\n Seq:     839 Epoch:      0 Step:  838.00000 Loss: 2.3257343769073486,\n Seq:     840 Epoch:      0 Step:  839.00000 Loss: 2.322715997695923,\n Seq:     841 Epoch:      0 Step:  840.00000 Loss: 2.2797207832336426,\n Seq:     842 Epoch:      0 Step:  841.00000 Loss: 2.3309388160705566,\n Seq:     843 Epoch:      0 Step:  842.00000 Loss: 2.2902028560638428,\n Seq:     844 Epoch:      0 Step:  843.00000 Loss: 2.3744356632232666,\n Seq:     845 Epoch:      0 Step:  844.00000 Loss: 2.2833714485168457,\n Seq:     846 Epoch:      0 Step:  845.00000 Loss: 2.3052029609680176,\n Seq:     847 Epoch:      0 Step:  846.00000 Loss: 2.328714370727539,\n Seq:     848 Epoch:      0 Step:  847.00000 Loss: 2.3161537647247314,\n Seq:     849 Epoch:      0 Step:  848.00000 Loss: 2.3075568675994873,\n Seq:     850 Epoch:      0 Step:  849.00000 Loss: 2.3516385555267334,\n Seq:     851 Epoch:      0 Step:  850.00000 Loss: 2.263519525527954,\n Seq:     852 Epoch:      0 Step:  851.00000 Loss: 2.346979856491089,\n Seq:     853 Epoch:      0 Step:  852.00000 Loss: 2.364389181137085,\n Seq:     854 Epoch:      0 Step:  853.00000 Loss: 2.3068015575408936,\n Seq:     855 Epoch:      0 Step:  854.00000 Loss: 2.2699105739593506,\n Seq:     856 Epoch:      0 Step:  855.00000 Loss: 2.35257625579834,\n Seq:     857 Epoch:      0 Step:  856.00000 Loss: 2.3182790279388428,\n Seq:     858 Epoch:      0 Step:  857.00000 Loss: 2.3516383171081543,\n Seq:     859 Epoch:      0 Step:  858.00000 Loss: 2.3568673133850098,\n Seq:     860 Epoch:      0 Step:  859.00000 Loss: 2.299177646636963,\n Seq:     861 Epoch:      0 Step:  860.00000 Loss: 2.3274052143096924,\n Seq:     862 Epoch:      0 Step:  861.00000 Loss: 2.2798144817352295,\n Seq:     863 Epoch:      0 Step:  862.00000 Loss: 2.346151351928711,\n Seq:     864 Epoch:      0 Step:  863.00000 Loss: 2.351175546646118,\n Seq:     865 Epoch:      0 Step:  864.00000 Loss: 2.300804376602173,\n Seq:     866 Epoch:      0 Step:  865.00000 Loss: 2.2630982398986816,\n Seq:     867 Epoch:      0 Step:  866.00000 Loss: 2.3009417057037354,\n Seq:     868 Epoch:      0 Step:  867.00000 Loss: 2.3178250789642334,\n Seq:     869 Epoch:      0 Step:  868.00000 Loss: 2.3661720752716064,\n Seq:     870 Epoch:      0 Step:  869.00000 Loss: 2.3305306434631348,\n Seq:     871 Epoch:      0 Step:  870.00000 Loss: 2.2601592540740967,\n Seq:     872 Epoch:      0 Step:  871.00000 Loss: 2.289050340652466,\n Seq:     873 Epoch:      0 Step:  872.00000 Loss: 2.2743115425109863,\n Seq:     874 Epoch:      0 Step:  873.00000 Loss: 2.379342794418335,\n Seq:     875 Epoch:      0 Step:  874.00000 Loss: 2.2935214042663574,\n Seq:     876 Epoch:      0 Step:  875.00000 Loss: 2.305494785308838,\n Seq:     877 Epoch:      0 Step:  876.00000 Loss: 2.3237197399139404,\n Seq:     878 Epoch:      0 Step:  877.00000 Loss: 2.282799482345581,\n Seq:     879 Epoch:      0 Step:  878.00000 Loss: 2.3534317016601562,\n Seq:     880 Epoch:      0 Step:  879.00000 Loss: 2.2763097286224365,\n Seq:     881 Epoch:      0 Step:  880.00000 Loss: 2.3066132068634033,\n Seq:     882 Epoch:      0 Step:  881.00000 Loss: 2.3510732650756836,\n Seq:     883 Epoch:      0 Step:  882.00000 Loss: 2.4161996841430664,\n Seq:     884 Epoch:      0 Step:  883.00000 Loss: 2.2989628314971924,\n Seq:     885 Epoch:      0 Step:  884.00000 Loss: 2.3502440452575684,\n Seq:     886 Epoch:      0 Step:  885.00000 Loss: 2.320108413696289,\n Seq:     887 Epoch:      0 Step:  886.00000 Loss: 2.264491319656372,\n Seq:     888 Epoch:      0 Step:  887.00000 Loss: 2.31445050239563,\n Seq:     889 Epoch:      0 Step:  888.00000 Loss: 2.3467209339141846,\n Seq:     890 Epoch:      0 Step:  889.00000 Loss: 2.3742783069610596,\n Seq:     891 Epoch:      0 Step:  890.00000 Loss: 2.283759593963623,\n Seq:     892 Epoch:      0 Step:  891.00000 Loss: 2.3587863445281982,\n Seq:     893 Epoch:      0 Step:  892.00000 Loss: 2.277632236480713,\n Seq:     894 Epoch:      0 Step:  893.00000 Loss: 2.263671398162842,\n Seq:     895 Epoch:      0 Step:  894.00000 Loss: 2.340040683746338,\n Seq:     896 Epoch:      0 Step:  895.00000 Loss: 2.2698516845703125,\n Seq:     897 Epoch:      0 Step:  896.00000 Loss: 2.354165554046631,\n Seq:     898 Epoch:      0 Step:  897.00000 Loss: 2.311290979385376,\n Seq:     899 Epoch:      0 Step:  898.00000 Loss: 2.3038954734802246,\n Seq:     900 Epoch:      0 Step:  899.00000 Loss: 2.323265314102173,\n Seq:     901 Epoch:      0 Step:  900.00000 Loss: 2.323441505432129,\n Seq:     902 Epoch:      0 Step:  901.00000 Loss: 2.2495856285095215,\n Seq:     903 Epoch:      0 Step:  902.00000 Loss: 2.34730863571167,\n Seq:     904 Epoch:      0 Step:  903.00000 Loss: 2.3284952640533447,\n Seq:     905 Epoch:      0 Step:  904.00000 Loss: 2.3050572872161865,\n Seq:     906 Epoch:      0 Step:  905.00000 Loss: 2.373753070831299,\n Seq:     907 Epoch:      0 Step:  906.00000 Loss: 2.368382453918457,\n Seq:     908 Epoch:      0 Step:  907.00000 Loss: 2.3133225440979004,\n Seq:     909 Epoch:      0 Step:  908.00000 Loss: 2.3302013874053955,\n Seq:     910 Epoch:      0 Step:  909.00000 Loss: 2.329160213470459,\n Seq:     911 Epoch:      0 Step:  910.00000 Loss: 2.277186632156372,\n Seq:     912 Epoch:      0 Step:  911.00000 Loss: 2.262392282485962,\n Seq:     913 Epoch:      0 Step:  912.00000 Loss: 2.3345019817352295,\n Seq:     914 Epoch:      0 Step:  913.00000 Loss: 2.2904951572418213,\n Seq:     915 Epoch:      0 Step:  914.00000 Loss: 2.295562505722046,\n Seq:     916 Epoch:      0 Step:  915.00000 Loss: 2.3344967365264893,\n Seq:     917 Epoch:      0 Step:  916.00000 Loss: 2.2673654556274414,\n Seq:     918 Epoch:      0 Step:  917.00000 Loss: 2.3427648544311523,\n Seq:     919 Epoch:      0 Step:  918.00000 Loss: 2.2965683937072754,\n Seq:     920 Epoch:      0 Step:  919.00000 Loss: 2.305402994155884,\n Seq:     921 Epoch:      0 Step:  920.00000 Loss: 2.2935690879821777,\n Seq:     922 Epoch:      0 Step:  921.00000 Loss: 2.333491802215576,\n Seq:     923 Epoch:      0 Step:  922.00000 Loss: 2.320338726043701,\n Seq:     924 Epoch:      0 Step:  923.00000 Loss: 2.2739334106445312,\n Seq:     925 Epoch:      0 Step:  924.00000 Loss: 2.3682100772857666,\n Seq:     926 Epoch:      0 Step:  925.00000 Loss: 2.3481223583221436,\n Seq:     927 Epoch:      0 Step:  926.00000 Loss: 2.3349416255950928,\n Seq:     928 Epoch:      0 Step:  927.00000 Loss: 2.236359119415283,\n Seq:     929 Epoch:      0 Step:  928.00000 Loss: 2.3516900539398193,\n Seq:     930 Epoch:      0 Step:  929.00000 Loss: 2.327564239501953,\n Seq:     931 Epoch:      0 Step:  930.00000 Loss: 2.3858120441436768,\n Seq:     932 Epoch:      0 Step:  931.00000 Loss: 2.289167642593384,\n Seq:     933 Epoch:      0 Step:  932.00000 Loss: 2.337083339691162,\n Seq:     934 Epoch:      0 Step:  933.00000 Loss: 2.2783308029174805,\n Seq:     935 Epoch:      0 Step:  934.00000 Loss: 2.28843355178833,\n Seq:     936 Epoch:      0 Step:  935.00000 Loss: 2.374099016189575,\n Seq:     937 Epoch:      0 Step:  936.00000 Loss: 2.264341115951538,\n Seq:     938 Epoch:      0 Step:  937.00000 Loss: 2.3871383666992188,\n Seq:     939 Epoch:      0 Step:  938.00000 Loss: 2.3942906856536865,\n Seq:     940 Epoch:      0 Step:  939.00000 Loss: 2.294750690460205,\n Seq:     941 Epoch:      0 Step:  940.00000 Loss: 2.344998836517334,\n Seq:     942 Epoch:      0 Step:  941.00000 Loss: 2.299746513366699,\n Seq:     943 Epoch:      0 Step:  942.00000 Loss: 2.3172059059143066,\n Seq:     944 Epoch:      0 Step:  943.00000 Loss: 2.2917211055755615,\n Seq:     945 Epoch:      0 Step:  944.00000 Loss: 2.2779369354248047,\n Seq:     946 Epoch:      0 Step:  945.00000 Loss: 2.3051106929779053,\n Seq:     947 Epoch:      0 Step:  946.00000 Loss: 2.330051898956299,\n Seq:     948 Epoch:      0 Step:  947.00000 Loss: 2.282418966293335,\n Seq:     949 Epoch:      0 Step:  948.00000 Loss: 2.2819905281066895,\n Seq:     950 Epoch:      0 Step:  949.00000 Loss: 2.26621413230896,\n Seq:     951 Epoch:      0 Step:  950.00000 Loss: 2.427276372909546,\n Seq:     952 Epoch:      0 Step:  951.00000 Loss: 2.2932395935058594,\n Seq:     953 Epoch:      0 Step:  952.00000 Loss: 2.304954767227173,\n Seq:     954 Epoch:      0 Step:  953.00000 Loss: 2.3708672523498535,\n Seq:     955 Epoch:      0 Step:  954.00000 Loss: 2.314338445663452,\n Seq:     956 Epoch:      0 Step:  955.00000 Loss: 2.288275718688965,\n Seq:     957 Epoch:      0 Step:  956.00000 Loss: 2.3501617908477783,\n Seq:     958 Epoch:      0 Step:  957.00000 Loss: 2.2729623317718506,\n Seq:     959 Epoch:      0 Step:  958.00000 Loss: 2.3344478607177734,\n Seq:     960 Epoch:      0 Step:  959.00000 Loss: 2.486445426940918,\n Seq:     961 Epoch:      0 Step:  960.00000 Loss: 2.383640766143799,\n Seq:     962 Epoch:      0 Step:  961.00000 Loss: 2.307065010070801,\n Seq:     963 Epoch:      0 Step:  962.00000 Loss: 2.388091802597046,\n Seq:     964 Epoch:      0 Step:  963.00000 Loss: 2.2540056705474854,\n Seq:     965 Epoch:      0 Step:  964.00000 Loss: 2.313660144805908,\n Seq:     966 Epoch:      0 Step:  965.00000 Loss: 2.2980408668518066,\n Seq:     967 Epoch:      0 Step:  966.00000 Loss: 2.2349045276641846,\n Seq:     968 Epoch:      0 Step:  967.00000 Loss: 2.2588744163513184,\n Seq:     969 Epoch:      0 Step:  968.00000 Loss: 2.290590524673462,\n Seq:     970 Epoch:      0 Step:  969.00000 Loss: 2.3154592514038086,\n Seq:     971 Epoch:      0 Step:  970.00000 Loss: 2.3716750144958496,\n Seq:     972 Epoch:      0 Step:  971.00000 Loss: 2.274895191192627,\n Seq:     973 Epoch:      0 Step:  972.00000 Loss: 2.3228626251220703,\n Seq:     974 Epoch:      0 Step:  973.00000 Loss: 2.2711079120635986,\n Seq:     975 Epoch:      0 Step:  974.00000 Loss: 2.3246326446533203,\n Seq:     976 Epoch:      0 Step:  975.00000 Loss: 2.3350367546081543,\n Seq:     977 Epoch:      0 Step:  976.00000 Loss: 2.314842462539673,\n Seq:     978 Epoch:      0 Step:  977.00000 Loss: 2.3454842567443848,\n Seq:     979 Epoch:      0 Step:  978.00000 Loss: 2.346597671508789,\n Seq:     980 Epoch:      0 Step:  979.00000 Loss: 2.40559458732605,\n Seq:     981 Epoch:      0 Step:  980.00000 Loss: 2.3279943466186523,\n Seq:     982 Epoch:      0 Step:  981.00000 Loss: 2.2755751609802246,\n Seq:     983 Epoch:      0 Step:  982.00000 Loss: 2.2800447940826416,\n Seq:     984 Epoch:      0 Step:  983.00000 Loss: 2.3222999572753906,\n Seq:     985 Epoch:      0 Step:  984.00000 Loss: 2.250641345977783,\n Seq:     986 Epoch:      0 Step:  985.00000 Loss: 2.3607261180877686,\n Seq:     987 Epoch:      0 Step:  986.00000 Loss: 2.301537275314331,\n Seq:     988 Epoch:      0 Step:  987.00000 Loss: 2.397785186767578,\n Seq:     989 Epoch:      0 Step:  988.00000 Loss: 2.363307476043701,\n Seq:     990 Epoch:      0 Step:  989.00000 Loss: 2.3170971870422363,\n Seq:     991 Epoch:      0 Step:  990.00000 Loss: 2.364078998565674,\n Seq:     992 Epoch:      0 Step:  991.00000 Loss: 2.2844274044036865,\n Seq:     993 Epoch:      0 Step:  992.00000 Loss: 2.2756588459014893,\n Seq:     994 Epoch:      0 Step:  993.00000 Loss: 2.3733978271484375,\n Seq:     995 Epoch:      0 Step:  994.00000 Loss: 2.3425323963165283,\n Seq:     996 Epoch:      0 Step:  995.00000 Loss: 2.298971176147461,\n Seq:     997 Epoch:      0 Step:  996.00000 Loss: 2.3115434646606445,\n Seq:     998 Epoch:      0 Step:  997.00000 Loss: 2.2031972408294678,\n Seq:     999 Epoch:      0 Step:  998.00000 Loss: 2.2798397541046143,\n Seq:    1000 Epoch:      0 Step:  999.00000 Loss: 2.2385897636413574,\n ...]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.with_target_labels(classifier.LABELS)\n",
    "classifier.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:01:59.523577147Z",
     "start_time": "2023-06-09T21:01:59.460243117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cat ==> ship\n",
      "    ship ==> ship\n",
      "    ship ==> ship\n",
      "   plane ==> ship\n",
      "    frog ==> ship\n",
      "    frog ==> ship\n",
      "     car ==> ship\n",
      "    frog ==> ship\n",
      "     cat ==> ship\n",
      "     car ==> ship\n",
      "   plane ==> ship\n",
      "   truck ==> ship\n",
      "     dog ==> ship\n",
      "   horse ==> ship\n",
      "   truck ==> ship\n",
      "    ship ==> ship\n",
      "There were 13 errors in prediction, out of 16 test samples. Accuracy: 0.1875\n"
     ]
    }
   ],
   "source": [
    "# Now let us get a mini-batch from test-data\n",
    "data_iter = iter(classifier.test_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Let us make predictions on these.\n",
    "predictions = classifier.predict(images.to(classifier.device))\n",
    "original_labels = [classifier.LABELS[label] for label in labels]\n",
    "\n",
    "# initialize error and total count.\n",
    "count = errors = 0\n",
    "\n",
    "for y, y_hat in zip(original_labels, predictions):\n",
    "    print(f'{y:>8s} ==> {y_hat}')\n",
    "    count += 1\n",
    "    if y != y_hat:\n",
    "        errors += 1\n",
    "print (f'There were {errors} errors in prediction, out of {count} test samples. Accuracy: { (count - errors)/count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model to a file\n",
    "\n",
    "Once a model has been trained, it is a valuable asset that should be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:02:07.662809986Z",
     "start_time": "2023-06-09T21:02:07.657973619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the classifier model to the file: {file_path}\n",
      "Saved the model to the file: /tmp/cifar-model.txt\n"
     ]
    }
   ],
   "source": [
    "# Save and Load\n",
    "file_name = '/tmp/cifar-model.txt'\n",
    "classifier.save(file_name)\n",
    "print(f'Saved the model to the file: {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model at inference time\n",
    "\n",
    "When it is inference time, for example, in a production deployment environment, it is time to load the pre-trained model, and use it for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T21:02:11.679127Z",
     "start_time": "2023-06-09T21:02:10.616555625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, going to load it back from the file: /tmp/cifar-model.txt\n",
      "The device on which the computations will happen is: cuda. Hopefully CUDA?\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No neural-net provided to the classifier yet!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Populate the underlying network with parameters from the file: /tmp/cifar-model.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ClassifierBase.create_optimizer() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNow, going to load it back from the file: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m loaded_classifier \u001B[38;5;241m=\u001B[39m Cifar10Classifier()\n\u001B[0;32m----> 3\u001B[0m \u001B[43mloaded_classifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSimpleCifarNet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m;\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(loaded_classifier)\n",
      "File \u001B[0;32m~/GitHub/neural-architectures/svlearn/common/base_classifier.py:235\u001B[0m, in \u001B[0;36mClassifierBase.load\u001B[0;34m(self, file_path, network, lr, momentum)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork \u001B[38;5;241m=\u001B[39m network\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mload_state_dict(state_dict)\n\u001B[0;32m--> 235\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_optimizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: ClassifierBase.create_optimizer() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "print(f'Now, going to load it back from the file: {file_name}')\n",
    "loaded_classifier = Cifar10Classifier()\n",
    "loaded_classifier.load(file_name, SimpleCifarNet());\n",
    "print(loaded_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "#### Source Code Reading\n",
    "\n",
    "Read the python module cifar10.py, and see if you can get a general sense of the code. At this moment, we have not covered the theory, so most things will look unfamiliar. In a couple of weeks, this will all begin to look very easy.\n",
    "\n",
    "#### Changing the hyper-parameters\n",
    "\n",
    "In this exercise we will see how the training accuracy responds to the change of various hyperparameters of the classifier, and the number of epochs of training.\n",
    "\n",
    "#### More epochs\n",
    "\n",
    "See what happens if you increase the number of epochs from 1 to a higher number in the code:\n",
    "\n",
    "``classifier.train(epochs=1)``\n",
    "\n",
    "\n",
    "We will understand the concept of epochs next week. For the time being, simply treat it as something you can tweak.\n",
    "\n",
    "* Does it improve the accuracy? \n",
    "* How far does accuracy increase with more epochs? \n",
    "* What happens when you train the classifier for ten epochs? \n",
    "* What inference do your draw from this?\n",
    "\n",
    "#### What happens when you change the learning rate in the Cifar10Classifier constructor?\n",
    "\n",
    "What happens if you give the classifier a different learning rate, say 0.1? Play around with different learning rate, and plot a graph of accuracy vs learning rate.\n",
    "\n",
    "#### What happens if you change the mini_batch_size in the Cifar10Classifier constructor?\n",
    "\n",
    "See how the accuracy responds to change in the mini-batch size (colloqually referred to simply as the batch size). Plot out a graph of mini-batch size vs accuracy, keeping the epochs to say, epochs = 3 or 2.\n",
    "\n",
    "#### [Challenging] Tweaking the underlying neural-network\n",
    "\n",
    "See if you can tweak the SimpleCifarNet class so that you get a better accuracy of prediction. This may be a hard task at the moment, since we have not yet learned about Convolutional neural networks, and how to build them carefully. So it may be an exercise in blind trial and error: still it is worth trying out to see if we can bring out an improvement in accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss!\n",
    "Please discuss your findings on the forum for the Lab forum, in the educational portal. This is an opportunity to learn from others, and help others learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T21:02:48.683860732Z",
     "start_time": "2023-06-09T21:02:17.045823565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device on which the computations will happen is: cuda. Hopefully CUDA?\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,   100] loss: 2.181\n",
      "[1,   200] loss: 1.949\n",
      "[1,   300] loss: 1.842\n",
      "[1,   400] loss: 1.742\n",
      "[1,   500] loss: 1.681\n",
      "[1,   600] loss: 1.629\n",
      "[1,   700] loss: 1.607\n",
      "[1,   800] loss: 1.626\n",
      "[1,   900] loss: 1.547\n",
      "[1,  1000] loss: 1.554\n",
      "[1,  1100] loss: 1.580\n",
      "[1,  1200] loss: 1.515\n",
      "[1,  1300] loss: 1.558\n",
      "[1,  1400] loss: 1.466\n",
      "[1,  1500] loss: 1.485\n",
      "[1,  1600] loss: 1.478\n",
      "[1,  1700] loss: 1.457\n",
      "[1,  1800] loss: 1.424\n",
      "[1,  1900] loss: 1.451\n",
      "[1,  2000] loss: 1.411\n",
      "[1,  2100] loss: 1.416\n",
      "[1,  2200] loss: 1.405\n",
      "[1,  2300] loss: 1.412\n",
      "[1,  2400] loss: 1.336\n",
      "[1,  2500] loss: 1.394\n",
      "[1,  2600] loss: 1.387\n",
      "[1,  2700] loss: 1.352\n",
      "[1,  2800] loss: 1.338\n",
      "[1,  2900] loss: 1.340\n",
      "[1,  3000] loss: 1.306\n",
      "[1,  3100] loss: 1.298\n",
      "[2,   100] loss: 1.323\n",
      "[2,   200] loss: 1.268\n",
      "[2,   300] loss: 1.278\n",
      "[2,   400] loss: 1.299\n",
      "[2,   500] loss: 1.304\n",
      "[2,   600] loss: 1.207\n",
      "[2,   700] loss: 1.259\n",
      "[2,   800] loss: 1.267\n",
      "[2,   900] loss: 1.178\n",
      "[2,  1000] loss: 1.266\n",
      "[2,  1100] loss: 1.222\n",
      "[2,  1200] loss: 1.221\n",
      "[2,  1300] loss: 1.242\n",
      "[2,  1400] loss: 1.249\n",
      "[2,  1500] loss: 1.234\n",
      "[2,  1600] loss: 1.210\n",
      "[2,  1700] loss: 1.200\n",
      "[2,  1800] loss: 1.189\n",
      "[2,  1900] loss: 1.201\n",
      "[2,  2000] loss: 1.216\n",
      "[2,  2100] loss: 1.203\n",
      "[2,  2200] loss: 1.150\n",
      "[2,  2300] loss: 1.151\n",
      "[2,  2400] loss: 1.182\n",
      "[2,  2500] loss: 1.185\n",
      "[2,  2600] loss: 1.206\n",
      "[2,  2700] loss: 1.196\n",
      "[2,  2800] loss: 1.159\n",
      "[2,  2900] loss: 1.209\n",
      "[2,  3000] loss: 1.157\n",
      "[2,  3100] loss: 1.171\n",
      "[3,   100] loss: 1.077\n",
      "[3,   200] loss: 1.148\n",
      "[3,   300] loss: 1.130\n",
      "[3,   400] loss: 1.071\n",
      "[3,   500] loss: 1.137\n",
      "[3,   600] loss: 1.086\n",
      "[3,   700] loss: 1.128\n",
      "[3,   800] loss: 1.117\n",
      "[3,   900] loss: 1.124\n",
      "[3,  1000] loss: 1.098\n",
      "[3,  1100] loss: 1.078\n",
      "[3,  1200] loss: 1.113\n",
      "[3,  1300] loss: 1.133\n",
      "[3,  1400] loss: 1.111\n",
      "[3,  1500] loss: 1.109\n",
      "[3,  1600] loss: 1.034\n",
      "[3,  1700] loss: 1.094\n",
      "[3,  1800] loss: 1.102\n",
      "[3,  1900] loss: 1.096\n",
      "[3,  2000] loss: 1.080\n",
      "[3,  2100] loss: 1.097\n",
      "[3,  2200] loss: 1.051\n",
      "[3,  2300] loss: 1.099\n",
      "[3,  2400] loss: 1.084\n",
      "[3,  2500] loss: 1.044\n",
      "[3,  2600] loss: 1.092\n",
      "[3,  2700] loss: 1.053\n",
      "[3,  2800] loss: 1.020\n",
      "[3,  2900] loss: 1.060\n",
      "[3,  3000] loss: 1.056\n",
      "[3,  3100] loss: 1.128\n",
      "[4,   100] loss: 1.008\n",
      "[4,   200] loss: 1.022\n",
      "[4,   300] loss: 1.049\n",
      "[4,   400] loss: 1.061\n",
      "[4,   500] loss: 1.041\n",
      "[4,   600] loss: 0.994\n",
      "[4,   700] loss: 1.083\n",
      "[4,   800] loss: 1.056\n",
      "[4,   900] loss: 0.989\n",
      "[4,  1000] loss: 1.016\n",
      "[4,  1100] loss: 0.989\n",
      "[4,  1200] loss: 0.987\n",
      "[4,  1300] loss: 0.978\n",
      "[4,  1400] loss: 1.014\n",
      "[4,  1500] loss: 0.991\n",
      "[4,  1600] loss: 1.062\n",
      "[4,  1700] loss: 1.055\n",
      "[4,  1800] loss: 0.996\n",
      "[4,  1900] loss: 1.069\n",
      "[4,  2000] loss: 1.053\n",
      "[4,  2100] loss: 0.965\n",
      "[4,  2200] loss: 1.013\n",
      "[4,  2300] loss: 1.004\n",
      "[4,  2400] loss: 1.013\n",
      "[4,  2500] loss: 1.019\n",
      "[4,  2600] loss: 1.038\n",
      "[4,  2700] loss: 1.025\n",
      "[4,  2800] loss: 1.032\n",
      "[4,  2900] loss: 1.012\n",
      "[4,  3000] loss: 0.982\n",
      "[4,  3100] loss: 0.986\n",
      "[5,   100] loss: 0.955\n",
      "[5,   200] loss: 0.975\n",
      "[5,   300] loss: 0.958\n",
      "[5,   400] loss: 0.984\n",
      "[5,   500] loss: 0.982\n",
      "[5,   600] loss: 0.960\n",
      "[5,   700] loss: 0.999\n",
      "[5,   800] loss: 0.981\n",
      "[5,   900] loss: 0.955\n",
      "[5,  1000] loss: 0.981\n",
      "[5,  1100] loss: 0.929\n",
      "[5,  1200] loss: 0.958\n",
      "[5,  1300] loss: 0.972\n",
      "[5,  1400] loss: 0.948\n",
      "[5,  1500] loss: 0.969\n",
      "[5,  1600] loss: 0.931\n",
      "[5,  1700] loss: 0.989\n",
      "[5,  1800] loss: 0.964\n",
      "[5,  1900] loss: 0.924\n",
      "[5,  2000] loss: 0.998\n",
      "[5,  2100] loss: 0.989\n",
      "[5,  2200] loss: 1.027\n",
      "[5,  2300] loss: 0.906\n",
      "[5,  2400] loss: 0.945\n",
      "[5,  2500] loss: 0.985\n",
      "[5,  2600] loss: 0.980\n",
      "[5,  2700] loss: 0.950\n",
      "[5,  2800] loss: 0.974\n",
      "[5,  2900] loss: 0.989\n",
      "[5,  3000] loss: 0.899\n",
      "[5,  3100] loss: 0.971\n",
      "[6,   100] loss: 0.910\n",
      "[6,   200] loss: 0.892\n",
      "[6,   300] loss: 0.940\n",
      "[6,   400] loss: 0.885\n",
      "[6,   500] loss: 0.887\n",
      "[6,   600] loss: 0.965\n",
      "[6,   700] loss: 0.960\n",
      "[6,   800] loss: 0.931\n",
      "[6,   900] loss: 0.933\n",
      "[6,  1000] loss: 0.890\n",
      "[6,  1100] loss: 0.911\n",
      "[6,  1200] loss: 0.890\n",
      "[6,  1300] loss: 0.882\n",
      "[6,  1400] loss: 0.940\n",
      "[6,  1500] loss: 0.916\n",
      "[6,  1600] loss: 0.900\n",
      "[6,  1700] loss: 0.902\n",
      "[6,  1800] loss: 0.965\n",
      "[6,  1900] loss: 0.904\n",
      "[6,  2000] loss: 0.864\n",
      "[6,  2100] loss: 0.935\n",
      "[6,  2200] loss: 0.925\n",
      "[6,  2300] loss: 0.947\n",
      "[6,  2400] loss: 0.985\n",
      "[6,  2500] loss: 0.926\n",
      "[6,  2600] loss: 0.868\n",
      "[6,  2700] loss: 0.938\n",
      "[6,  2800] loss: 0.944\n",
      "[6,  2900] loss: 0.961\n",
      "[6,  3000] loss: 0.897\n",
      "[6,  3100] loss: 0.963\n",
      "[7,   100] loss: 0.798\n",
      "[7,   200] loss: 0.843\n",
      "[7,   300] loss: 0.858\n",
      "[7,   400] loss: 0.860\n",
      "[7,   500] loss: 0.888\n",
      "[7,   600] loss: 0.907\n",
      "[7,   700] loss: 0.864\n",
      "[7,   800] loss: 0.857\n",
      "[7,   900] loss: 0.901\n",
      "[7,  1000] loss: 0.952\n",
      "[7,  1100] loss: 0.824\n",
      "[7,  1200] loss: 0.863\n",
      "[7,  1300] loss: 0.854\n",
      "[7,  1400] loss: 0.900\n",
      "[7,  1500] loss: 0.905\n",
      "[7,  1600] loss: 0.905\n",
      "[7,  1700] loss: 0.853\n",
      "[7,  1800] loss: 0.884\n",
      "[7,  1900] loss: 0.903\n",
      "[7,  2000] loss: 0.916\n",
      "[7,  2100] loss: 0.904\n",
      "[7,  2200] loss: 0.920\n",
      "[7,  2300] loss: 0.881\n",
      "[7,  2400] loss: 0.911\n",
      "[7,  2500] loss: 0.904\n",
      "[7,  2600] loss: 0.882\n",
      "[7,  2700] loss: 0.907\n",
      "[7,  2800] loss: 0.908\n",
      "[7,  2900] loss: 0.916\n",
      "[7,  3000] loss: 0.876\n",
      "[7,  3100] loss: 0.876\n",
      "[8,   100] loss: 0.831\n",
      "[8,   200] loss: 0.822\n",
      "[8,   300] loss: 0.809\n",
      "[8,   400] loss: 0.852\n",
      "[8,   500] loss: 0.790\n",
      "[8,   600] loss: 0.871\n",
      "[8,   700] loss: 0.867\n",
      "[8,   800] loss: 0.818\n",
      "[8,   900] loss: 0.818\n",
      "[8,  1000] loss: 0.791\n",
      "[8,  1100] loss: 0.927\n",
      "[8,  1200] loss: 0.865\n",
      "[8,  1300] loss: 0.860\n",
      "[8,  1400] loss: 0.853\n",
      "[8,  1500] loss: 0.874\n",
      "[8,  1600] loss: 0.825\n",
      "[8,  1700] loss: 0.856\n",
      "[8,  1800] loss: 0.825\n",
      "[8,  1900] loss: 0.889\n",
      "[8,  2000] loss: 0.860\n",
      "[8,  2100] loss: 0.886\n",
      "[8,  2200] loss: 0.838\n",
      "[8,  2300] loss: 0.903\n",
      "[8,  2400] loss: 0.859\n",
      "[8,  2500] loss: 0.878\n",
      "[8,  2600] loss: 0.849\n",
      "[8,  2700] loss: 0.897\n",
      "[8,  2800] loss: 0.856\n",
      "[8,  2900] loss: 0.870\n",
      "[8,  3000] loss: 0.821\n",
      "[8,  3100] loss: 0.836\n",
      "[9,   100] loss: 0.853\n",
      "[9,   200] loss: 0.791\n",
      "[9,   300] loss: 0.800\n",
      "[9,   400] loss: 0.844\n",
      "[9,   500] loss: 0.764\n",
      "[9,   600] loss: 0.812\n",
      "[9,   700] loss: 0.810\n",
      "[9,   800] loss: 0.779\n",
      "[9,   900] loss: 0.833\n",
      "[9,  1000] loss: 0.821\n",
      "[9,  1100] loss: 0.839\n",
      "[9,  1200] loss: 0.792\n",
      "[9,  1300] loss: 0.760\n",
      "[9,  1400] loss: 0.831\n",
      "[9,  1500] loss: 0.831\n",
      "[9,  1600] loss: 0.846\n",
      "[9,  1700] loss: 0.803\n",
      "[9,  1800] loss: 0.889\n",
      "[9,  1900] loss: 0.778\n",
      "[9,  2000] loss: 0.878\n",
      "[9,  2100] loss: 0.863\n",
      "[9,  2200] loss: 0.866\n",
      "[9,  2300] loss: 0.870\n",
      "[9,  2400] loss: 0.875\n",
      "[9,  2500] loss: 0.849\n",
      "[9,  2600] loss: 0.838\n",
      "[9,  2700] loss: 0.772\n",
      "[9,  2800] loss: 0.837\n",
      "[9,  2900] loss: 0.766\n",
      "[9,  3000] loss: 0.822\n",
      "[9,  3100] loss: 0.872\n",
      "[10,   100] loss: 0.762\n",
      "[10,   200] loss: 0.763\n",
      "[10,   300] loss: 0.781\n",
      "[10,   400] loss: 0.816\n",
      "[10,   500] loss: 0.784\n",
      "[10,   600] loss: 0.762\n",
      "[10,   700] loss: 0.813\n",
      "[10,   800] loss: 0.790\n",
      "[10,   900] loss: 0.819\n",
      "[10,  1000] loss: 0.795\n",
      "[10,  1100] loss: 0.781\n",
      "[10,  1200] loss: 0.835\n",
      "[10,  1300] loss: 0.807\n",
      "[10,  1400] loss: 0.776\n",
      "[10,  1500] loss: 0.846\n",
      "[10,  1600] loss: 0.792\n",
      "[10,  1700] loss: 0.863\n",
      "[10,  1800] loss: 0.787\n",
      "[10,  1900] loss: 0.763\n",
      "[10,  2000] loss: 0.788\n",
      "[10,  2100] loss: 0.823\n",
      "[10,  2200] loss: 0.824\n",
      "[10,  2300] loss: 0.809\n",
      "[10,  2400] loss: 0.864\n",
      "[10,  2500] loss: 0.791\n",
      "[10,  2600] loss: 0.830\n",
      "[10,  2700] loss: 0.809\n",
      "[10,  2800] loss: 0.809\n",
      "[10,  2900] loss: 0.783\n",
      "[10,  3000] loss: 0.797\n",
      "[10,  3100] loss: 0.833\n",
      "Finished training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Now let us get a mini-batch from test-data\u001B[39;00m\n\u001B[1;32m      8\u001B[0m data_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(classifier\u001B[38;5;241m.\u001B[39mtest_loader)\n\u001B[0;32m----> 9\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[43mdata_iter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Let us make predictions_list on these.\u001B[39;00m\n\u001B[1;32m     12\u001B[0m predictions \u001B[38;5;241m=\u001B[39m classifier\u001B[38;5;241m.\u001B[39mpredict(images\u001B[38;5;241m.\u001B[39mto(classifier\u001B[38;5;241m.\u001B[39mdevice))\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "from svlearn.datasets.cifar10 import Cifar10Classifier, SimpleCifarNet, AnotherCnnExampleNet\n",
    "    \n",
    "classifier = Cifar10Classifier(AnotherCnnExampleNet())\n",
    "classifier.with_target_labels(classifier.LABELS)\n",
    "classifier.train(epochs=10)\n",
    "\n",
    "# Now let us get a mini-batch from test-data\n",
    "data_iter = iter(classifier.test_loader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Let us make predictions_list on these.\n",
    "predictions = classifier.predict(images.to(classifier.device))\n",
    "original_labels = [classifier.LABELS[label] for label in labels]\n",
    "\n",
    "# initialize error and total count.\n",
    "count = errors = 0\n",
    "for y, y_hat in zip(original_labels, predictions):\n",
    "    print(f'{y:>8s} ==> {y_hat}')\n",
    "    count += 1\n",
    "    if y != y_hat:\n",
    "        errors += 1\n",
    "print(\n",
    "        f'There were {errors} errors in prediction, out of {count} test samples. Accuracy: {(count - errors) / count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
