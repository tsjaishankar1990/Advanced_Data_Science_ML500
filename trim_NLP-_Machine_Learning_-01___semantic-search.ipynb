{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:53.489230Z",
     "start_time": "2023-03-25T10:55:51.576481Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "       \n",
       "    \n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"NLP with Transformers\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "           \n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "    \n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "   \n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "    \n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_43673/4157984209.py:50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Yellowbrick\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureImportances\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix, ClassificationReport, ROCAUC\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupportvectors-common.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2954\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   2952\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 2954\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:270\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_43673/4157984209.py:50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Yellowbrick\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureImportances\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix, ClassificationReport, ROCAUC\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:28:41.781265Z",
     "start_time": "2021-12-15T23:28:41.750084Z"
    }
   },
   "source": [
    "# Semantic search\n",
    "\n",
    "Traditionally, one would search through a corpus of documents using a keywords-based search engine like Lucene, Solr, ElasticSearch, etc. While the technology has matured, the basic underlying approach behind keyword search engines is to maintain an *inverted-index* mapping keywords to a list of documents that contain them, with associated relevances.\n",
    "\n",
    "In general, the keywords-based search approach has been quite successful over the years, and have matured with added features and linguistic capabilities.\n",
    "\n",
    "However, this approach has had its limitations. The principal cause of it goes to the fact that when we enter keywords, it is a human tendency to describe the intent of what we are looking for. For example, if we enter \"breakfast places\", we implicitly also mean restaurants, cafe, etc that serve items appropriate for breakfast. There may be a restaurant described as a shop for expresso, or crepe, that a keywords-search will likely miss, since its keywords do not match the query terms. And yet, we would hope to see it near the top of the search results.\n",
    "\n",
    "Semantic search is an NLP approach largely relying on deep-neural networks, and in particular, the transformers that make it possible to more closely infer the human intent behind the search terms, the relationship between the words, and the underlying context. It allows for entire sentences -- and even paragraphs -- describing what the searcher's intent is, and retrieves results more relevant or aligned to it.\n",
    "\n",
    "## How would we do this NLP task with AI?\n",
    "\n",
    "Let us represent the functional behavior we expect: \n",
    "\n",
    "\n",
    "![](images/semantic-search-functionality.png)\n",
    "\n",
    "\n",
    "### Magic happens: breaking it down into steps\n",
    "\n",
    "We recall that machine-learning algorithms work with vectors ($\\mathbf{X}$) representation of data.\n",
    "\n",
    "So the first order of business would be to map each of the document texts $D_i$ to its corresponding vector $X_i$ in an appropriate $d$-dimensional space, $\\mathbb{R}^d$, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "D_i \\longrightarrow X_i \\in \\mathbb{R}^d\n",
    "\\end{equation}\n",
    "\n",
    "This resulting vectors are called **sentence embeddings**. Once these embeddings are for each of the documents, we can store the collection of tuples $[<D_1, X_1>, <D_2, X_2>, ..., <D_n, X_n>]$. Here each tuple corresponds to a document and its sentence embedding.\n",
    "\n",
    "This collection of tuples, therefore, becomes our **search index**.\n",
    "\n",
    "### Search\n",
    "\n",
    "Now, when the user described what she is looking for, we consider the entire text as a \"sentence\".\n",
    "<p>\n",
    "<div class=\"alert-box alert-warning\" style=\"padding-top:30px\">\n",
    "   \n",
    "<b >Caveat Emptor</b>\n",
    "\n",
    "> Note that we have a rather relaxed definition of a *sentence* in NLP: it diverges from a grammmatical definition of a sentence somewhat.  For example, in the English language, we would consider a sentence to be terminated with a punctuation, such as a period, question-mark or exclamation. However, in NLP, we loosely consider the entire text -- whether it is just a word, or a few keywords, or an english sentence, or a few sentences together -- as one **sentence** for the purposes of natual language processing task.\n",
    "    \n",
    "<p>\n",
    "</div>\n",
    "    \n",
    "Therefore, it is common to consider an entire document text as a *sentence* if the text is relatively short. Alternatively, it is partitioned into smaller chunks (of say 512-tokens each), and each such chunk is considered an NLP *sentence*.\n",
    "\n",
    "Since we consider the entire query text as a sentence, we can map it to its **sentence embedding vector**, ${Q}$.\n",
    "\n",
    "#### Vector Similarity\n",
    "Once we have this, we simply need to compare the query vector ${Q}$ with each of the document vectors $X_i$, and sort the document vectors in descending order of similarity.\n",
    "\n",
    "The rest is trivial: pick the top-k  in the sorted document vectors list. Then for each vector, look up its corresponding document, and return the list as sorted search result of relevant document.\n",
    "\n",
    "We expect that these documents will exhibit high semantic similarity with the search query, assuming that the search index did contain such documents.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png\">\n",
    "    <caption> Semantic similarity as vector proximity in the embedding space. <br>\n",
    "    (Figure source: Sbert.net documentation).\n",
    "    </caption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Similarity measures\n",
    "\n",
    "The sentence embedding vectors typically exist in very large dimensional space (e.g., 300 dimensions). In such large dimensional spaces, the notion of euclidean distance is not as effective. Therefore, it is far more common to use one of the two below measures for vector similarity:\n",
    "\n",
    "* **dot-product**, the (inner) dot-product between the embedding vectors.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{dot-similarity} = \\langle X_i, X_j \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "* **cosine-similarity**, the $\\cos \\left(\\theta_{ij}\\right)$ gives degree of directional alignment between the vectors, but ignores their magnitudes. Here, $\\theta_{ij}$ is the angle between $X_i$ and $X_j$ (embedding) vectors.\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{cosine-similarity} = \\frac{\\langle X_i, X_j \\rangle} {\\| X_i \\| \\| X_j \\|}\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert-box alert-info\" style=\"padding-top:30px\">\n",
    "   \n",
    "**Important**\n",
    "    \n",
    ">  Sentence transformer models trained with cosine-similarity tend to favor the shorter document texts in the search results, whereas the models trained on the dot-product similarity tend to favor longer texts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric vs asymmetric search\n",
    "\n",
    "One of the technical aspects to be careful of is the relative textual length of the query sentence compared to the actual documents. Different sentence-transformer models have been trained specifically for each of these use-cases. \n",
    "\n",
    "* **symmetric search** when we expect the query-sentence to be approximately the same length as the document sentences.\n",
    "\n",
    "* **asymmetric search** when we expect the document texts to be significantly larger in length to the query sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an appropriate model\n",
    "\n",
    "Let us consider the use-case where we are searching through some reasonably large documents. In such a case, it would be appropriate to use an asymmetric-search model. \n",
    "\n",
    "Let us consider an asymmetric model trained with *cosine-similarity* as the distance measure. In particular, let us use one of the below models:\n",
    "\n",
    "* `\n",
    "\n",
    "\n",
    "We load the model with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:56.147887Z",
     "start_time": "2023-03-25T10:55:53.491388Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL = 'msmarco-distilbert-base-v4'\n",
    "embedder = SentenceTransformer(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a toy corpus\n",
    "\n",
    "Let us now load a toy corpus of some simple, long texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:56.204028Z",
     "start_time": "2023-03-25T10:55:56.149807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "\n",
      "Alec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\n",
      "\n",
      "Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1\n",
      "\n",
      "Abstract\n",
      "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw\n",
      "text about images is a promising alternative which\n",
      "leverages a much broader source of supervision.\n",
      "We demonstrate that the simple pre-training task\n",
      "of predicting which caption goes with which im-\n",
      "age is an efficient and scalable way to learn SOTA\n",
      "image representations from scratch on a dataset\n",
      "of 400 million (image, text) pairs collected from\n",
      "the internet. After pre-training, natural language\n",
      "is used to reference learned visual concepts (or\n",
      "describe new ones) enabling zero-shot transfer\n",
      "of the model to downstream tasks. We study\n",
      "the performance of this approach by benchmark-\n",
      "ing on over 30 different existing computer vi-\n",
      "sion datasets, spanning tasks such as OCR, ac-\n",
      "tion recognition in videos, geo-localization, and\n",
      "many types of fine-grained object classification.\n",
      "The model transfers non-trivially to most tasks\n",
      "and is often competitive with a fully supervised\n",
      "baseline without the need for any dataset spe-\n",
      "cific training. For instance, we match the ac-\n",
      "curacy of the original ResNet-50 on ImageNet\n",
      "zero-shot without needing to use any of the 1.28\n",
      "million training examples it was trained on. We\n",
      "release our code and pre-trained model weights at\n",
      "https://github.com/OpenAI/CLIP.\n",
      "\n",
      "1. Introduction and Motivating Work\n",
      "Pre-training methods which learn directly from raw text\n",
      "have revolutionized NLP over the last few years (Dai &\n",
      "Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\n",
      "ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\n",
      "\n",
      "*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\n",
      "Correspondence to: <{alec, jongwook}@openai.com>.\n",
      "\n",
      "Task-agnostic objectives such as autoregressive and masked\n",
      "language modeling have scaled across many orders of mag-\n",
      "nitude in compute, model capacity, and data, steadily im-\n",
      "proving capabilities. The development of “text-to-text” as\n",
      "a standardized input-output interface (McCann et al., 2018;\n",
      "Radford et al., 2019; Raffel et al., 2019) has enabled task-\n",
      "agnostic architectures to zero-shot transfer to downstream\n",
      "datasets removing the need for specialized output heads or\n",
      "dataset specific customization. Flagship systems like GPT-3\n",
      "(Brown et al., 2020) are now competitive across many tasks\n",
      "with bespoke models while requiring little to no dataset\n",
      "specific training data.\n",
      "\n",
      "These results suggest that the aggregate supervision acces-\n",
      "sible to modern pre-training methods within web-scale col-\n",
      "lections of text surpasses that of high-quality crowd-labeled\n",
      "NLP datasets. However, in other fields such as computer\n",
      "vision it is still standard practice to pre-train models on\n",
      "crowd-labeled datasets such as ImageNet (Deng et al., 2009).\n",
      "Could scalable pre-training methods which learn directly\n",
      "from web text result in a similar breakthrough in computer\n",
      "vision? Prior work is encouraging.\n",
      "\n",
      "Over 20 years ago Mori et al. (1999) explored improving\n",
      "content based image retrieval by training a model to pre-\n",
      "dict the nouns and adjectives in text documents paired with\n",
      "images. Quattoni et al. (2007) demonstrated it was possi-\n",
      "ble to learn more data efficient image representations via\n",
      "manifold learning in the weight space of classifiers trained\n",
      "to predict words in captions associated with images. Sri-\n",
      "vastava & Salakhutdinov (2012) explored deep represen-\n",
      "tation learning by training multimodal Deep Boltzmann\n",
      "Machines on top of low-level image and text tag features.\n",
      "Joulin et al. (2016) modernized this line of work and demon-\n",
      "strated that CNNs trained to predict words in image cap-\n",
      "tions learn useful image representations. They converted\n",
      "the title, description, and hashtag metadata of images in the\n",
      "YFCC100M dataset (Thomee et al., 2016) into a bag-of-\n",
      "words multi-label classification task and showed that pre-\n",
      "training AlexNet (Krizhevsky et al., 2012) to predict these\n",
      "labels learned representations which preformed similarly\n",
      "to ImageNet-based pre-training on transfer tasks. Li et al.\n",
      "(2017) then extended this approach to predicting phrase n-\n",
      "grams in addition to individual words and demonstrated the\n",
      "ability of their system to zero-shot transfer to other image\n",
      "\n",
      "ar\n",
      "X\n",
      "\n",
      "iv\n",
      ":2\n",
      "\n",
      "10\n",
      "3.\n",
      "\n",
      "00\n",
      "02\n",
      "\n",
      "0v\n",
      "1 \n",
      "\n",
      " [\n",
      "cs\n",
      "\n",
      ".C\n",
      "V\n",
      "\n",
      "] \n",
      " 2\n",
      "\n",
      "6 \n",
      "Fe\n",
      "\n",
      "b \n",
      "20\n",
      "\n",
      "21\n",
      "\n",
      "https://github.com/OpenAI/CLIP\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 2\n",
      "\n",
      "I1·T2 I1·T3 …\n",
      "\n",
      "I2·T1 I2·T3 …\n",
      "\n",
      "I3·T1 I3·T2 …\n",
      "\n",
      "⋮ ⋮ ⋮\n",
      "\n",
      "I1·T1\n",
      "\n",
      "I2·T2\n",
      "\n",
      "I3·T3\n",
      "\n",
      "(1) Contrastive pre-training\n",
      "\n",
      "Image\n",
      "Encoder\n",
      "\n",
      "Text\n",
      "EncoderPepper\tthe\n",
      "\n",
      "aussie\tpup\n",
      "\n",
      "Pepper\tthe\n",
      "aussie\tpup\n",
      "\n",
      "Pepper\tthe\n",
      "aussie\tpup\n",
      "\n",
      "Pepper\tthe\n",
      "aussie\tpup\n",
      "\n",
      "T1 T2 T3 …\n",
      "\n",
      "I1\n",
      "\n",
      "I2\n",
      "\n",
      "I3\n",
      "\n",
      "⋮\n",
      "\n",
      "(2) Create dataset classifier from label text\n",
      "\n",
      "plane\n",
      "\n",
      "car\n",
      "\n",
      "dog\n",
      "\n",
      "⋮\n",
      "\n",
      "bird\n",
      "\n",
      "A\tphoto\tof\n",
      "a\t{object}.\n",
      "\n",
      "⋮\n",
      "\n",
      "Text\n",
      "Encoder\n",
      "\n",
      "T1 T2 T3 TN\n",
      "\n",
      "…\n",
      "\n",
      "(3) Use for zero-shot prediction\n",
      "\n",
      "Image\n",
      "Encoder\n",
      "\n",
      "I1 I1·T2 I1·TNI1·T1\n",
      "\n",
      "…\n",
      "\n",
      "…\n",
      "\n",
      "A\tphoto\tof\n",
      "\ta\tdog.\n",
      "\n",
      "TN\n",
      "\n",
      "IN·T1 IN·T2 IN·T3\n",
      "\n",
      "I1·TN\n",
      "\n",
      "I2·TN\n",
      "\n",
      "I3·TN\n",
      "\n",
      "⋮\n",
      "\n",
      "…IN\n",
      "\n",
      "…\n",
      "\n",
      "⋮ ⋱\n",
      "\n",
      "IN·TN\n",
      "\n",
      "I1·T3\n",
      "\n",
      "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\n",
      "some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\n",
      "examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\n",
      "target dataset’s classes.\n",
      "\n",
      "classification datasets by scoring target classes based on\n",
      "their dictionary of learned visual n-grams and predicting the\n",
      "one with the highest score. Adopting more recent architec-\n",
      "tures and pre-training approaches, VirTex (Desai & Johnson,\n",
      "2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\n",
      "VIRT (Zhang et al., 2020) have recently demonstrated the\n",
      "potential of transformer-based language modeling, masked\n",
      "language modeling, and contrastive objectives to learn im-\n",
      "age representations from text.\n",
      "\n",
      "While exciting as proofs of concept, using natural language\n",
      "supervision for image representation learning is still rare.\n",
      "This is likely because demonstrated performance on com-\n",
      "mon benchmarks is much lower than alternative approaches.\n",
      "For example, Li et al. (2017) reach only 11.5% accuracy\n",
      "on ImageNet in a zero-shot setting. This is well below the\n",
      "88.4% accuracy of the current state of the art (Xie et al.,\n",
      "2020). It is even below the 50% accuracy of classic com-\n",
      "puter vision approaches (Deng et al., 2012). Instead, more\n",
      "narrowly scoped but well-targeted uses of weak supervision\n",
      "have improved performance. Mahajan et al. (2018) showed\n",
      "that predicting ImageNet-related hashtags on Instagram im-\n",
      "ages is an effective pre-training task. When fine-tuned to\n",
      "ImageNet these pre-trained models increased accuracy by\n",
      "over 5% and improved the overall state of the art at the time.\n",
      "Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\n",
      "also demonstrated large gains on a broader set of transfer\n",
      "benchmarks by pre-training models to predict the classes of\n",
      "the noisily labeled JFT-300M dataset.\n",
      "\n",
      "This line of work represents the current pragmatic middle\n",
      "ground between learning from a limited amount of super-\n",
      "vised “gold-labels” and learning from practically unlimited\n",
      "amounts of raw text. However, it is not without compro-\n",
      "\n",
      "mises. Both works carefully design, and in the process limit,\n",
      "their supervision to 1000 and 18291 classes respectively.\n",
      "Natural language is able to express, and therefore supervise,\n",
      "a much wider set of visual concepts through its general-\n",
      "ity. Both approaches also use static softmax classifiers to\n",
      "perform prediction and lack a mechanism for dynamic out-\n",
      "puts. This severely curtails their flexibility and limits their\n",
      "“zero-shot” capabilities.\n",
      "\n",
      "A crucial difference between these weakly supervised mod-\n",
      "els and recent explorations of learning image representations\n",
      "directly from natural language is scale. While Mahajan et al.\n",
      "(2018) and Kolesnikov et al. (2019) trained their models for\n",
      "accelerator years on millions to billions of images, VirTex,\n",
      "ICMLM, and ConVIRT trained for accelerator days on one\n",
      "to two hundred thousand images. In this work, we close\n",
      "this gap and study the behaviors of image classifiers trained\n",
      "with natural language supervision at large scale. Enabled\n",
      "by the large amounts of publicly available data of this form\n",
      "on the internet, we create a new dataset of 400 million (im-\n",
      "age, text) pairs and demonstrate that a simplified version of\n",
      "ConVIRT trained from scratch, which we call CLIP, for Con-\n",
      "trastive Language-Image Pre-training, is an efficient method\n",
      "of learning from natural language supervision. We study\n",
      "the scalability of CLIP by training a series of eight models\n",
      "spanning almost 2 orders of magnitude of compute and ob-\n",
      "serve that transfer performance is a smoothly predictable\n",
      "function of compute (Hestness et al., 2017; Kaplan et al.,\n",
      "2020). We find that CLIP, similar to the GPT family, learns\n",
      "to perform a wide set of tasks during pre-training including\n",
      "OCR, geo-localization, action recognition, and many others.\n",
      "We measure this by benchmarking the zero-shot transfer\n",
      "performance of CLIP on over 30 existing datasets and find\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 3\n",
      "\n",
      "2M 33M 67M 134M 268M 400M\n",
      "# of images processed\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "Ze\n",
      "ro\n",
      "\n",
      "-S\n",
      "ho\n",
      "\n",
      "t I\n",
      "m\n",
      "\n",
      "ag\n",
      "eN\n",
      "\n",
      "et\n",
      " A\n",
      "\n",
      "cc\n",
      "ur\n",
      "\n",
      "ac\n",
      "y\n",
      "\n",
      "3X efficiency4X efficiency\n",
      "\n",
      "Bag of Words Contrastive (CLIP)\n",
      "Bag of Words Prediction\n",
      "Transformer Language Model\n",
      "\n",
      "Figure 2. CLIP is much more efficient at zero-shot transfer\n",
      "than our image caption baseline. Although highly expressive,\n",
      "we found that transformer-based language models are relatively\n",
      "weak at zero-shot ImageNet classification. Here, we see that it\n",
      "learns 3x slower than a baseline which predicts a bag-of-words\n",
      "(BoW) encoding of the text (Joulin et al., 2016). Swapping the\n",
      "prediction objective for the contrastive objective of CLIP further\n",
      "improves efficiency another 4x.\n",
      "\n",
      "it can be competitive with prior task-specific supervised\n",
      "models. We also confirm these findings with linear-probe\n",
      "representation learning analysis and show that CLIP out-\n",
      "performs the best publicly available ImageNet model while\n",
      "also being more computationally efficient. We additionally\n",
      "find that zero-shot CLIP models are much more robust than\n",
      "equivalent accuracy supervised ImageNet models which\n",
      "suggests that zero-shot evaluation of task-agnostic models is\n",
      "much more representative of a model’s capability. These re-\n",
      "sults have significant policy and ethical implications, which\n",
      "we consider in Section 7.\n",
      "\n",
      "2. Approach\n",
      "2.1. Natural Language Supervision\n",
      "\n",
      "At the core of our approach is the idea of learning percep-\n",
      "tion from supervision contained in natural language. As\n",
      "discussed in the introduction, this is not at all a new idea,\n",
      "however terminology used to describe work in this space\n",
      "is varied, even seemingly contradictory, and stated motiva-\n",
      "tions are diverse. Zhang et al. (2020), Gomez et al. (2017),\n",
      "Joulin et al. (2016), and Desai & Johnson (2020) all intro-\n",
      "duce methods which learn visual representations from text\n",
      "paired with images but describe their approaches as unsuper-\n",
      "vised, self-supervised, weakly supervised, and supervised\n",
      "respectively.\n",
      "\n",
      "We emphasize that what is common across this line of work\n",
      "is not any of the details of the particular methods used but\n",
      "the appreciation of natural language as a training signal. All\n",
      "these approaches are learning from natural language super-\n",
      "\n",
      "vision. Although early work wrestled with the complexity\n",
      "of natural language when using topic model and n-gram\n",
      "representations, improvements in deep contextual represen-\n",
      "tation learning suggest we now have the tools to effectively\n",
      "leverage this abundant source of supervision (McCann et al.,\n",
      "2017).\n",
      "\n",
      "Learning from natural language has several potential\n",
      "strengths over other training methods. It’s much easier\n",
      "to scale natural language supervision compared to standard\n",
      "crowd-sourced labeling for image classification since it does\n",
      "not require annotations to be in a classic “machine learning\n",
      "compatible format” such as the canonical 1-of-N majority\n",
      "vote “gold label”. Instead, methods which work on natural\n",
      "language can learn passively from the supervision contained\n",
      "in the vast amount of text on the internet. Learning from\n",
      "natural language also has an important advantage over most\n",
      "unsupervised or self-supervised learning approaches in that\n",
      "it doesn’t “just” learn a representation but also connects that\n",
      "representation to language which enables flexible zero-shot\n",
      "transfer. In the following subsections, we detail the specific\n",
      "approach we settled on.\n",
      "\n",
      "2.2. Creating a Sufficiently Large Dataset\n",
      "\n",
      "Existing work has mainly used three datasets, MS-COCO\n",
      "(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\n",
      "YFCC100M (Thomee et al., 2016). While MS-COCO and\n",
      "Visual Genome are high quality crowd-labeled datasets, they\n",
      "are small by modern standards with approximately 100,000\n",
      "training photos each. By comparison, other computer vision\n",
      "systems are trained on up to 3.5 billion Instagram photos\n",
      "(Mahajan et al., 2018). YFCC100M, at 100 million photos,\n",
      "is a possible alternative, but the metadata for each image is\n",
      "sparse and of varying quality. Many images use automati-\n",
      "cally generated filenames like 20160716 113957.JPG\n",
      "as “titles” or contain “descriptions” of camera exposure\n",
      "settings. After filtering to keep only images with natural\n",
      "language titles and/or descriptions in English, the dataset\n",
      "shrunk by a factor of 6 to only 15 million photos. This is\n",
      "approximately the same size as ImageNet.\n",
      "\n",
      "A major motivation for natural language supervision is the\n",
      "large quantities of data of this form available publicly on the\n",
      "internet. Since existing datasets do not adequately reflect\n",
      "this possibility, considering results only on them would un-\n",
      "derestimate the potential of this line of research. To address\n",
      "this, we constructed a new dataset of 400 million (image,\n",
      "text) pairs collected form a variety of publicly available\n",
      "sources on the Internet. To attempt to cover as broad a set\n",
      "of visual concepts as possible, we search for (image, text)\n",
      "pairs as part of the construction process whose text includes\n",
      "one of a set of 500,000 queries.1 We approximately class\n",
      "\n",
      "1The base query list is all words occurring at least 100 times in\n",
      "the English version of Wikipedia. This is augmented with bi-grams\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 4\n",
      "\n",
      "balance the results by including up to 20,000 (image, text)\n",
      "pairs per query. The resulting dataset has a similar total\n",
      "word count as the WebText dataset used to train GPT-2. We\n",
      "refer to this dataset as WIT for WebImageText.\n",
      "\n",
      "2.3. Selecting an Efficient Pre-Training Method\n",
      "\n",
      "State-of-the-art computer vision systems use very large\n",
      "amounts of compute. Mahajan et al. (2018) required 19\n",
      "GPU years to train their ResNeXt101-32x48d and Xie et al.\n",
      "(2020) required 33 TPUv3 core-years to train their Noisy\n",
      "Student EfficientNet-L2. When considering that both these\n",
      "systems were trained to predict only 1000 ImageNet classes,\n",
      "the task of learning an open set of visual concepts from\n",
      "natural language seems daunting. In the course of our ef-\n",
      "forts, we found training efficiency was key to successfully\n",
      "scaling natural language supervision and we selected our\n",
      "final pre-training method based on this metric.\n",
      "\n",
      "Our initial approach, similar to VirTex, jointly trained an\n",
      "image CNN and text transformer from scratch to predict the\n",
      "caption of an image. However, we encountered difficulties\n",
      "efficiently scaling this method. In Figure 2 we show that a\n",
      "63 million parameter transformer language model, which\n",
      "already uses twice the compute of its ResNet-50 image\n",
      "encoder, learns to recognize ImageNet classes three times\n",
      "slower than a much simpler baseline that predicts a bag-of-\n",
      "words encoding of the same text.\n",
      "\n",
      "Both these approaches share a key similarity. They try to pre-\n",
      "dict the exact words of the text accompanying each image.\n",
      "This is a difficult task due to the wide variety of descriptions,\n",
      "comments, and related text that co-occur with images. Re-\n",
      "cent work in contrastive representation learning for images\n",
      "has found that contrastive objectives can learn better repre-\n",
      "sentations than their equivalent predictive objective (Tian\n",
      "et al., 2019). Other work has found that although generative\n",
      "models of images can learn high quality image representa-\n",
      "tions, they require over an order of magnitude more compute\n",
      "than contrastive models with the same performance (Chen\n",
      "et al., 2020a). Noting these findings, we explored training\n",
      "a system to solve the potentially easier proxy task of pre-\n",
      "dicting only which text as a whole is paired with which\n",
      "image and not the exact words of that text. Starting with\n",
      "the same bag-of-words encoding baseline, we swapped the\n",
      "predictive objective for a contrastive objective in Figure 2\n",
      "and observed a further 4x efficiency improvement in the rate\n",
      "of zero-shot transfer to ImageNet.\n",
      "\n",
      "Given a batch of N (image, text) pairs, CLIP is trained to\n",
      "predict which of the N ×N possible (image, text) pairings\n",
      "across a batch actually occurred. To do this, CLIP learns a\n",
      "\n",
      "with high pointwise mutual information as well as the names of\n",
      "all Wikipedia articles above a certain search volume. Finally all\n",
      "WordNet synsets not already in the query list are added.\n",
      "\n",
      "multi-modal embedding space by jointly training an image\n",
      "encoder and text encoder to maximize the cosine similar-\n",
      "ity of the image and text embeddings of the N real pairs\n",
      "in the batch while minimizing the cosine similarity of the\n",
      "embeddings of the N2 − N incorrect pairings. We opti-\n",
      "mize a symmetric cross entropy loss over these similarity\n",
      "scores. In Figure 3 we include pseudocode of the core of an\n",
      "implementation of CLIP. To our knowledge this batch con-\n",
      "struction technique and objective was first introduced in the\n",
      "area of deep metric learning as the multi-class N-pair loss\n",
      "Sohn (2016), was popularized for contrastive representation\n",
      "learning by Oord et al. (2018) as the InfoNCE loss, and was\n",
      "recently adapted for contrastive (text, image) representation\n",
      "learning in the domain of medical imaging by Zhang et al.\n",
      "(2020).\n",
      "\n",
      "Due to the large size of our pre-training dataset, over-fitting\n",
      "is not a major concern and the details of training CLIP are\n",
      "simplified compared to the implementation of Zhang et al.\n",
      "(2020). We train CLIP from scratch without initializing the\n",
      "image encoder with ImageNet weights or the text encoder\n",
      "with pre-trained weights. We do not use the non-linear\n",
      "projection between the representation and the contrastive\n",
      "embedding space, a change which was introduced by Bach-\n",
      "man et al. (2019) and popularized by Chen et al. (2020b).\n",
      "We instead use only a linear projection to map from each en-\n",
      "coder’s representation to the multi-modal embedding space.\n",
      "We did not notice a difference in training efficiency between\n",
      "the two versions and speculate that non-linear projections\n",
      "may be co-adapted with details of current image only in\n",
      "self-supervised representation learning methods. We also\n",
      "remove the text transformation function tu from Zhang et al.\n",
      "(2020) which samples a single sentence at uniform from\n",
      "the text since many of the (image, text) pairs in CLIP’s pre-\n",
      "training dataset are only a single sentence. We also simplify\n",
      "the image transformation function tv. A random square\n",
      "crop from resized images is the only data augmentation\n",
      "used during training. Finally, the temperature parameter\n",
      "which controls the range of the logits in the softmax, τ , is\n",
      "directly optimized during training as a log-parameterized\n",
      "multiplicative scalar to avoid turning as a hyper-parameter.\n",
      "\n",
      "2.4. Choosing and Scaling a Model\n",
      "\n",
      "We consider two different architectures for the image en-\n",
      "coder. For the first, we use ResNet-50 (He et al., 2016a)\n",
      "as the base architecture for the image encoder due to its\n",
      "widespread adoption and proven performance. We make sev-\n",
      "eral modifications to the original version using the ResNet-\n",
      "D improvements from He et al. (2019) and the antialiased\n",
      "rect-2 blur pooling from Zhang (2019). We also replace\n",
      "the global average pooling layer with an attention pooling\n",
      "mechanism. The attention pooling is implemented as a sin-\n",
      "gle layer of “transformer-style” multi-head QKV attention\n",
      "where the query is conditioned on the global average-pooled\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 5\n",
      "\n",
      "# image_encoder - ResNet or Vision Transformer\n",
      "# text_encoder  - CBOW or Text Transformer\n",
      "# I[n, h, w, c] - minibatch of aligned images\n",
      "# T[n, l]       - minibatch of aligned texts\n",
      "# W_i[d_i, d_e] - learned proj of image to embed\n",
      "# W_t[d_t, d_e] - learned proj of text to embed\n",
      "# t             - learned temperature parameter\n",
      "\n",
      "# extract feature representations of each modality\n",
      "I_f = image_encoder(I) #[n, d_i]\n",
      "T_f = text_encoder(T)  #[n, d_t]\n",
      "\n",
      "# joint multimodal embedding [n, d_e]\n",
      "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
      "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
      "\n",
      "# scaled pairwise cosine similarities [n, n]\n",
      "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
      "\n",
      "# symmetric loss function\n",
      "labels = np.arange(n)\n",
      "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
      "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
      "loss   = (loss_i + loss_t)/2\n",
      "\n",
      "Figure 3. Numpy-like pseudocode for the core of an implementa-\n",
      "tion of CLIP.\n",
      "\n",
      "representation of the image. For the second architecture, we\n",
      "experiment with the recently introduced Vision Transformer\n",
      "(ViT) (Dosovitskiy et al., 2020). We closely follow their\n",
      "implementation with only the minor modification of adding\n",
      "an additional layer normalization to the combined patch\n",
      "and position embeddings before the transformer and use a\n",
      "slightly different initialization scheme.\n",
      "\n",
      "The text encoder is a Transformer (Vaswani et al., 2017)\n",
      "with the architecture modifications described in Radford\n",
      "et al. (2019). As a base size we use a 63M-parameter 12-\n",
      "layer 512-wide model with 8 attention heads. The trans-\n",
      "former operates on a lower-cased byte pair encoding (BPE)\n",
      "representation of the text with a 49,152 vocab size (Sen-\n",
      "nrich et al., 2015). For computational efficiency, the max\n",
      "sequence length was capped at 76. The text sequence is\n",
      "bracketed with [SOS] and [EOS] tokens and the activa-\n",
      "tions of the highest layer of the transformer at the [EOS]\n",
      "token are treated as the feature representation of the text\n",
      "which is layer normalized and then linearly projected into\n",
      "the multi-modal embedding space. Masked self-attention\n",
      "was used in the text encoder to preserve the ability to ini-\n",
      "tialize with a pre-trained language model or add language\n",
      "modeling as an auxiliary objective, though exploration of\n",
      "this is left as future work.\n",
      "\n",
      "While previous computer vision research has often scaled\n",
      "models by increasing the width (Mahajan et al., 2018) or\n",
      "depth (He et al., 2016a) in isolation, for the ResNet image\n",
      "encoders we adapt the approach of Tan & Le (2019) which\n",
      "found that allocating additional compute across all of width,\n",
      "depth, and resolution outperforms only allocating it to only\n",
      "\n",
      "one dimension of the model. While Tan & Le (2019) tune\n",
      "the ratio of compute allocated to each dimension for their\n",
      "EfficientNet architecture, we use a simple baseline of allo-\n",
      "cating additional compute equally to increasing the width,\n",
      "depth, and resolution of the model. For the text encoder, we\n",
      "only scale the width of the model to be proportional to the\n",
      "calculated increase in width of the ResNet and do not scale\n",
      "the depth at all, as we found CLIP’s performance to be less\n",
      "sensitive to the capacity of the text encoder.\n",
      "\n",
      "2.5. Training\n",
      "\n",
      "We train a series of 5 ResNets and 3 Vision Transformers.\n",
      "For the ResNets we train a ResNet-50, a ResNet-101, and\n",
      "then 3 more which follow EfficientNet-style model scaling\n",
      "and use approximately 4x, 16x, and 64x the compute of a\n",
      "ResNet-50. They are denoted as RN50x4, RN50x16, and\n",
      "RN50x64 respectively. For the Vision Transformers we\n",
      "train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\n",
      "models for 32 epochs. We use the Adam optimizer (Kingma\n",
      "& Ba, 2014) with decoupled weight decay regularization\n",
      "(Loshchilov & Hutter, 2017) applied to all weights that are\n",
      "not gains or biases, and decay the learning rate using a\n",
      "cosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\n",
      "parameters were set using a combination of grid searches,\n",
      "random search, and manual tuning on the baseline ResNet-\n",
      "50 model when trained for 1 epoch. Hyper-parameters were\n",
      "then adapted heuristically for larger models due to compu-\n",
      "tational constraints. The learnable temperature parameter\n",
      "τ was initialized to the equivalent of 0.07 from (Wu et al.,\n",
      "2018) and clipped to prevent scaling the logits by more\n",
      "than 100 which we found necessary to prevent training in-\n",
      "stability. We use a very large minibatch size of 32,768.\n",
      "Mixed-precision (Micikevicius et al., 2017) was used to ac-\n",
      "celerate training and save memory. To save additional mem-\n",
      "ory, gradient checkpointing (Griewank & Walther, 2000;\n",
      "Chen et al., 2016), half-precision Adam statistics (Dhariwal\n",
      "et al., 2020), and half-precision stochastically rounded text\n",
      "encoder weights were used. The calculation of embedding\n",
      "similarities was also sharded with individual GPUs comput-\n",
      "ing only the subset of the pairwise similarities necessary for\n",
      "their local batch of embeddings. The largest ResNet model,\n",
      "RN50x64, took 18 days to train on 592 V100 GPUs while\n",
      "the largest Vision Transformer took 12 days on 256 V100\n",
      "GPUs. For the ViT-L/14 we also pre-train at a higher 336\n",
      "pixel resolution for one additional epoch to boost perfor-\n",
      "mance similar to FixRes (Touvron et al., 2019). We denote\n",
      "this model as ViT-L/14@336px. Unless otherwise specified,\n",
      "all results reported in this paper as “CLIP” use this model\n",
      "which we found to perform best.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 6\n",
      "\n",
      "3. Experiments\n",
      "3.1. Zero-Shot Transfer\n",
      "\n",
      "3.1.1. MOTIVATION\n",
      "\n",
      "In computer vision, zero-shot learning usually refers to the\n",
      "study of generalizing to unseen object categories in image\n",
      "classification (Lampert et al., 2009). We instead use the\n",
      "term in a broader sense and study generalization to unseen\n",
      "datasets. We motivate this as a proxy for performing un-\n",
      "seen tasks, as aspired to in the zero-data learning paper of\n",
      "Larochelle et al. (2008). While much research in the field of\n",
      "unsupervised learning focuses on the representation learn-\n",
      "ing capabilities of machine learning systems, we motivate\n",
      "studying zero-shot transfer as a way of measuring the task-\n",
      "learning capabilities of machine learning systems. In this\n",
      "view, a dataset evaluates performance on a task on a spe-\n",
      "cific distribution. However, many popular computer vision\n",
      "datasets were created by the research community primarily\n",
      "as benchmarks to guide the development of generic image\n",
      "classification methods rather than measuring performance\n",
      "on a specific task. While it is reasonable to say that the\n",
      "SVHN dataset measures the task of street number transcrip-\n",
      "tion on the distribution of Google Street View photos, it is\n",
      "unclear what “real” task the CIFAR-10 dataset measures.\n",
      "It is clear, however, what distribution CIFAR-10 is drawn\n",
      "from - TinyImages (Torralba et al., 2008). On these kinds of\n",
      "datasets, zero-shot transfer is more an evaluation of CLIP’s\n",
      "robustness to distribution shift and domain generalization\n",
      "rather than task generalization. Please see Section 3.3 for\n",
      "analysis focused on this.\n",
      "\n",
      "To our knowledge, Visual N-Grams (Li et al., 2017) first\n",
      "studied zero-shot transfer to existing image classification\n",
      "datasets in the manner described above. It is also the only\n",
      "other work we are aware of that has studied zero-shot trans-\n",
      "fer to standard image classification datasets using a gener-\n",
      "ically pre-trained model and serves as the best reference\n",
      "point for contextualizing CLIP. Their approach learns the\n",
      "parameters of a dictionary of 142,806 visual n-grams (span-\n",
      "ning 1- to 5- grams) and optimizes these n-grams using a\n",
      "differential version of Jelinek-Mercer smoothing to maxi-\n",
      "mize the probability of all text n-grams for a given image.\n",
      "In order to perform zero-shot transfer, they first convert the\n",
      "text of each of the dataset’s class names into its n-gram\n",
      "representation and then compute its probability according\n",
      "to their model, predicting the one with the highest score.\n",
      "\n",
      "Our focus on studying zero-shot transfer as an evaluation of\n",
      "task learning is inspired by work demonstrating task learn-\n",
      "ing in the field of NLP. To our knowledge Liu et al. (2018)\n",
      "first identified task learning as an “unexpected side-effect”\n",
      "when a language model trained to generate Wikipedia ar-\n",
      "ticles learned to reliably transliterate names between lan-\n",
      "guages. While GPT-1 (Radford et al., 2018) focused on pre-\n",
      "\n",
      "training as a transfer learning method to improve supervised\n",
      "fine-tuning, it also included an ablation study demonstrat-\n",
      "ing that the performance of four heuristic zero-shot transfer\n",
      "methods improved steadily over the course of pre-training,\n",
      "without any supervised adaption. This analysis served as the\n",
      "basis for GPT-2 (Radford et al., 2019) which focused exclu-\n",
      "sively on studying the task-learning capabilities of language\n",
      "models via zero-shot transfer.\n",
      "\n",
      "3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER\n",
      "\n",
      "CLIP is pre-trained to predict if an image and a text snippet\n",
      "are paired together in its dataset. To perform zero-shot clas-\n",
      "sification, we reuse this capability. For each dataset, we use\n",
      "the names of all the classes in the dataset as the set of poten-\n",
      "tial text pairings and predict the most probable (image, text)\n",
      "pair according to CLIP. In a bit more detail, we first compute\n",
      "the feature embedding of the image and the feature embed-\n",
      "ding of the set of possible texts by their respective encoders.\n",
      "The cosine similarity of these embeddings is then calculated,\n",
      "scaled by a temperature parameter τ , and normalized into a\n",
      "probability distribution via a softmax. Note that this predic-\n",
      "tion layer is a multinomial logistic regression classifier with\n",
      "L2-normalized inputs, L2-normalized weights, no bias, and\n",
      "temperature scaling. When interpreted this way, the image\n",
      "encoder is the computer vision backbone which computes a\n",
      "feature representation for the image and the text encoder is a\n",
      "hypernetwork (Ha et al., 2016) which generates the weights\n",
      "of a linear classifier based on the text specifying the visual\n",
      "concepts that the classes represent. Lei Ba et al. (2015) first\n",
      "introduced a zero-shot image classifier of this form while\n",
      "the idea of generating a classifier from natural language\n",
      "dates back to at least Elhoseiny et al. (2013). Continuing\n",
      "with this interpretation, every step of CLIP pre-training can\n",
      "be viewed as optimizing the performance of a randomly\n",
      "created proxy to a computer vision dataset which contains 1\n",
      "example per class and has 32,768 total classes defined via\n",
      "natural language descriptions. For zero-shot evaluation, we\n",
      "cache the zero-shot classifier once it has been computed by\n",
      "the text encoder and reuse it for all subsequent predictions.\n",
      "This allows the cost of generating it to be amortized across\n",
      "all the predictions in a dataset.\n",
      "\n",
      "3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS\n",
      "\n",
      "In Table 1 we compare Visual N-Grams to CLIP. The best\n",
      "CLIP model improves accuracy on ImageNet from a proof\n",
      "of concept 11.5% to 76.2% and matches the performance\n",
      "of the original ResNet-50 despite using none of the 1.28\n",
      "million crowd-labeled training examples available for this\n",
      "dataset. Additionally, the top-5 accuracy of CLIP models\n",
      "are noticeably higher than their top-1, and this model has a\n",
      "95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\n",
      "2016). The ability to match the performance of a strong,\n",
      "fully supervised baselines in a zero-shot setting suggests\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 7\n",
      "\n",
      "aYahoo ImageNet SUN\n",
      "\n",
      "Visual N-Grams 72.4 11.5 23.0\n",
      "CLIP 98.4 76.2 58.5\n",
      "\n",
      "Table 1. Comparing CLIP to prior zero-shot transfer image classi-\n",
      "fication results. CLIP improves performance on all three datasets\n",
      "by a large amount. This improvement reflects many differences\n",
      "in the 4 years since the development of Visual N-Grams (Li et al.,\n",
      "2017).\n",
      "\n",
      "CLIP is a significant step towards flexible and practical\n",
      "zero-shot computer vision classifiers. As mentioned above,\n",
      "the comparison to Visual N-Grams is meant for contextu-\n",
      "alizing the performance of CLIP and should not be inter-\n",
      "preted as a direct methods comparison between CLIP and\n",
      "Visual N-Grams as many performance relevant differences\n",
      "between the two systems were not controlled for. For in-\n",
      "stance, we train on a dataset that is 10x larger, use a vision\n",
      "model that requires nearly 100x more compute per predic-\n",
      "tion, likely used over 1000x their training compute, and\n",
      "use a transformer-based model which did not exist when\n",
      "Visual N-Grams was published. As a closer comparison, we\n",
      "trained a CLIP ResNet-50 on the same YFCC100M dataset\n",
      "that Visual N-Grams was trained on and found it matched\n",
      "their reported ImageNet performance within a V100 GPU\n",
      "day. This baseline was also trained from scratch instead of\n",
      "being initialized from pre-trained ImageNet weights as in\n",
      "Visual N-Grams.\n",
      "\n",
      "CLIP also outperforms Visual N-Grams on the other 2 re-\n",
      "ported datasets. On aYahoo, CLIP achieves a 95% reduction\n",
      "in the number of errors, and on SUN, CLIP more than dou-\n",
      "bles the accuracy of Visual N-Grams. To conduct a more\n",
      "comprehensive analysis and stress test, we implement a\n",
      "much larger evaluation suite detailed in Appendix A. In\n",
      "total we expand from the 3 datasets reported in Visual N-\n",
      "Grams to include over 30 datasets and compare to over 50\n",
      "existing computer vision systems to contextualize results.\n",
      "\n",
      "3.1.4. PROMPT ENGINEERING AND ENSEMBLING\n",
      "\n",
      "Most standard image classification datasets treat the infor-\n",
      "mation naming or describing classes which enables natural\n",
      "language based zero-shot transfer as an afterthought. The\n",
      "vast majority of datasets annotate images with just a numeric\n",
      "id of the label and contain a file mapping these ids back to\n",
      "their names in English. Some datasets, such as Flowers102\n",
      "and GTSRB, don’t appear to include this mapping at all\n",
      "in their released versions preventing zero-shot transfer en-\n",
      "tirely.2 For many datasets, we observed these labels may be\n",
      "\n",
      "2Alec learned much more about flower species and German\n",
      "traffic signs over the course of this project than he originally antic-\n",
      "ipated.\n",
      "\n",
      "6.1 9.9 21.5 75.3 265.9\n",
      "Model GFLOPs\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "Sc\n",
      "or\n",
      "\n",
      "e \n",
      "(%\n",
      "\n",
      ")\n",
      "\n",
      "4X efficiency gain\n",
      "\n",
      "5 point\n",
      "improvement\n",
      "\n",
      "RN50\n",
      "\n",
      "RN101\n",
      "\n",
      "RN50x4\n",
      "\n",
      "RN50x16\n",
      "\n",
      "RN50x64\n",
      "\n",
      "Prompt engineering and ensembling\n",
      "Contextless class names (Li et al. 2017)\n",
      "\n",
      "Figure 4. Prompt engineering and ensembling improve zero-\n",
      "shot performance. Compared to the baseline of using contextless\n",
      "class names, prompt engineering and ensembling boost zero-shot\n",
      "classification performance by almost 5 points on average across\n",
      "36 datasets. This improvement is similar to the gain from using\n",
      "4 times more compute with the baseline zero-shot method but is\n",
      "“free” when amortized over many predictions.\n",
      "\n",
      "chosen somewhat haphazardly and do not anticipate issues\n",
      "related to zero-shot transfer which relies on task description\n",
      "in order to transfer successfully.\n",
      "\n",
      "A common issue is polysemy. When the name of a class\n",
      "is the only information provided to CLIP’s text encoder it\n",
      "is unable to differentiate which word sense is meant due to\n",
      "the lack of context. In some cases multiple meanings of the\n",
      "same word might be included as different classes in the same\n",
      "dataset! This happens in ImageNet which contains both\n",
      "construction cranes and cranes that fly. Another example is\n",
      "found in classes of the Oxford-IIIT Pet dataset where the\n",
      "word boxer is, from context, clearly referring to a breed of\n",
      "dog, but to a text encoder lacking context could just as likely\n",
      "refer to a type of athlete.\n",
      "\n",
      "Another issue we encountered is that it’s relatively rare in\n",
      "our pre-training dataset for the text paired with the image\n",
      "to be just a single word. Usually the text is a full sentence\n",
      "describing the image in some way. To help bridge this\n",
      "distribution gap, we found that using the prompt template\n",
      "“A photo of a {label}.” to be a good default that\n",
      "helps specify the text is about the content of the image. This\n",
      "often improves performance over the baseline of using only\n",
      "the label text. For instance, just using this prompt improves\n",
      "accuracy on ImageNet by 1.3%.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 8\n",
      "\n",
      "Similar to the “prompt engineering” discussion around GPT-\n",
      "3 (Brown et al., 2020; Gao et al., 2020), we have also\n",
      "observed that zero-shot performance can be significantly\n",
      "improved by customizing the prompt text to each task. A\n",
      "few, non exhaustive, examples follow. We found on several\n",
      "fine-grained image classification datasets that it helped to\n",
      "specify the category. For example on Oxford-IIIT Pets, us-\n",
      "ing “A photo of a {label}, a type of pet.”\n",
      "to help provide context worked well. Likewise, on Food101\n",
      "specifying a type of food and on FGVC Aircraft a type of\n",
      "aircraft helped too. For OCR datasets, we found that putting\n",
      "quotes around the text or number to be recognized improved\n",
      "performance. Finally, we found that on satellite image classi-\n",
      "fication datasets it helped to specify that the images were of\n",
      "this form and we use variants of “a satellite photo\n",
      "of a {label}.”.\n",
      "\n",
      "We also experimented with ensembling over multiple zero-\n",
      "shot classifiers as another way of improving performance.\n",
      "These classifiers are computed by using different context\n",
      "prompts such as ‘A photo of a big {label}” and\n",
      "“A photo of a small {label}”. We construct the\n",
      "ensemble over the embedding space instead of probability\n",
      "space. This allows us to cache a single set of averaged text\n",
      "embeddings so that the compute cost of the ensemble is the\n",
      "same as using a single classifier when amortized over many\n",
      "predictions. We’ve observed ensembling across many gen-\n",
      "erated zero-shot classifiers to reliably improve performance\n",
      "and use it for the majority of datasets. On ImageNet, we\n",
      "ensemble 80 different context prompts and this improves\n",
      "performance by an additional 3.5% over the single default\n",
      "prompt discussed above. When considered together, prompt\n",
      "engineering and ensembling improve ImageNet accuracy\n",
      "by almost 5%. In Figure 4 we visualize how prompt engi-\n",
      "neering and ensembling change the performance of a set of\n",
      "CLIP models compared to the contextless baseline approach\n",
      "of directly embedding the class name as done in Li et al.\n",
      "(2017).\n",
      "\n",
      "3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE\n",
      "\n",
      "Since task-agnostic zero-shot classifiers for computer vision\n",
      "have been understudied, CLIP provides a promising oppor-\n",
      "tunity to gain a better understanding of this type of model.\n",
      "In this section, we conduct a study of various properties of\n",
      "CLIP’s zero-shot classifiers. As a first question, we look\n",
      "simply at how well zero-shot classifiers perform. To con-\n",
      "textualize this, we compare to the performance of a simple\n",
      "off-the-shelf baseline: fitting a fully supervised, regularized,\n",
      "logistic regression classifier on the features of the canonical\n",
      "ResNet-50. In Figure 5 we show this comparison across 27\n",
      "datasets. Please see Appendix A for details of datasets and\n",
      "setup.\n",
      "\n",
      "Zero-shot CLIP outperforms this baseline slightly more of-\n",
      "\n",
      "40 30 20 10 0 10 20 30 40\n",
      " Score (%)\n",
      "\n",
      "Zero-Shot CLIP vs. Linear Probe on ResNet50\n",
      "\n",
      "EuroSAT-37.1\n",
      "KITTI Distance-34.0\n",
      "PatchCamelyon-19.5\n",
      "GTSRB-18.4\n",
      "CLEVRCounts-18.2\n",
      "DTD-16.6\n",
      "Flowers102-12.5\n",
      "RESISC45-11.9\n",
      "FGVCAircraft-11.3\n",
      "MNIST-10.0\n",
      "Birdsnap-3.2\n",
      "+0.5PascalVOC2007\n",
      "+1.1OxfordPets\n",
      "+1.9ImageNet\n",
      "+2.0Caltech101\n",
      "+2.8FER2013\n",
      "+3.0STL10\n",
      "+3.0CIFAR100\n",
      "+3.9CIFAR10\n",
      "\n",
      "+6.7HatefulMemes\n",
      "+7.7UCF101\n",
      "+7.8SUN397\n",
      "\n",
      "+12.4SST2\n",
      "+14.5Kinetics700\n",
      "\n",
      "+22.5Food101\n",
      "+23.2Country211\n",
      "\n",
      "+28.9StanfordCars\n",
      "\n",
      "Figure 5. Zero-shot CLIP is competitive with a fully super-\n",
      "vised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\n",
      "classifier outperforms a fully supervised linear classifier fitted on\n",
      "ResNet-50 features on 16 datasets, including ImageNet.\n",
      "\n",
      "ten than not and wins on 16 of the 27 datasets. Looking at\n",
      "individual datasets reveals some interesting behavior. On\n",
      "fine-grained classification tasks, we observe a wide spread\n",
      "in performance. On two of these datasets, Stanford Cars and\n",
      "Food101, zero-shot CLIP outperforms logistic regression\n",
      "on ResNet-50 features by over 20% while on two others,\n",
      "Flowers102 and FGVCAircraft, zero-shot CLIP underper-\n",
      "forms by over 10%. On OxfordPets and Birdsnap, per-\n",
      "formance is much closer. We suspect these difference are\n",
      "primarily due to varying amounts of per-task supervision\n",
      "between WIT and ImageNet. On “general” object classifica-\n",
      "tion datasets such as ImageNet, CIFAR10/100, STL10, and\n",
      "PascalVOC2007 performance is relatively similar with a\n",
      "slight advantage for zero-shot CLIP in all cases. On STL10,\n",
      "CLIP achieves 99.3% overall which appears to be a new\n",
      "state of the art despite not using any training examples. Zero-\n",
      "shot CLIP significantly outperforms a ResNet-50 on two\n",
      "datasets measuring action recognition in videos. On Kinet-\n",
      "ics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\n",
      "shot CLIP also outperforms a ResNet-50’s features by 7.7%\n",
      "on UCF101. We speculate this is due to natural language\n",
      "providing wider supervision for visual concepts involving\n",
      "verbs, compared to the noun-centric object supervision in\n",
      "ImageNet.\n",
      "\n",
      "Looking at where zero-shot CLIP notably underperforms,\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 9\n",
      "\n",
      "0 1 2 4 8 16\n",
      "# of labeled training examples per class\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "Av\n",
      "\n",
      "er\n",
      "ag\n",
      "\n",
      "e \n",
      "Sc\n",
      "\n",
      "or\n",
      "e \n",
      "\n",
      "(%\n",
      ")\n",
      "\n",
      "Zero-Shot\n",
      "CLIP BiT-M (ImageNet-21K)\n",
      "\n",
      "Linear Probe CLIP\n",
      "\n",
      "SimCLRv2\n",
      "\n",
      "ResNet50\n",
      "\n",
      "Figure 6. Zero-shot CLIP outperforms few-shot linear probes.\n",
      "Zero-shot CLIP matches the average performance of a 4-shot linear\n",
      "classifier trained on the same feature space and nearly matches the\n",
      "best results of a 16-shot linear classifier across publicly available\n",
      "models. For both BiT-M and SimCLRv2, the best performing\n",
      "model is highlighted. Light gray lines are other models in the eval\n",
      "suite. The 20 datasets with at least 16 examples per class were\n",
      "used in this analysis.\n",
      "\n",
      "we see that zero-shot CLIP is quite weak on several spe-\n",
      "cialized, complex, or abstract tasks such as satellite image\n",
      "classification (EuroSAT and RESISC45), lymph node tumor\n",
      "detection (PatchCamelyon), counting objects in synthetic\n",
      "scenes (CLEVRCounts), self-driving related tasks such as\n",
      "German traffic sign recognition (GTSRB), recognizing dis-\n",
      "tance to the nearest car (KITTI Distance). These results\n",
      "highlight the poor capability of zero-shot CLIP on more\n",
      "complex tasks. By contrast, non-expert humans can robustly\n",
      "perform several of these tasks, such as counting, satellite\n",
      "image classification, and traffic sign recognition, suggesting\n",
      "significant room for improvement. However, we caution\n",
      "that it is unclear whether measuring zero-shot transfer, as\n",
      "opposed to few-shot transfer, is a meaningful evaluation for\n",
      "difficult tasks that a learner has no prior experience with,\n",
      "such as lymph node tumor classification for almost all hu-\n",
      "mans (and possibly CLIP).\n",
      "\n",
      "While comparing zero-shot performance to fully supervised\n",
      "models contextualizes the task-learning capabilities of CLIP,\n",
      "comparing to few-shot methods is a more direct compari-\n",
      "son, since zero-shot is its limit. In Figure 6, we visualize\n",
      "how zero-shot CLIP compares to few-shot logistic regres-\n",
      "sion on the features of many image models including the\n",
      "best publicly available ImageNet models, self-supervised\n",
      "learning methods, and CLIP itself. While it is intuitive to\n",
      "\n",
      "expect zero-shot to underperform one-shot, we instead find\n",
      "that zero-shot CLIP matches the performance of 4-shot lo-\n",
      "gistic regression on the same feature space. This is likely\n",
      "due to an important difference between the zero-shot and\n",
      "few-shot approach. First, CLIP’s zero-shot classifier is gen-\n",
      "erated via natural language which allows for visual concepts\n",
      "to be directly specified (“communicated”). By contrast,\n",
      "“normal” supervised learning must infer concepts indirectly\n",
      "from training examples. Context-less example-based learn-\n",
      "ing has the drawback that many different hypotheses can\n",
      "be consistent with the data, especially in the one-shot case.\n",
      "A single image often contains many different visual con-\n",
      "cepts. Although a capable learner is able to exploit visual\n",
      "cues and heuristics, such as assuming that the concept being\n",
      "demonstrated is the primary object in an image, there is no\n",
      "guarantee.\n",
      "\n",
      "A potential resolution of this discrepancy between zero-\n",
      "shot and few-shot performance is to use CLIP’s zero-shot\n",
      "classifier as a prior for the weights of the few-shot classifier.\n",
      "While adding an L2 penalty towards the generated weights\n",
      "is a straightforward implementation of this idea, we found\n",
      "that hyperparameter optimization would often select for\n",
      "such a large value of this regularizer that the resulting few-\n",
      "shot classifier was “just” the zero-shot classifier. Research\n",
      "into better methods of combining the strength of zero-shot\n",
      "transfer with flexibility of few-shot learning is a promising\n",
      "direction for future work.\n",
      "\n",
      "When comparing zero-shot CLIP to few-shot logistic re-\n",
      "gression on the features of other models, zero-shot CLIP\n",
      "roughly matches the performance of the best performing\n",
      "16-shot classifier in our evaluation suite, which uses the fea-\n",
      "tures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\n",
      "We are certain that a BiT-L model trained on JFT-300M\n",
      "would perform even better but these models have not been\n",
      "publicly released. That a BiT-M ResNet-152x2 performs\n",
      "best in a 16-shot setting is somewhat surprising since, as\n",
      "analyzed in Section 3.2, the Noisy Student EfficientNet-L2\n",
      "outperforms it in a fully supervised setting by almost 5% on\n",
      "average across 27 datasets.\n",
      "\n",
      "In addition to studying the average performance of zero-shot\n",
      "CLIP and few-shot logistic regression, we also examine\n",
      "performance on individual datasets. In Figure 7, we show\n",
      "estimates for the number of labeled examples per class that\n",
      "a logistic regression classifier on the same feature space\n",
      "requires to match the performance of zero-shot CLIP. Since\n",
      "zero-shot CLIP is also a linear classifier, this estimates the\n",
      "effective data efficiency of zero-shot transfer in this setting.\n",
      "In order to avoid training thousands of linear classifiers,\n",
      "we estimate the effective data efficiency based on a log-\n",
      "linear interpolation of the performance of a 1, 2, 4, 8, 16-\n",
      "shot (when possible), and a fully supervised linear classifier\n",
      "trained on each dataset. We find that zero-shot transfer can\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 10\n",
      "\n",
      "0 25 50 75 100 125 150 175 200\n",
      "# of labeled examples per class\n",
      "\n",
      "required to match zero-shot\n",
      "\n",
      "Flowers102\n",
      "EuroSAT\n",
      "\n",
      "RESISC45\n",
      "CLEVRCounts\n",
      "\n",
      "GTSRB\n",
      "FGVCAircraft\n",
      "\n",
      "DTD\n",
      "Birdsnap\n",
      "UCF101\n",
      "\n",
      "KITTI Distance\n",
      "Caltech101\n",
      "\n",
      "SUN397\n",
      "MNIST\n",
      "\n",
      "StanfordCars\n",
      "HatefulMemes\n",
      "\n",
      "CIFAR100\n",
      "STL10\n",
      "\n",
      "Kinetics700\n",
      "SST2\n",
      "\n",
      "PCam\n",
      "ImageNet\n",
      "\n",
      "Country211\n",
      "OxfordPets\n",
      "\n",
      "Food101\n",
      "CIFAR10\n",
      "FER2013\n",
      "\n",
      "0.9\n",
      "0.9\n",
      "1.5\n",
      "1.5\n",
      "1.6\n",
      "2.0\n",
      "2.6\n",
      "2.7\n",
      "2.9\n",
      "2.9\n",
      "3.5\n",
      "3.9\n",
      "4.8\n",
      "6.0\n",
      "9.8\n",
      "12.0\n",
      "12.7\n",
      "13.6\n",
      "14.4\n",
      "14.7\n",
      "16.0\n",
      "\n",
      "32\n",
      "48\n",
      "\n",
      "64\n",
      "81\n",
      "\n",
      "184\n",
      "\n",
      "Median: 5.4\n",
      "Mean:  20.8\n",
      "\n",
      "Figure 7. The data efficiency of zero-shot transfer varies\n",
      "widely. Calculating the number of labeled examples per class\n",
      "a linear classifier on the same CLIP feature space requires to match\n",
      "the performance of the zero-shot classifier contextualizes the ef-\n",
      "fectiveness of zero-shot transfer. Values are estimated based on\n",
      "log-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\n",
      "results. Performance varies widely from still underperforming a\n",
      "one-shot classifier on two datasets to matching an estimated 184\n",
      "labeled examples per class.\n",
      "\n",
      "have widely varying efficiency per dataset from less than 1\n",
      "labeled example per class to 184. Two datasets, Flowers102\n",
      "and EuroSAT underperform one-shot models. Half of the\n",
      "datasets require less than 5 examples per class with a median\n",
      "of 5.4. However, the mean estimated data efficiency is 20.8\n",
      "examples per class. This is due to the 20% of datasets\n",
      "where supervised classifiers require many labeled examples\n",
      "per class in order to match performance. On ImageNet,\n",
      "zero-shot CLIP matches the performance of a 16-shot linear\n",
      "classifier trained on the same feature space.\n",
      "\n",
      "If we assume that evaluation datasets are large enough that\n",
      "the parameters of linear classifiers trained on them are well\n",
      "estimated, then, because CLIP’s zero-shot classifier is also\n",
      "a linear classifier, the performance of the fully supervised\n",
      "classifiers roughly sets an upper bound for what zero-shot\n",
      "transfer can achieve. In Figure 8 we compare CLIP’s zero-\n",
      "shot performance with fully supervised linear classifiers\n",
      "across datasets. The dashed, y = x line represents an “op-\n",
      "timal” zero-shot classifier that matches the performance of\n",
      "its fully supervised equivalent. For most datasets, the per-\n",
      "formance of zero-shot classifiers still underperform fully su-\n",
      "pervised classifiers by 10% to 25%, suggesting that there is\n",
      "still plenty of headroom for improving CLIP’s task-learning\n",
      "and zero-shot transfer capabilities.\n",
      "\n",
      "There is a positive correlation of 0.82 (p-value < 10−6)\n",
      "between zero-shot performance and fully supervised perfor-\n",
      "\n",
      "20 30 40 50 60 70 80 90 100\n",
      "Linear Probe CLIP Performance\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "Ze\n",
      "ro\n",
      "\n",
      "-S\n",
      "ho\n",
      "\n",
      "t C\n",
      "LI\n",
      "\n",
      "P \n",
      "Pe\n",
      "\n",
      "rfo\n",
      "rm\n",
      "\n",
      "an\n",
      "ce\n",
      "\n",
      "r = 0.82\n",
      "\n",
      "VOC2007\n",
      "\n",
      "Country211\n",
      "\n",
      "HatefulMemes\n",
      "\n",
      "MNIST\n",
      "\n",
      "CIFAR10\n",
      "\n",
      "SST2\n",
      "\n",
      "DTD\n",
      "\n",
      "PCAM\n",
      "\n",
      "RESISC45\n",
      "\n",
      "EuroSAT\n",
      "\n",
      "GTSRB\n",
      "\n",
      "CLEVRCounts\n",
      "\n",
      "FER2013\n",
      "\n",
      "UCF101\n",
      "\n",
      "Birdsnap\n",
      "\n",
      "OxfordPets\n",
      "\n",
      "CIFAR100\n",
      "\n",
      "FGVCAircraft\n",
      "\n",
      "Food101\n",
      "\n",
      "Flowers102Stanford Cars\n",
      "\n",
      "Caltech101\n",
      "\n",
      "SUN397\n",
      "\n",
      "ImageNet\n",
      "\n",
      "STL10\n",
      "\n",
      "KITTI Distance\n",
      "\n",
      "Kinetics700\n",
      "\n",
      "Figure 8. Zero-shot performance is correlated with linear\n",
      "probe performance but still mostly sub-optimal. Comparing\n",
      "zero-shot and linear probe performance across datasets shows a\n",
      "strong correlation with zero-shot performance mostly shifted 10 to\n",
      "25 points lower. On only 5 datasets does zero-shot performance\n",
      "approach linear probe performance (≤3 point difference).\n",
      "\n",
      "mance, suggesting that CLIP is relatively consistent at con-\n",
      "necting underlying representation and task learning to zero-\n",
      "shot transfer. However, zero-shot CLIP only approaches\n",
      "fully supervised performance on 5 datasets: STL10, CI-\n",
      "FAR10, Food101, OxfordPets, and Caltech101. On all 5\n",
      "datasets, both zero-shot accuracy and fully supervised accu-\n",
      "racy are over 90%. This suggests that CLIP may be more\n",
      "effective at zero-shot transfer for tasks where its underly-\n",
      "ing representations are also high quality. The slope of a\n",
      "linear regression model predicting zero-shot performance\n",
      "as a function of fully supervised performance estimates that\n",
      "for every 1% improvement in fully supervised performance,\n",
      "zero-shot performance improves by 1.28%. However, the\n",
      "95th-percentile confidence intervals still include values of\n",
      "less than 1 (0.93-1.79).\n",
      "\n",
      "Over the past few years, empirical studies of deep learning\n",
      "systems have documented that performance is predictable as\n",
      "a function of important quantities such as training compute\n",
      "and dataset size (Hestness et al., 2017; Kaplan et al., 2020).\n",
      "The GPT family of models has so far demonstrated consis-\n",
      "tent improvements in zero-shot performance across a 1000x\n",
      "increase in training compute. In Figure 9, we check whether\n",
      "the zero-shot performance of CLIP follows a similar scaling\n",
      "pattern. We plot the average error rate of the 5 ResNet CLIP\n",
      "models across 39 evaluations on 36 different datasets and\n",
      "find that a similar log-log linear scaling trend holds for CLIP\n",
      "across a 44x increase in model compute. While the overall\n",
      "trend is smooth, we found that performance on individual\n",
      "evaluations can be much noisier. We are unsure whether\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 11\n",
      "\n",
      "6.1 9.9 21.5 75.3 265.9\n",
      "Model GFLOPs\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "Er\n",
      "ro\n",
      "\n",
      "r (\n",
      "%\n",
      "\n",
      ")\n",
      "\n",
      "RN50\n",
      "\n",
      "RN101\n",
      "\n",
      "RN50x4\n",
      "\n",
      "RN50x16\n",
      "\n",
      "RN50x64\n",
      "\n",
      "Figure 9. Zero-shot CLIP performance scales smoothly as a\n",
      "function of model compute. Across 39 evals on 36 different\n",
      "datasets, average zero-shot error is well modeled by a log-log lin-\n",
      "ear trend across a 44x range of compute spanning 5 different CLIP\n",
      "models. Lightly shaded lines are performance on individual evals,\n",
      "showing that performance is much more varied despite the smooth\n",
      "overall trend.\n",
      "\n",
      "this is caused by high variance between individual training\n",
      "runs on sub-tasks (as documented in D’Amour et al. (2020))\n",
      "masking a steadily improving trend or whether performance\n",
      "is actually non-monotonic as a function of compute on some\n",
      "tasks.\n",
      "\n",
      "3.2. Representation Learning\n",
      "\n",
      "While we have extensively analyzed the task-learning ca-\n",
      "pabilities of CLIP through zero-shot transfer in the previ-\n",
      "ous section, it is more common to study the representation\n",
      "learning capabilities of a model. There exist many ways to\n",
      "evaluate the quality of representations as well as disagree-\n",
      "ments over what properties an “ideal” representation should\n",
      "have (Locatello et al., 2020). Fitting a linear classifier on\n",
      "a representation extracted from the model and measuring\n",
      "its performance on various datasets is a common approach.\n",
      "An alternative is measuring the performance of end-to-end\n",
      "fine-tuning of the model. This increases flexibility, and\n",
      "prior work has convincingly demonstrated that fine-tuning\n",
      "outperforms linear classification on most image classifi-\n",
      "cation datasets (Kornblith et al., 2019; Zhai et al., 2019).\n",
      "While the high performance of fine-tuning motivates its\n",
      "study for practical reasons, we still opt for linear classifier\n",
      "based evaluation for several reasons. Our work is focused\n",
      "on developing a high-performing task and dataset-agnostic\n",
      "pre-training approach. Fine-tuning, because it adapts rep-\n",
      "resentations to each dataset during the fine-tuning phase,\n",
      "can compensate for and potentially mask failures to learn\n",
      "general and robust representations during the pre-training\n",
      "phase. Linear classifiers, because of their limited flexibility,\n",
      "instead highlight these failures and provide clear feedback\n",
      "during development. For CLIP, training supervised linear\n",
      "\n",
      "classifiers has the added benefit of being very similar to the\n",
      "approach used for its zero-shot classifiers which enables\n",
      "extensive comparisons and analysis in Section 3.1. Finally,\n",
      "we aim to compare CLIP to a comprehensive set of existing\n",
      "models across many tasks. Studying 66 different models on\n",
      "27 different datasets requires tuning 1782 different evalua-\n",
      "tions. Fine-tuning opens up a much larger design and hyper-\n",
      "parameter space, which makes it difficult to fairly evaluate\n",
      "and computationally expensive to compare a diverse set of\n",
      "techniques as discussed in other large scale empirical studies\n",
      "(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\n",
      "classifiers require minimal hyper-parameter tuning and have\n",
      "standardized implementations and evaluation procedures.\n",
      "Please see Appendix A for further details on evaluation.\n",
      "\n",
      "Figure 10 summarizes our findings. To minimize selection\n",
      "effects that could raise concerns of confirmation or reporting\n",
      "bias, we first study performance on the 12 dataset evaluation\n",
      "suite from Kornblith et al. (2019). While small CLIP mod-\n",
      "els such as a ResNet-50 and ResNet-101 outperform other\n",
      "ResNets trained on ImageNet-1K (BiT-S and the originals),\n",
      "they underperform ResNets trained on ImageNet-21K (BiT-\n",
      "M). These small CLIP models also underperform models\n",
      "in the EfficientNet family with similar compute require-\n",
      "ments. However, models trained with CLIP scale very well\n",
      "and the largest model we trained (ResNet-50x64) slightly\n",
      "outperforms the best performing existing model (a Noisy\n",
      "Student EfficientNet-L2) on both overall score and compute\n",
      "efficiency. We also find that CLIP vision transformers are\n",
      "about 3x more compute efficient than CLIP ResNets, which\n",
      "allows us to reach higher overall performance within our\n",
      "compute budget. These results qualitatively replicate the\n",
      "findings of Dosovitskiy et al. (2020) which reported that\n",
      "vision transformers are more compute efficient than con-\n",
      "vnets when trained on sufficiently large datasets. Our best\n",
      "overall model is a ViT-L/14 that is fine-tuned at a higher res-\n",
      "olution of 336 pixels on our dataset for 1 additional epoch.\n",
      "This model outperforms the best existing model across this\n",
      "evaluation suite by an average of 2.6%.\n",
      "\n",
      "As Figure 21 qualitatively shows, CLIP models learn a wider\n",
      "set of tasks than has previously been demonstrated in a sin-\n",
      "gle computer vision model trained end-to-end from random\n",
      "initialization. These tasks include geo-localization, optical\n",
      "character recognition, facial emotion recognition, and action\n",
      "recognition. None of these tasks are measured in the evalua-\n",
      "tion suite of Kornblith et al. (2019). This could be argued\n",
      "to be a form of selection bias in Kornblith et al. (2019)’s\n",
      "study towards tasks that overlap with ImageNet. To address\n",
      "this, we also measure performance on a broader 27 dataset\n",
      "evaluation suite. This evaluation suite, detailed in Appendix\n",
      "A includes datasets representing the aforementioned tasks,\n",
      "German Traffic Signs Recognition Benchmark (Stallkamp\n",
      "et al., 2011), as well as several other datasets adapted from\n",
      "VTAB (Zhai et al., 2019).\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 12\n",
      "\n",
      "100 101 102\n",
      "\n",
      "Forward-pass GFLOPs/image\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "Sc\n",
      "or\n",
      "\n",
      "e \n",
      "(%\n",
      "\n",
      ")\n",
      "Linear probe average over Kornblith et al.'s 12 datasets\n",
      "\n",
      "100 101 102\n",
      "\n",
      "Forward-pass GFLOPs/image\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "Sc\n",
      "or\n",
      "\n",
      "e \n",
      "(%\n",
      "\n",
      ")\n",
      "\n",
      "Linear probe average over all 27 datasets\n",
      "\n",
      "CLIP-ViT\n",
      "CLIP-ResNet\n",
      "EfficientNet-NoisyStudent\n",
      "EfficientNet\n",
      "\n",
      "Instagram-pretrained\n",
      "SimCLRv2\n",
      "BYOL\n",
      "MoCo\n",
      "\n",
      "ViT (ImageNet-21k)\n",
      "BiT-M\n",
      "BiT-S\n",
      "ResNet\n",
      "\n",
      "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\n",
      "EfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\n",
      "Touvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\n",
      "2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\n",
      "(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or\n",
      "evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.\n",
      "\n",
      "On this broader evaluation suite, the benefits of CLIP are\n",
      "more clear. All CLIP models, regardless of scale, outper-\n",
      "form all evaluated systems in terms of compute efficiency.\n",
      "The improvement in average score of the best model over\n",
      "previous systems increases from 2.6% to 5%. We also find\n",
      "that self-supervised systems do noticeably better on our\n",
      "broader evaluation suite. For instance, while SimCLRv2\n",
      "still underperforms BiT-M on average on the 12 datasets\n",
      "of Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\n",
      "on our 27 dataset evaluation suite. These findings suggest\n",
      "continuing to expand task diversity and coverage in order\n",
      "to better understand the “general” performance of systems.\n",
      "We suspect additional evaluation efforts along the lines of\n",
      "VTAB to be valuable.\n",
      "\n",
      "In addition to the aggregate analysis above, we visualize\n",
      "per-dataset differences in the performance of the best CLIP\n",
      "model and the best model in our evaluation suite across\n",
      "all 27 datasets in Figure 11. CLIP outperforms the Noisy\n",
      "Student EfficientNet-L2 on 21 of the 27 datasets. CLIP\n",
      "improves the most on tasks which require OCR (SST2\n",
      "\n",
      "and HatefulMemes), geo-localization and scene recognition\n",
      "(Country211, SUN397), and activity recognition in videos\n",
      "(Kinetics700 and UCF101). In addition CLIP also does\n",
      "much better on fine-grained car and traffic sign recognition\n",
      "(Stanford Cars and GTSRB). This may reflect a problem\n",
      "with overly narrow supervision in ImageNet. A result such\n",
      "as the 14.7% improvement on GTSRB could be indicative\n",
      "of an issue with ImageNet-1K, which has only a single la-\n",
      "bel for all traffic and street signs. This could encourage\n",
      "a supervised representation to collapse intra-class details\n",
      "and hurt accuracy on a fine-grained downstream task. As\n",
      "mentioned, CLIP still underperforms the EfficientNet on\n",
      "several datasets. Unsurprisingly, the dataset that the Effi-\n",
      "cientNet does best relative to CLIP on is the one it was\n",
      "trained on: ImageNet. The EffcientNet also slightly outper-\n",
      "forms CLIP on low-resolution datasets such as CIFAR10\n",
      "and CIFAR100. We suspect this is at least partly due to the\n",
      "lack of scale-based data augmentation in CLIP. The Effi-\n",
      "cientNet also does slightly better on PatchCamelyon and\n",
      "CLEVRCounts, datasets where overall performance is still\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 13\n",
      "\n",
      "10 5 0 5 10 15 20 25\n",
      " Score (%)\n",
      "\n",
      "Logistic Regression on CLIP vs. EfficientNet L2 NS\n",
      "\n",
      "ImageNet-3.0\n",
      "CLEVRCounts-2.4\n",
      "CIFAR100-1.7\n",
      "PatchCamelyon-1.2\n",
      "CIFAR10-0.8\n",
      "OxfordPets-0.5\n",
      "+0.0STL10\n",
      "+0.5VOC2007\n",
      "+0.5DTD\n",
      "+0.6MNIST\n",
      "+0.9EuroSAT\n",
      "+1.3Caltech101\n",
      "+1.4Flowers102\n",
      "+1.4Birdsnap\n",
      "+2.3KITTI Distance\n",
      "+3.1UCF101\n",
      "+3.2FGVCAircraft\n",
      "+3.9Food101\n",
      "+4.5FER2013\n",
      "+5.1RESISC45\n",
      "\n",
      "+6.2Kinetics700\n",
      "+6.5SUN397\n",
      "\n",
      "+14.7GTSRB\n",
      "+15.9StanfordCars\n",
      "\n",
      "+18.8HatefulMemes\n",
      "+22.7Country211\n",
      "\n",
      "+23.6SST2\n",
      "\n",
      "Figure 11. CLIP’s features outperform the features of the best\n",
      "ImageNet model on a wide variety of datasets. Fitting a linear\n",
      "classifier on CLIP’s features outperforms using the Noisy Student\n",
      "EfficientNet-L2 on 21 out of 27 datasets.\n",
      "\n",
      "low for both approaches.\n",
      "\n",
      "3.3. Robustness to Natural Distribution Shift\n",
      "\n",
      "In 2015, it was announced that a deep learning model ex-\n",
      "ceeded human performance on the ImageNet test set (He\n",
      "et al., 2015). However, research in the subsequent years\n",
      "has repeatedly found that these models still make many sim-\n",
      "ple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\n",
      "Alcorn et al., 2019), and new benchmarks testing these sys-\n",
      "tems has often found their performance to be much lower\n",
      "than both their ImageNet accuracy and human accuracy\n",
      "(Recht et al., 2019; Barbu et al., 2019). What explains this\n",
      "discrepancy? Various ideas have been suggested and stud-\n",
      "ied (Ilyas et al., 2019; Geirhos et al., 2020). A common\n",
      "theme of proposed explanations is that deep learning models\n",
      "are exceedingly adept at finding correlations and patterns\n",
      "which hold across their training dataset and thus improve\n",
      "in-distribution performance. However many of these corre-\n",
      "lations and patterns are actually spurious and do not hold for\n",
      "other distributions and result in large drops in performance\n",
      "on other datasets.\n",
      "\n",
      "We caution that, to date, most of these studies limit their\n",
      "evaluation to models trained on ImageNet. Recalling the\n",
      "topic of discussion, it may be a mistake to generalize too\n",
      "far from these initial findings. To what degree are these\n",
      "failures attributable to deep learning, ImageNet, or some\n",
      "\n",
      "combination of the two? CLIP models, which are trained via\n",
      "natural language supervision on a very large dataset and are\n",
      "capable of high zero-shot performance, are an opportunity\n",
      "to investigate this question from a different angle.\n",
      "\n",
      "Taori et al. (2020) is a recent comprehensive study mov-\n",
      "ing towards quantifying and understanding these behaviors\n",
      "for ImageNet models. Taori et al. (2020) study how the\n",
      "performance of ImageNet models change when evaluated\n",
      "on natural distribution shifts. They measure performance\n",
      "on a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\n",
      "2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\n",
      "and ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\n",
      "et al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\n",
      "and ImageNet Rendition (Hendrycks et al., 2020a). They\n",
      "distinguish these datasets, which all consist of novel images\n",
      "collected from a variety of sources, from synthetic distri-\n",
      "bution shifts such as ImageNet-C (Hendrycks & Dietterich,\n",
      "2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\n",
      "sarial attacks (Goodfellow et al., 2014) which are created by\n",
      "perturbing existing images in various ways. They propose\n",
      "this distinction because in part because they find that while\n",
      "several techniques have been demonstrated to improve per-\n",
      "formance on synthetic distribution shifts, they often fail to\n",
      "yield consistent improvements on natural distributions.3\n",
      "\n",
      "Across these collected datasets, the accuracy of ImageNet\n",
      "models drop well below the expectation set by the Ima-\n",
      "geNet validation set. For the following summary discussion\n",
      "we report average accuracy across all 7 natural distribution\n",
      "shift datasets and average accuracy across the correspond-\n",
      "ing class subsets of ImageNet unless otherwise specified.\n",
      "Additionally, for Youtube-BB and ImageNet-Vid, which\n",
      "have two different evaluation settings, we use the average\n",
      "of pm-0 and pm-10 accuracy.\n",
      "\n",
      "A ResNet-101 makes 5 times as many mistakes when eval-\n",
      "uated on these natural distribution shifts compared to the\n",
      "ImageNet validation set. Encouragingly however, Taori et al.\n",
      "(2020) find that accuracy under distribution shift increases\n",
      "predictably with ImageNet accuracy and is well modeled\n",
      "as a linear function of logit-transformed accuracy. Taori\n",
      "et al. (2020) use this finding to propose that robustness\n",
      "analysis should distinguish between effective and relative\n",
      "robustness. Effective robustness measures improvements\n",
      "in accuracy under distribution shift above what is predicted\n",
      "by the documented relationship between in-distribution and\n",
      "out-of-distribution accuracy. Relative robustness captures\n",
      "any improvement in out-of-distribution accuracy. Taori et al.\n",
      "(2020) argue that robustness techniques should aim to im-\n",
      "prove both effective robustness and relative robustness.\n",
      "\n",
      "Almost all models studied in Taori et al. (2020) are trained\n",
      "3We refer readers to Hendrycks et al. (2020a) for additional\n",
      "\n",
      "experiments and discussion on this claim.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 14\n",
      "\n",
      "65 70 75 80 85 90\n",
      "ImageNet Score (%)\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "Tr\n",
      "an\n",
      "\n",
      "sf\n",
      "er\n",
      "\n",
      " S\n",
      "co\n",
      "\n",
      "re\n",
      " (%\n",
      "\n",
      ")\n",
      "Linear probe average over Kornblith et al.'s 12 datasets\n",
      "\n",
      "65 70 75 80 85 90\n",
      "ImageNet Score (%)\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "Tr\n",
      "an\n",
      "\n",
      "sf\n",
      "er\n",
      "\n",
      " S\n",
      "co\n",
      "\n",
      "re\n",
      " (%\n",
      "\n",
      ")\n",
      "\n",
      "Linear probe average over 26 datasets\n",
      "\n",
      "CLIP-ViT\n",
      "CLIP-ResNet\n",
      "EfficientNet-NoisyStudent\n",
      "EfficientNet\n",
      "\n",
      "Instagram\n",
      "SimCLRv2\n",
      "BYOL\n",
      "MoCo\n",
      "\n",
      "ViT (ImageNet-21k)\n",
      "BiT-M\n",
      "BiT-S\n",
      "ResNet\n",
      "\n",
      "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\n",
      "splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\n",
      "ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.\n",
      "\n",
      "or fine-tuned on the ImageNet dataset. Returning to the\n",
      "discussion in the introduction to this section - is training\n",
      "or adapting to the ImageNet dataset distribution the cause\n",
      "of the observed robustness gap? Intuitively, a zero-shot\n",
      "model should not be able to exploit spurious correlations\n",
      "or patterns that hold only on a specific distribution, since it\n",
      "is not trained on that distribution. 4 Thus it is reasonable\n",
      "to expect zero-shot models to have much higher effective\n",
      "robustness. In Figure 13, we compare the performance of\n",
      "zero-shot CLIP with existing ImageNet models on natural\n",
      "distribution shifts. All zero-shot CLIP models improve\n",
      "effective robustness by a large amount and reduce the size\n",
      "of the gap between ImageNet accuracy and accuracy under\n",
      "distribution shift by up to 75%.\n",
      "\n",
      "While these results show that zero-shot models can be much\n",
      "more robust, they do not necessarily mean that supervised\n",
      "learning on ImageNet causes a robustness gap. Other details\n",
      "of CLIP, such as its large and diverse pre-training dataset\n",
      "or use of natural language supervision could also result\n",
      "\n",
      "4We caution that a zero-shot model can still exploit spurious\n",
      "correlations that are shared between the pre-training and evaluation\n",
      "distributions.\n",
      "\n",
      "in much more robust models regardless of whether they\n",
      "are zero-shot or fine-tuned. As an initial experiment to\n",
      "potentially begin narrowing this down, we also measure\n",
      "how the performance of CLIP models change after adapting\n",
      "to the ImageNet distribution via a L2 regularized logistic\n",
      "regression classifier fit to CLIP features on the ImageNet\n",
      "training set. We visualize how performance changes from\n",
      "the zero-shot classifier in Figure 14. Although adapting\n",
      "CLIP to the ImageNet distribution increases its ImageNet\n",
      "accuracy by 9.2% to 85.4% overall, and ties the accuracy\n",
      "of the 2018 SOTA from Mahajan et al. (2018), average\n",
      "accuracy under distribution shift slightly decreases.\n",
      "\n",
      "It is surprising to see a 9.2% increase in accuracy, which cor-\n",
      "responds to roughly 3 years of improvement in SOTA, fail\n",
      "to translate into any improvement in average performance\n",
      "under distribution shift. We also break down the differences\n",
      "between zero-shot accuracy and linear classifier accuracy\n",
      "per dataset in Figure 14 and find performance still increases\n",
      "significantly on one dataset, ImageNetV2. ImageNetV2\n",
      "closely followed the creation process of the original Ima-\n",
      "geNet dataset which suggests that gains in accuracy from\n",
      "supervised adaptation are closely concentrated around the\n",
      "ImageNet distribution. Performance decreases by 4.7% on\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 15\n",
      "\n",
      "65 70 75 80 85 90 95 100\n",
      "Average on class subsampled ImageNet (top-1, %)\n",
      "\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "\n",
      "100\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "on\n",
      " 7\n",
      "\n",
      " n\n",
      "at\n",
      "\n",
      "ur\n",
      "al\n",
      "\n",
      " d\n",
      "ist\n",
      "\n",
      "rib\n",
      "ut\n",
      "\n",
      "io\n",
      "n \n",
      "\n",
      "sh\n",
      "ift\n",
      "\n",
      " d\n",
      "at\n",
      "\n",
      "as\n",
      "et\n",
      "\n",
      "s (\n",
      "to\n",
      "\n",
      "p-\n",
      "1,\n",
      "\n",
      " %\n",
      ")\n",
      "\n",
      "Ideal robust model (y = x)\n",
      "Zero-Shot CLIP\n",
      "Standard ImageNet training\n",
      "Exisiting robustness techniques ImageNet\n",
      "\n",
      "ImageNetV2\n",
      "\n",
      "ImageNet-A\n",
      "\n",
      "ImageNet-R\n",
      "\n",
      "ObjectNet\n",
      "\n",
      "ImageNet \n",
      "Sketch\n",
      "\n",
      "76.2 76.2\n",
      "\n",
      "64.3 70.1\n",
      "\n",
      "2.7 77.1\n",
      "\n",
      "37.7 88.9\n",
      "\n",
      "32.6 72.3\n",
      "\n",
      "25.2 60.2\n",
      "\n",
      "ImageNet\n",
      "ResNet101\n",
      "\n",
      "Zero-Shot\n",
      "CLIP\n",
      "\n",
      "0%\n",
      "\n",
      "+5.8%\n",
      "\n",
      "+74.4%\n",
      "\n",
      "+51.2%\n",
      "\n",
      "+39.7%\n",
      "\n",
      "+35.0%\n",
      "\n",
      "Δ ScoreDataset Examples\n",
      "\n",
      "Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\n",
      "(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\n",
      "this “robustness gap” by up to 75%. Linear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals.\n",
      "(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\n",
      "the best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\n",
      "set, ResNet-101.\n",
      "\n",
      "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\n",
      "and 1.9% on ImageNet-A. The change in accuracy on the\n",
      "two other datasets, Youtube-BB and ImageNet Vid, is in-\n",
      "significant.\n",
      "\n",
      "How is it possible to improve accuracy by 9.2% on the Im-\n",
      "ageNet dataset with little to no increase in accuracy under\n",
      "distribution shift? Is the gain primarily from “exploiting\n",
      "spurious correlations”? Is this behavior unique to some com-\n",
      "bination of CLIP, the ImageNet datatset, and the distribution\n",
      "shifts studied, or a more general phenomena? Does it hold\n",
      "for end-to-end finetuning as well as linear classifiers? We\n",
      "do not have confident answers to these questions at this time.\n",
      "Prior work has also pre-trained models on distributions other\n",
      "than ImageNet, but it is common to study and release mod-\n",
      "els only after they have been fine-tuned to ImageNet. As a\n",
      "step towards understanding whether pre-trained zero-shot\n",
      "models consistently have higher effective robustness than\n",
      "fine-tuned models, we encourage the authors of Mahajan\n",
      "et al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\n",
      "(2020) to, if possible, study these questions on their models\n",
      "as well.\n",
      "\n",
      "We also investigate another robustness intervention enabled\n",
      "by flexible zero-shot natural-language-based image classi-\n",
      "fiers. The target classes across the 7 transfer datasets are\n",
      "not always perfectly aligned with those of ImageNet. Two\n",
      "datasets, Youtube-BB and ImageNet-Vid, consist of super-\n",
      "classes of ImageNet. This presents a problem when trying\n",
      "to use the fixed 1000-way classifier of an ImageNet model\n",
      "to make predictions. Taori et al. (2020) handle this by max-\n",
      "\n",
      "pooling predictions across all sub-classes according to the\n",
      "ImageNet class hierarchy. Sometimes this mapping is much\n",
      "less than perfect. For the person class in Youtube-BB, pre-\n",
      "dictions are made by pooling over the ImageNet classes for\n",
      "a baseball player, a bridegroom, and a scuba diver. With\n",
      "CLIP we can instead generate a custom zero-shot classi-\n",
      "fier for each dataset directly based on its class names. In\n",
      "Figure 14 we see that this improves average effective ro-\n",
      "bustness by 5% but is concentrated in large improvements\n",
      "on only a few datasets. Curiously, accuracy on ObjectNet\n",
      "also increases by 2.3%. Although the dataset was designed\n",
      "to closely overlap with ImageNet classes, using the names\n",
      "provided for each class by ObjectNet’s creators still helps a\n",
      "small amount compared to using ImageNet class names and\n",
      "pooling predictions when necessary.\n",
      "\n",
      "While zero-shot CLIP improves effective robustness, Figure\n",
      "14 shows that the benefit is almost entirely gone in a fully\n",
      "supervised setting. To better understand this difference, we\n",
      "investigate how effective robustness changes on the contin-\n",
      "uum from zero-shot to fully supervised. In Figure 15 we\n",
      "visualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\n",
      "..., 128-shot, and fully supervised logistic regression classi-\n",
      "fiers on the best CLIP model’s features. We see that while\n",
      "few-shot models also show higher effective robustness than\n",
      "existing models, this benefit fades as in-distribution per-\n",
      "formance increases with more training data and is mostly,\n",
      "though not entirely, gone for the fully supervised model.\n",
      "Additionally, zero-shot CLIP is notably more robust than\n",
      "a few-shot model with equivalent ImageNet performance.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 16\n",
      "\n",
      "70 75 80 85 90 95\n",
      "Average on class subsampled ImageNet (top-1, %)\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "on\n",
      " 7\n",
      "\n",
      " n\n",
      "at\n",
      "\n",
      "ur\n",
      "al\n",
      "\n",
      " d\n",
      "ist\n",
      "\n",
      "rib\n",
      "ut\n",
      "\n",
      "io\n",
      "n \n",
      "\n",
      "sh\n",
      "ift\n",
      "\n",
      " d\n",
      "at\n",
      "\n",
      "as\n",
      "et\n",
      "\n",
      "s (\n",
      "to\n",
      "\n",
      "p-\n",
      "1,\n",
      "\n",
      " %\n",
      ")\n",
      "\n",
      "Adapt to class shift\n",
      "\n",
      "Adapt to ImageNet\n",
      "\n",
      "Ideal robust model (y = x)\n",
      "Adaptive Zero-Shot CLIP\n",
      "ImageNet Zero-Shot CLIP\n",
      "Logistic Regression CLIP\n",
      "Standard ImageNet training\n",
      "Robustness intervention\n",
      "Trained with more data\n",
      "\n",
      "10 5 0 5 10 15 20 25 30\n",
      "Change from zero-shot ImageNet classifier accuracy (%)\n",
      "\n",
      "ImageNet-R-4.7\n",
      "ObjectNet-3.8\n",
      "ImageNet Sketch-2.8\n",
      "ImageNet-A-1.9\n",
      "ImageNet Vid-0.5\n",
      "+0.6Youtube-BB\n",
      "\n",
      "+5.8ImageNetV2\n",
      "+9.2ImageNet\n",
      "\n",
      "Adapt to ImageNet\n",
      "\n",
      "10 5 0 5 10 15 20 25 30\n",
      "Change from zero-shot ImageNet classifier accuracy (%)\n",
      "\n",
      "0ImageNet\n",
      "0ImageNetV2\n",
      "0ImageNet-A\n",
      "0ImageNet-R\n",
      "0ImageNet Sketch\n",
      "\n",
      "+2.3ObjectNet\n",
      "+8.3ImageNet Vid\n",
      "\n",
      "+26.9Youtube-BB\n",
      "Adapt to class shift\n",
      "\n",
      "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\n",
      "(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier\n",
      "and pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\n",
      "as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\n",
      "ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot\n",
      "classifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\n",
      "with ImageNet categories.\n",
      "\n",
      "Across our experiments, high effective robustness seems to\n",
      "result from minimizing the amount of distribution specific\n",
      "training data a model has access to, but this comes at a cost\n",
      "of reducing dataset-specific performance.\n",
      "\n",
      "Taken together, these results suggest that the recent shift\n",
      "towards large-scale task and dataset agnostic pre-training\n",
      "combined with a reorientation towards zero-shot and few-\n",
      "shot benchmarking on broad evaluation suites (as advocated\n",
      "by Yogatama et al. (2019) and Linzen (2020)) promotes the\n",
      "development of more robust systems and provides a more\n",
      "accurate assessment of performance. We are curious to see\n",
      "if the same results hold for zero-shot models in the field\n",
      "of NLP such as the GPT family. While Hendrycks et al.\n",
      "(2020b) has reported that pre-training improves relative ro-\n",
      "bustness on sentiment analysis, Miller et al. (2020)’s study\n",
      "of the robustness of question answering models under nat-\n",
      "ural distribution shift finds, similar to Taori et al. (2020),\n",
      "little evidence of effective robustness improvements to date.\n",
      "\n",
      "4. Comparison to Human Performance\n",
      "How does CLIP compare to human performance and human\n",
      "learning? To get a better understanding of how well humans\n",
      "perform in similar evaluation settings to CLIP, we evaluated\n",
      "\n",
      "humans on one of our tasks. We wanted to get a sense of\n",
      "how strong human zero-shot performance is at these tasks,\n",
      "and how much human performance is improved if they are\n",
      "shown one or two image samples. This can help us to\n",
      "compare task difficulty for humans and CLIP, and identify\n",
      "correlations and differences between them.\n",
      "\n",
      "We had five different humans look at each of 3669 images\n",
      "in the test split of the Oxford IIT Pets dataset (Parkhi et al.,\n",
      "2012) and select which of the 37 cat or dog breeds best\n",
      "matched the image (or ‘I don’t know’ if they were com-\n",
      "pletely uncertain). In the zero-shot case the humans were\n",
      "given no examples of the breeds and asked to label them\n",
      "to the best of their ability without an internet search. In\n",
      "the one-shot experiment the humans were given one sample\n",
      "image of each breed and in the two-shot experiment they\n",
      "were given two sample images of each breed.5\n",
      "\n",
      "One possible concern was that the human workers were not\n",
      "sufficiently motivated in the zero-shot task. High human\n",
      "accuracy of 94% on the STL-10 dataset (Coates et al., 2011)\n",
      "\n",
      "5There is not a perfect correspondence between the human\n",
      "few-shot tasks and the model’s few-shot performance since the\n",
      "model cannot refer to sample images in the way that the humans\n",
      "can.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 17\n",
      "\n",
      "65 70 75 80 85 90 95\n",
      "Average on class subsampled ImageNet (top-1, %)\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "Av\n",
      "er\n",
      "\n",
      "ag\n",
      "e \n",
      "\n",
      "on\n",
      " 7\n",
      "\n",
      " n\n",
      "at\n",
      "\n",
      "ur\n",
      "al\n",
      "\n",
      " d\n",
      "ist\n",
      "\n",
      "rib\n",
      "ut\n",
      "\n",
      "io\n",
      "n \n",
      "\n",
      "sh\n",
      "ift\n",
      "\n",
      " d\n",
      "at\n",
      "\n",
      "as\n",
      "et\n",
      "\n",
      "s (\n",
      "to\n",
      "\n",
      "p-\n",
      "1,\n",
      "\n",
      " %\n",
      ")\n",
      "\n",
      "1 shot\n",
      "\n",
      "2 shot\n",
      "\n",
      "4 shot\n",
      "\n",
      "8 shot\n",
      "\n",
      "16 shot\n",
      "\n",
      "32\n",
      "64\n",
      "\n",
      "128\n",
      "all0 shot\n",
      "\n",
      "Ideal robust model (y = x)\n",
      "Few-Shot CLIP (best model)\n",
      "Zero-Shot CLIP (best model)\n",
      "Standard ImageNet training\n",
      "Robustness intervention\n",
      "Trained with more data\n",
      "\n",
      "Figure 15. Few-shot CLIP also increases effective robustness\n",
      "compared to existing ImageNet models but is less robust than\n",
      "zero-shot CLIP. Minimizing the amount of ImageNet training\n",
      "data used for adaption increases effective robustness at the cost of\n",
      "decreasing relative robustness. 16-shot logistic regression CLIP\n",
      "matches zero-shot CLIP on ImageNet, as previously reported in\n",
      "Figure 7, but is less robust.\n",
      "\n",
      "and 97-100% accuracy on the subset of attention check\n",
      "images increased our trust in the human workers.\n",
      "\n",
      "Interestingly, humans went from a performance average of\n",
      "54% to 76% with just one training example per class, and\n",
      "the marginal gain from an additional training example is\n",
      "minimal. The gain in accuracy going from zero to one shot\n",
      "is almost entirely on images that humans were uncertain\n",
      "about. This suggests that humans “know what they don’t\n",
      "know” and are able to update their priors on the images they\n",
      "are most uncertain in based on a single example. Given this,\n",
      "it seems that while CLIP is a promising training strategy\n",
      "for zero-shot performance (Figure 5) and does well on tests\n",
      "of natural distribution shift (Figure 13), there is a large\n",
      "difference between how humans learn from a few examples\n",
      "and the few-shot methods in this paper.\n",
      "\n",
      "This suggests that there are still algorithmic improvements\n",
      "waiting to be made to decrease the gap between machine\n",
      "and human sample efficiency, as noted by Lake et al. (2016)\n",
      "and others. Because these few-shot evaluations of CLIP\n",
      "don’t make effective use of prior knowledge and the humans\n",
      "do, we speculate that finding a method to properly integrate\n",
      "prior knowledge into few-shot learning is an important step\n",
      "in algorithmic improvements to CLIP. To our knowledge,\n",
      "using a linear classifier on top of the features of a high-\n",
      "\n",
      "Accuracy Majority Vote\n",
      "on Full Dataset\n",
      "\n",
      "Accuracy\n",
      "on Guesses\n",
      "\n",
      "Majority Vote\n",
      "Accuracy\n",
      "\n",
      "on Guesses\n",
      "\n",
      "Zero-shot human 53.7 57.0 69.7 63.9\n",
      "Zero-shot CLIP 93.5 93.5 93.5 93.5\n",
      "One-shot human 75.7 80.3 78.5 81.2\n",
      "Two-shot human 75.7 85.0 79.2 86.1\n",
      "\n",
      "Table 2. Comparison of human performance on Oxford IIT Pets.\n",
      "As in Parkhi et al. (2012), the metric is average per-class classifica-\n",
      "tion accuracy. Most of the gain in performance when going from\n",
      "the human zero shot case to the human one shot case is on images\n",
      "that participants were highly uncertain on. “Guesses” refers to\n",
      "restricting the dataset to where participants selected an answer\n",
      "other than “I don’t know”, the “majority vote” is taking the most\n",
      "frequent (exclusive of ties) answer per image.\n",
      "\n",
      "quality pre-trained model is near state-of-the-art for few\n",
      "shot learning (Tian et al., 2020), which suggests that there is\n",
      "a gap between the best few-shot machine learning methods\n",
      "and human few-shot learning.\n",
      "\n",
      "If we plot human accuracy vs CLIP’s zero shot accuracy\n",
      "(Figure 16), we see that the hardest problems for CLIP are\n",
      "also hard for humans. To the extent that errors are consistent,\n",
      "our hypothesis is that this is due to at least a two factors:\n",
      "noise in the dataset (including mislabeled images) and out of\n",
      "distribution images being hard for both humans and models.\n",
      "\n",
      "5. Data Overlap Analysis\n",
      "A concern with pre-training on a very large internet dataset\n",
      "is unintentional overlap with downstream evals. This is\n",
      "important to investigate since, in a worst-case scenario, a\n",
      "complete copy of an evaluation dataset could leak into the\n",
      "pre-training dataset and invalidate the evaluation as a mean-\n",
      "ingful test of generalization. One option to prevent this is to\n",
      "identify and remove all duplicates before training a model.\n",
      "While this guarantees reporting true hold-out performance,\n",
      "it requires knowing all possible data which a model might\n",
      "be evaluated on ahead of time. This has the downside of\n",
      "limiting the scope of benchmarking and analysis. Adding a\n",
      "new evaluation would require an expensive re-train or risk\n",
      "reporting an un-quantified benefit due to overlap.\n",
      "\n",
      "Instead, we document how much overlap occurs and how\n",
      "performance changes due to these overlaps. To do this, we\n",
      "use the following procedure:\n",
      "\n",
      "1) For each evaluation dataset, we run a duplicate detector\n",
      "(see Appendix C) on its examples. We then manually inspect\n",
      "the found nearest neighbors and set a per dataset threshold\n",
      "to keep high precision while maximizing recall. Using\n",
      "this threshold, we then create two new subsets, Overlap,\n",
      "which contains all examples which have a similarity to a\n",
      "training example above the threshold, and Clean, which\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 18\n",
      "\n",
      "pu\n",
      "g\n",
      "\n",
      "sp\n",
      "hy\n",
      "\n",
      "nx\n",
      "ge\n",
      "\n",
      "rm\n",
      "an\n",
      "\n",
      "_s\n",
      "ho\n",
      "\n",
      "rth\n",
      "ai\n",
      "\n",
      "re\n",
      "d\n",
      "\n",
      "sh\n",
      "ib\n",
      "\n",
      "a_\n",
      "in\n",
      "\n",
      "u\n",
      "be\n",
      "\n",
      "ag\n",
      "le\n",
      "\n",
      "gr\n",
      "ea\n",
      "\n",
      "t_\n",
      "py\n",
      "\n",
      "re\n",
      "ne\n",
      "\n",
      "es\n",
      "en\n",
      "\n",
      "gl\n",
      "ish\n",
      "\n",
      "_s\n",
      "et\n",
      "\n",
      "te\n",
      "r\n",
      "\n",
      "sa\n",
      "m\n",
      "\n",
      "oy\n",
      "ed\n",
      "\n",
      "sa\n",
      "in\n",
      "\n",
      "t_\n",
      "be\n",
      "\n",
      "rn\n",
      "ar\n",
      "\n",
      "d\n",
      "po\n",
      "\n",
      "m\n",
      "er\n",
      "\n",
      "an\n",
      "ia\n",
      "\n",
      "n\n",
      "ne\n",
      "\n",
      "wf\n",
      "ou\n",
      "\n",
      "nd\n",
      "la\n",
      "\n",
      "nd\n",
      "wh\n",
      "\n",
      "ea\n",
      "te\n",
      "\n",
      "n_\n",
      "te\n",
      "\n",
      "rri\n",
      "er\n",
      "\n",
      "sc\n",
      "ot\n",
      "\n",
      "tis\n",
      "h_\n",
      "\n",
      "te\n",
      "rri\n",
      "\n",
      "er\n",
      "yo\n",
      "\n",
      "rk\n",
      "sh\n",
      "\n",
      "ire\n",
      "_t\n",
      "\n",
      "er\n",
      "rie\n",
      "\n",
      "r\n",
      "sia\n",
      "\n",
      "m\n",
      "es\n",
      "\n",
      "e\n",
      "m\n",
      "\n",
      "in\n",
      "ia\n",
      "\n",
      "tu\n",
      "re\n",
      "\n",
      "_p\n",
      "in\n",
      "\n",
      "sc\n",
      "he\n",
      "\n",
      "r\n",
      "ha\n",
      "\n",
      "va\n",
      "ne\n",
      "\n",
      "se\n",
      "ke\n",
      "\n",
      "es\n",
      "ho\n",
      "\n",
      "nd\n",
      "bo\n",
      "\n",
      "m\n",
      "ba\n",
      "\n",
      "y\n",
      "m\n",
      "\n",
      "ai\n",
      "ne\n",
      "\n",
      "_c\n",
      "oo\n",
      "\n",
      "n\n",
      "ch\n",
      "\n",
      "ih\n",
      "ua\n",
      "\n",
      "hu\n",
      "a\n",
      "\n",
      "ba\n",
      "ss\n",
      "\n",
      "et\n",
      "_h\n",
      "\n",
      "ou\n",
      "nd\n",
      "\n",
      "ja\n",
      "pa\n",
      "\n",
      "ne\n",
      "se\n",
      "\n",
      "_c\n",
      "hi\n",
      "\n",
      "n\n",
      "ru\n",
      "\n",
      "ss\n",
      "ia\n",
      "\n",
      "n_\n",
      "bl\n",
      "\n",
      "ue\n",
      "am\n",
      "\n",
      "er\n",
      "ica\n",
      "\n",
      "n_\n",
      "bu\n",
      "\n",
      "lld\n",
      "og\n",
      "\n",
      "pe\n",
      "rs\n",
      "\n",
      "ia\n",
      "n\n",
      "\n",
      "be\n",
      "ng\n",
      "\n",
      "al\n",
      "le\n",
      "\n",
      "on\n",
      "be\n",
      "\n",
      "rg\n",
      "er\n",
      "\n",
      "ab\n",
      "ys\n",
      "\n",
      "sin\n",
      "ia\n",
      "\n",
      "n\n",
      "bo\n",
      "\n",
      "xe\n",
      "r\n",
      "\n",
      "br\n",
      "iti\n",
      "\n",
      "sh\n",
      "_s\n",
      "\n",
      "ho\n",
      "rth\n",
      "\n",
      "ai\n",
      "r\n",
      "\n",
      "st\n",
      "af\n",
      "\n",
      "fo\n",
      "rd\n",
      "\n",
      "sh\n",
      "ire\n",
      "\n",
      "_b\n",
      "ul\n",
      "\n",
      "l_t\n",
      "er\n",
      "\n",
      "rie\n",
      "r\n",
      "\n",
      "am\n",
      "er\n",
      "\n",
      "ica\n",
      "n_\n",
      "\n",
      "pi\n",
      "t_\n",
      "\n",
      "bu\n",
      "ll_\n",
      "\n",
      "te\n",
      "rri\n",
      "\n",
      "er\n",
      "eg\n",
      "\n",
      "yp\n",
      "tia\n",
      "\n",
      "n_\n",
      "m\n",
      "\n",
      "au\n",
      "bi\n",
      "\n",
      "rm\n",
      "an\n",
      "\n",
      "en\n",
      "gl\n",
      "\n",
      "ish\n",
      "_c\n",
      "\n",
      "oc\n",
      "ke\n",
      "\n",
      "r_\n",
      "sp\n",
      "\n",
      "an\n",
      "ie\n",
      "\n",
      "l\n",
      "ra\n",
      "\n",
      "gd\n",
      "ol\n",
      "\n",
      "l\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "Ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      " (%\n",
      ")\n",
      "\n",
      "Zero-Shot CLIP\n",
      "One-Shot Human\n",
      "Zero-Shot Human\n",
      "\n",
      "Figure 16. The hardest problems for CLIP also tend to be the hard-\n",
      "est problems for humans. Here we rank image categories by diffi-\n",
      "culty for CLIP as measured as probability of the correct label.\n",
      "\n",
      "contains all examples that are below this threshold. We\n",
      "denote the unaltered full dataset All for reference. From\n",
      "this we first record the degree of data contamination as the\n",
      "ratio of the number of examples in Overlap to the size of\n",
      "All.\n",
      "\n",
      "2) We then compute the zero-shot accuracy of CLIP\n",
      "RN50x64 on the three splits and report All - Clean\n",
      "as our main metric. This is the difference in accuracy due\n",
      "to contamination. When positive it is our estimate of how\n",
      "much the overall reported accuracy on the dataset was in-\n",
      "flated by over-fitting to overlapping data.\n",
      "\n",
      "3) The amount of overlap is often small so we also run a\n",
      "binomial significance test where we use the accuracy on\n",
      "Clean as the null hypothesis and compute the one-tailed\n",
      "(greater) p-value for the Overlap subset. We also calculate\n",
      "99.5% Clopper-Pearson confidence intervals on Dirty as\n",
      "another check.\n",
      "\n",
      "A summary of this analysis is presented in Figure 17. Out\n",
      "of 35 datasets studied, 9 datasets have no detected overlap\n",
      "at all. Most of these datasets are synthetic or specialized\n",
      "making them unlikely to be posted as normal images on\n",
      "the internet (for instance MNIST, CLEVR, and GTSRB) or\n",
      "are guaranteed to have no overlap due to containing novel\n",
      "data from after the date our dataset was created (ObjectNet\n",
      "and Hateful Memes). This demonstrates our detector has\n",
      "a low-false positive rate which is important as false posi-\n",
      "tives would under-estimate the effect of contamination in\n",
      "\n",
      "our analysis. There is a median overlap of 2.2% and an av-\n",
      "erage overlap of 3.2%. Due to this small amount of overlap,\n",
      "overall accuracy is rarely shifted by more than 0.1% with\n",
      "only 7 datasets above this threshold. Of these, only 2 are\n",
      "statistically significant after Bonferroni correction. The max\n",
      "detected improvement is only 0.6% on Birdsnap which has\n",
      "the second largest overlap at 12.1%. The largest overlap is\n",
      "for Country211 at 21.5%. This is due to it being constructed\n",
      "out of YFCC100M, which our pre-training dataset contains\n",
      "a filtered subset of. Despite this large overlap there is only\n",
      "a 0.2% increase in accuracy on Country211. This may be\n",
      "because the training text accompanying an example is often\n",
      "not related to the specific task a downstream eval measures.\n",
      "Country211 measures geo-localization ability, but inspect-\n",
      "ing the training text for these duplicates showed they often\n",
      "do not mention the location of the image.\n",
      "\n",
      "We are aware of two potential concerns with our analysis.\n",
      "First our detector is not perfect. While it achieves near\n",
      "100% accuracy on its proxy training task and manual in-\n",
      "spection + threshold tuning results in very high precision\n",
      "with good recall among the found nearest-neighbors, we can\n",
      "not tractably check its recall across 400 million examples.\n",
      "Another potential confounder of our analysis is that the un-\n",
      "derlying data distribution may shift between the Overlap\n",
      "and Clean subsets. For example, on Kinetics-700 many\n",
      "“overlaps” are in fact all black transition frames. This ex-\n",
      "plains why Kinetics-700 has an apparent 20% accuracy drop\n",
      "on Overlap. We suspect more subtle distribution shifts\n",
      "likely exist. One possibility we noticed on CIFAR-100 is\n",
      "that, due to the very low resolution of its images, many\n",
      "duplicates were false positives of small objects such as birds\n",
      "or planes. Changes in accuracy could instead be due to\n",
      "changes in the class distribution or difficulty of the dupli-\n",
      "cates. Unfortunately, these distribution and difficulty shifts\n",
      "could also mask the effects of over-fitting.\n",
      "\n",
      "However, these results closely follow the findings of simi-\n",
      "lar duplicate analysis in previous work on large scale pre-\n",
      "training. Mahajan et al. (2018) and Kolesnikov et al. (2019)\n",
      "detected similar overlap rates and found minimal changes in\n",
      "overall performance. Importantly, Kolesnikov et al. (2019)\n",
      "also compared the alternative de-duplication strategy dis-\n",
      "cussed in the introduction to this section with the approach\n",
      "we settled on and observed little difference between the two\n",
      "approaches.\n",
      "\n",
      "6. Limitations\n",
      "There are still many limitations to CLIP. While several of\n",
      "these are discussed as part of analysis in various sections,\n",
      "we summarize and collect them here.\n",
      "\n",
      "On datasets with training splits, the performance of zero-\n",
      "shot CLIP is on average competitive with the simple su-\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 19\n",
      "\n",
      "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\n",
      "Detected Data Overlap (%)\n",
      "\n",
      "-20\n",
      "\n",
      "-10\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "Di\n",
      "ffe\n",
      "\n",
      "re\n",
      "nc\n",
      "\n",
      "e \n",
      "in\n",
      "\n",
      " A\n",
      "cc\n",
      "\n",
      "ur\n",
      "ac\n",
      "\n",
      "y \n",
      "on\n",
      "\n",
      " O\n",
      "ve\n",
      "\n",
      "rla\n",
      "pp\n",
      "\n",
      "in\n",
      "g \n",
      "\n",
      "vs\n",
      ". C\n",
      "\n",
      "le\n",
      "an\n",
      "\n",
      " D\n",
      "at\n",
      "\n",
      "a \n",
      "(%\n",
      "\n",
      ")\n",
      "\n",
      "SUN397\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "ImageNet Sketch\n",
      "\n",
      "SUN\n",
      "\n",
      "Kinetics-700\n",
      "\n",
      "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\n",
      "Detected Data Overlap (%)\n",
      "\n",
      "-0.75\n",
      "\n",
      "-0.5\n",
      "\n",
      "-0.25\n",
      "\n",
      "0\n",
      "\n",
      "0.25\n",
      "\n",
      "0.5\n",
      "\n",
      "0.75\n",
      "\n",
      "Ov\n",
      "er\n",
      "\n",
      "al\n",
      "l A\n",
      "\n",
      "cc\n",
      "ur\n",
      "\n",
      "ac\n",
      "y \n",
      "\n",
      "Ch\n",
      "an\n",
      "\n",
      "ge\n",
      " D\n",
      "\n",
      "ue\n",
      " T\n",
      "\n",
      "o \n",
      "Ov\n",
      "\n",
      "er\n",
      "la\n",
      "\n",
      "p \n",
      "(%\n",
      "\n",
      ")\n",
      "\n",
      "Stanford CarsSUN397\n",
      "\n",
      "Birdsnap\n",
      "CIFAR-100\n",
      "\n",
      "FER2013\n",
      "\n",
      "Country211\n",
      "SUN\n",
      "\n",
      "p < 1e-3\n",
      "p < 0.05\n",
      "p > 0.05\n",
      "\n",
      "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have\n",
      "up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\n",
      "99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\n",
      "(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\n",
      "overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\n",
      "improvements statistically significant when calculated using a one-sided binomial test.\n",
      "\n",
      "pervised baseline of a linear classifier on top of ResNet-50\n",
      "features. On most of these datasets, the performance of\n",
      "this baseline is now well below the overall state of the art.\n",
      "Significant work is still needed to improve the task learning\n",
      "and transfer capabilities of CLIP. While scaling has so far\n",
      "steadily improved performance and suggests a route for con-\n",
      "tinued improvement, we estimate around a 1000x increase\n",
      "in compute is required for zero-shot CLIP to reach overall\n",
      "state-of-the-art performance. This is infeasible to train with\n",
      "current hardware. Further research into improving upon the\n",
      "computational and data efficiency of CLIP will be necessary.\n",
      "\n",
      "Analysis in Section 3.1 found that CLIP’s zero-shot perfor-\n",
      "mance is still quite weak on several kinds of tasks. When\n",
      "compared to task-specific models, the performance of CLIP\n",
      "is poor on several types of fine-grained classification such\n",
      "as differentiating models of cars, species of flowers, and\n",
      "variants of aircraft. CLIP also struggles with more abstract\n",
      "and systematic tasks such as counting the number of objects\n",
      "in an image. Finally for novel tasks which are unlikely to be\n",
      "included in CLIP’s pre-training dataset, such as classifying\n",
      "the distance to the nearest car in a photo, CLIP’s perfor-\n",
      "mance can be near random. We are confident that there are\n",
      "still many, many, tasks where CLIP’s zero-shot performance\n",
      "is near chance level.\n",
      "\n",
      "While zero-shot CLIP generalizes well to many natural im-\n",
      "age distributions as investigated in Section 3.3, we’ve ob-\n",
      "served that zero-shot CLIP still generalizes poorly to data\n",
      "that is truly out-of-distribution for it. An illustrative exam-\n",
      "ple occurs for the task of OCR as reported in Appendix E.\n",
      "\n",
      "CLIP learns a high quality semantic OCR representation that\n",
      "performs well on digitally rendered text, which is common\n",
      "in its pre-training dataset, as evidenced by performance on\n",
      "Rendered SST2. However, CLIP only achieves 88% accu-\n",
      "racy on the handwritten digits of MNIST. An embarrassingly\n",
      "simple baseline of logistic regression on raw pixels outper-\n",
      "forms zero-shot CLIP. Both semantic and near-duplicate\n",
      "nearest-neighbor retrieval verify that there are almost no im-\n",
      "ages that resemble MNIST digits in our pre-training dataset.\n",
      "This suggests CLIP does little to address the underlying\n",
      "problem of brittle generalization of deep learning models.\n",
      "Instead CLIP tries to circumvent the problem and hopes that\n",
      "by training on such a large and varied dataset that all data\n",
      "will be effectively in-distribution. This is a naive assumption\n",
      "that, as MNIST demonstrates, is easy to violate.\n",
      "\n",
      "Although CLIP can flexibly generate zero-shot classifiers\n",
      "for a wide variety of tasks and datasets, CLIP is still limited\n",
      "to choosing from only those concepts in a given zero-shot\n",
      "classifier. This is a significant restriction compared to a\n",
      "truly flexible approach like image captioning which could\n",
      "generate novel outputs. Unfortunately, as described in Sec-\n",
      "tion 2.3 we found the computational efficiency of the image\n",
      "caption baseline we tried to be much lower than CLIP. A\n",
      "simple idea worth trying is joint training of a contrastive\n",
      "and generative objective with the hope of combining the\n",
      "efficiency of CLIP with the flexibility of a caption model.\n",
      "As another alternative, search could be performed at infer-\n",
      "ence time over many natural language explanations of a\n",
      "given image, similar to approach proposed in Learning with\n",
      "Latent Language Andreas et al. (2017).\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 20\n",
      "\n",
      "CLIP also does not address the poor data efficiency of deep\n",
      "learning. Instead CLIP compensates by using a source of\n",
      "supervision that can be scaled to hundreds of millions of\n",
      "training examples. If every image seen during training of\n",
      "a CLIP model was presented at a rate of one per second,\n",
      "it would take 405 years to iterate through the 12.8 billion\n",
      "images seen over 32 training epochs. Combining CLIP\n",
      "with self-supervision (Henaff, 2020; Chen et al., 2020c) and\n",
      "self-training (Lee; Xie et al., 2020) methods is a promising\n",
      "direction given their demonstrated ability to improve data\n",
      "efficiency over standard supervised learning.\n",
      "\n",
      "Our methodology has several significant limitations. De-\n",
      "spite our focus on zero-shot transfer, we repeatedly queried\n",
      "performance on full validation sets to guide the develop-\n",
      "ment of CLIP. These validation sets often have thousands\n",
      "of examples, which is unrealistic for true zero-shot sce-\n",
      "narios. Similar concerns have been raised in the field of\n",
      "semi-supervised learning (Oliver et al., 2018). Another po-\n",
      "tential issue is our selection of evaluation datasets. While\n",
      "we have reported results on Kornblith et al. (2019)’s 12\n",
      "dataset evaluation suite as a standardized collection, our\n",
      "main results use a somewhat haphazardly assembled col-\n",
      "lection of 27 datasets that is undeniably co-adapted with\n",
      "the development and capabilities of CLIP. Creating a new\n",
      "benchmark of tasks designed explicitly to evaluate broad\n",
      "zero-shot transfer capabilities, rather than re-using existing\n",
      "supervised datasets, would help address these issues.\n",
      "\n",
      "CLIP is trained on text paired with images on the internet.\n",
      "These image-text pairs are unfiltered and uncurated and\n",
      "result in CLIP models learning many social biases. This\n",
      "has been previously demonstrated for image caption models\n",
      "(Bhargava & Forsyth, 2019). We refer readers to Section 7\n",
      "for detailed analysis and quantification of these behaviors for\n",
      "CLIP as well as discussion of potential mitigation strategies.\n",
      "\n",
      "While we have emphasized throughout this work that speci-\n",
      "fying image classifiers through natural language is a flexible\n",
      "and general interface, it has its own limitations. Many com-\n",
      "plex tasks and visual concepts can be difficult to specify\n",
      "just through text. Actual training examples are undeniably\n",
      "useful but CLIP does not optimize for few-shot performance\n",
      "directly. In our work, we fall back to fitting linear classifiers\n",
      "on top of CLIP’s features. This results in a counter-intuitive\n",
      "drop in performance when transitioning from a zero-shot\n",
      "to a few-shot setting. As discussed in Section 4, this is\n",
      "notably different from human performance which shows a\n",
      "large increase from a zero to a one shot setting. Future work\n",
      "is needed to develop methods that combine CLIP’s strong\n",
      "zero-shot performance with efficient few-shot learning.\n",
      "\n",
      "7. Broader Impacts\n",
      "CLIP has a wide range of capabilities due to its ability to\n",
      "carry out arbitrary image classification tasks. One can give\n",
      "it images of cats and dogs and ask it to classify cats, or give\n",
      "it images taken in a department store and ask it to classify\n",
      "shoplifters–a task with significant social implications and\n",
      "for which AI may be unfit. Like any image classification\n",
      "system, CLIP’s performance and fitness for purpose need to\n",
      "be evaluated, and its broader impacts analyzed in context.\n",
      "CLIP also introduces a capability that will magnify and alter\n",
      "such issues: CLIP makes it possible to easily create your\n",
      "own classes for categorization (to ‘roll your own classifier’)\n",
      "without a need for re-training. This capability introduces\n",
      "challenges similar to those found in characterizing other,\n",
      "large-scale generative models like GPT-3 (Brown et al.,\n",
      "2020); models that exhibit non-trivial zero-shot (or few-\n",
      "shot) generalization can have a vast range of capabilities,\n",
      "many of which are made clear only after testing for them.\n",
      "\n",
      "Our studies of CLIP in a zero-shot setting show that the\n",
      "model displays significant promise for widely-applicable\n",
      "tasks like image retrieval or search. For example, it can find\n",
      "relevant images in a database given text, or relevant text\n",
      "given an image. Further, the relative ease of steering CLIP\n",
      "toward bespoke applications with little or no additional data\n",
      "or training could unlock a variety of novel applications that\n",
      "are hard for us to envision today, as has occurred with large\n",
      "language models over the past few years.\n",
      "\n",
      "In addition to the more than 30 datasets studied in earlier\n",
      "sections of this paper, we evaluate CLIP’s performance on\n",
      "the FairFace benchmark and undertake exploratory bias\n",
      "probes. We then characterize the model’s performance in\n",
      "a downstream task, surveillance, and discuss its usefulness\n",
      "as compared with other available systems. Many of CLIP’s\n",
      "capabilities are omni-use in nature (e.g. OCR can be used\n",
      "to make scanned documents searchable, to power screen\n",
      "reading technologies, or to read license plates). Several\n",
      "of the capabilities measured, from action recognition, ob-\n",
      "ject classification, and geo-localization, to facial emotion\n",
      "recognition, can be used in surveillance. Given its social\n",
      "implications, we address this domain of use specifically in\n",
      "the Surveillance section.\n",
      "\n",
      "We have also sought to characterize the social biases inher-\n",
      "ent to the model. Our bias tests represent our initial efforts\n",
      "to probe aspects of how the model responds in different sce-\n",
      "narios, and are by nature limited in scope. CLIP and models\n",
      "like it will need to be analyzed in relation to their specific\n",
      "deployments to understand how bias manifests and iden-\n",
      "tify potential interventions. Further community exploration\n",
      "will be required to develop broader, more contextual, and\n",
      "more robust testing schemes so that AI developers can bet-\n",
      "ter characterize biases in general purpose computer vision\n",
      "models.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 21\n",
      "\n",
      "Model Race Gender Age\n",
      "\n",
      "FairFace Model 93.7 94.2 59.7\n",
      "Linear Probe CLIP 93.4 96.5 63.8\n",
      "Zero-Shot CLIP 58.3 95.9 57.1\n",
      "Linear Probe Instagram 90.8 93.2 54.2\n",
      "\n",
      "Table 3. Percent accuracy on Race, Gender, and Age classification\n",
      "of images in FairFace category ‘White’\n",
      "\n",
      "Model Race Gender Age\n",
      "\n",
      "FairFace Model 75.4 94.4 60.7\n",
      "Linear Probe CLIP 92.8 97.7 63.1\n",
      "Zero-Shot CLIP 91.3 97.2 54.3\n",
      "Linear Probe Instagram 87.2 93.9 54.1\n",
      "\n",
      "Table 4. Percent accuracy on Race, Gender, and Age classification\n",
      "of images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\n",
      "‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\n",
      "gether as FairFace category ‘Non-White’)\n",
      "\n",
      "Middle Southeast East\n",
      "Model Gender Black White Indian Latino Eastern Asian Asian Average\n",
      "\n",
      "Male 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\n",
      "Linear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\n",
      "\n",
      "97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5\n",
      "\n",
      "Male 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\n",
      "Zero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\n",
      "\n",
      "96.7 95.9 98.0 97.5 98.0 96.3 96.6\n",
      "\n",
      "Male 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\n",
      "Linear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\n",
      "\n",
      "91.3 93.2 95.6 94.0 95.6 93.4 93.9\n",
      "\n",
      "Table 5. Percent accuracy on gender classification of images by FairFace race category\n",
      "\n",
      "7.1. Bias\n",
      "\n",
      "Algorithmic decisions, training data, and choices about how\n",
      "classes are defined and taxonomized (which we refer to in-\n",
      "formally as “class design”) can all contribute to and amplify\n",
      "social biases and inequalities resulting from the use of AI\n",
      "systems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\n",
      "& Star, 2000). Class design is particularly relevant to mod-\n",
      "els like CLIP, since any developer can define a class and the\n",
      "model will provide some result.\n",
      "\n",
      "In this section, we provide preliminary analysis of some\n",
      "of the biases in CLIP, using bias probes inspired by those\n",
      "outlined in Buolamwini & Gebru (2018) and Kärkkäinen\n",
      "& Joo (2019). We also conduct exploratory bias research\n",
      "intended to find specific examples of biases in the model,\n",
      "similar to that conducted by Solaiman et al. (2019).\n",
      "\n",
      "We start by analyzing the performance of Zero-Shot CLIP on\n",
      "the face image dataset FairFace (Kärkkäinen & Joo, 2019)6\n",
      "\n",
      "6FairFace is a face image dataset designed to balance age, gen-\n",
      "der, and race, in order to reduce asymmetries common in previous\n",
      "face datasets. It categorizes gender into 2 groups: female and male\n",
      "and race into 7 groups: White, Black, Indian, East Asian, Southeast\n",
      "Asian, Middle Eastern, and Latino. There are inherent problems\n",
      "with race and gender classifications, as e.g. Bowker & Star (2000)\n",
      "\n",
      "as an initial bias probe, then probe the model further to\n",
      "surface additional biases and sources of biases, including\n",
      "class design.\n",
      "\n",
      "We evaluated two versions of CLIP on the FairFace dataset:\n",
      "a zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\n",
      "sion classifier fitted to FairFace’s dataset on top of CLIP’s\n",
      "features (“LR CLIP”). We find that LR CLIP gets higher\n",
      "accuracy on the FairFace dataset than both the ResNext-101\n",
      "32x48d Instagram model (“Linear Probe Instagram”) (Ma-\n",
      "hajan et al., 2018) and FairFace’s own model on most of the\n",
      "classification tests we ran7. ZS CLIP’s performance varies\n",
      "by category and is worse than that of FairFace’s model for a\n",
      "few categories, and better for others. (See Table 3 and Table\n",
      "4).\n",
      "\n",
      "and Keyes (2018) have shown. While FairFace’s dataset reduces\n",
      "the proportion of White faces, it still lacks representation of entire\n",
      "large demographic groups, effectively erasing such categories. We\n",
      "use the 2 gender categories and 7 race categories defined in the\n",
      "FairFace dataset in a number of our experiments not in order to\n",
      "reinforce or endorse the use of such reductive categories, but in\n",
      "order to enable us to make comparisons to prior work.\n",
      "\n",
      "7One challenge with this comparison is that the FairFace model\n",
      "uses binary classes for race (“White” and “Non-White”), instead\n",
      "of breaking down races into finer-grained sub-groups.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 22\n",
      "\n",
      "Middle Southeast East\n",
      "Category Black White Indian Latino Eastern Asian Asian\n",
      "\n",
      "Crime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\n",
      "Non-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0\n",
      "\n",
      "Table 6. Percent of images classified into crime-related and non-human categories by FairFace Race category. The label set included 7\n",
      "FairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\n",
      "\n",
      "Category Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\n",
      "\n",
      "Default Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\n",
      "Default Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4\n",
      "\n",
      "Table 7. Percent of images classified into crime-related and non-human categories by FairFace Age category, showing comparison between\n",
      "results obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\n",
      "race categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\n",
      "\n",
      "Additionally, we test the performance of the LR CLIP and\n",
      "ZS CLIP models across intersectional race and gender cate-\n",
      "gories as they are defined in the FairFace dataset. We find\n",
      "that model performance on gender classification is above\n",
      "95% for all race categories. Table 5 summarizes these re-\n",
      "sults.\n",
      "\n",
      "While LR CLIP achieves higher accuracy than the Linear\n",
      "Probe Instagram model on the FairFace benchmark dataset\n",
      "for gender, race and age classification of images by intersec-\n",
      "tional categories, accuracy on benchmarks offers only one\n",
      "approximation of algorithmic fairness, as Raji et al. (2020)\n",
      "have shown, and often fails as a meaningful measure of fair-\n",
      "ness in real world contexts. Even if a model has both higher\n",
      "accuracy and lower disparities in performance on different\n",
      "sub-groups, this does not mean it will have lower disparities\n",
      "in impact (Scheuerman et al., 2019). For example, higher\n",
      "performance on underrepresented groups might be used by\n",
      "a company to justify their use of facial recognition, and to\n",
      "then deploy it ways that affect demographic groups dispro-\n",
      "portionately. Our use of facial classification benchmarks to\n",
      "probe for biases is not intended to imply that facial classi-\n",
      "fication is an unproblematic task, nor to endorse the use of\n",
      "race, age, or gender classification in deployed contexts.\n",
      "\n",
      "We also probed the model using classification terms with\n",
      "high potential to cause representational harm, focusing on\n",
      "denigration harms in particular (Crawford, 2017). We car-\n",
      "ried out an experiment in which the ZS CLIP model was\n",
      "required to classify 10,000 images from the FairFace dataset.\n",
      "In addition to the FairFace classes, we added in the follow-\n",
      "ing classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\n",
      "‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\n",
      "experiment was to check if harms of denigration dispropor-\n",
      "tionately impact certain demographic subgroups.\n",
      "\n",
      "We found that 4.9% (confidence intervals between 4.6%\n",
      "and 5.4%) of the images were misclassified into one of\n",
      "the non-human classes we used in our probes (‘animal’,\n",
      "‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\n",
      "images had the highest misclassification rate (approximately\n",
      "14%; confidence intervals between [12.6% and 16.4%])\n",
      "while all other races had misclassification rates under 8%.\n",
      "People aged 0-20 years had the highest proportion being\n",
      "classified into this category at 14% .\n",
      "\n",
      "We also found that 16.5% of male images were misclassified\n",
      "into classes related to crime (‘thief’, ‘suspicious person’ and\n",
      "‘criminal’) as compared to 9.8% of female images. Inter-\n",
      "estingly, we found that people aged 0-20 years old were\n",
      "more likely to fall under these crime-related classes (approx-\n",
      "imately 18%) compared to images of people in different\n",
      "age ranges (approximately 12% for people aged 20-60 and\n",
      "0% for people over 70). We found significant disparities in\n",
      "classifications across races for crime related terms, which is\n",
      "captured in Table 6.\n",
      "\n",
      "Given that we observed that people under 20 were the most\n",
      "likely to be classified in both the crime-related and non-\n",
      "human animal categories, we carried out classification for\n",
      "the images with the same classes but with an additional\n",
      "category ‘child’ added to the categories. Our goal here\n",
      "was to see if this category would significantly change the\n",
      "behaviour of the model and shift how the denigration harms\n",
      "are distributed by age. We found that this drastically reduced\n",
      "the number of images of people under 20 classified in either\n",
      "crime-related categories or non-human animal categories\n",
      "(Table 7). This points to how class design has the potential\n",
      "to be a key factor determining both the model performance\n",
      "and the unwanted biases or behaviour the model may exhibit\n",
      "while also asks overarching questions about the use of face\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 23\n",
      "\n",
      "images to automatically classify people along such lines\n",
      "(y Arcas et al., 2017).\n",
      "\n",
      "The results of these probes can change based on the class\n",
      "categories one chooses to include as well as the specific\n",
      "language one uses to describe each class. Poor class design\n",
      "can lead to poor real world performance; this concern is\n",
      "particularly relevant to a model like CLIP, given how easily\n",
      "developers can design their own classes.\n",
      "\n",
      "We also carried out experiments similar to those outlined by\n",
      "Schwemmer et al. (2020) to test how CLIP treated images\n",
      "of men and women differently using images of Members\n",
      "of Congress. As part of these experiments, we studied\n",
      "how certain additional design decisions such as deciding\n",
      "thresholds for labels can impact the labels output by CLIP\n",
      "and how biases manifest.\n",
      "\n",
      "We carried out three experiments - we tested for accuracy\n",
      "on gender classification and we tested for how labels were\n",
      "differentially distributed across two different label sets. For\n",
      "our first label set, we used a label set of 300 occupations and\n",
      "for our second label set we used a combined set of labels that\n",
      "Google Cloud Vision, Amazon Rekognition and Microsoft\n",
      "Azure Computer Vision returned for all the images.\n",
      "\n",
      "We first simply looked into gender prediction performance\n",
      "of the model on the images of Members of Congress, in\n",
      "order to check to see if the model correctly recognized\n",
      "men as men and women as women given the image of a\n",
      "person who appeared to be in an official setting/position of\n",
      "power. We found that the model got 100% accuracy on the\n",
      "images. This is slightly better performance than the model’s\n",
      "performance on the FairFace dataset. We hypothesize that\n",
      "one of the reasons for this is that all the images in the\n",
      "Members of Congress dataset were high-quality and clear,\n",
      "with the people clearly centered, unlike those in the FairFace\n",
      "dataset.\n",
      "\n",
      "In order to study how the biases in returned labels depend on\n",
      "the thresholds set for label probability, we did an experiment\n",
      "in which we set threshold values at 0.5% and 4.0%. We\n",
      "found that the lower threshold led to lower quality of labels.\n",
      "However, even the differing distributions of labels under\n",
      "this threshold can hold signals for bias. For example, we\n",
      "find that under the 0.5% threshold labels such as ‘nanny’\n",
      "and ‘housekeeper’ start appearing for women whereas labels\n",
      "such as ‘prisoner’ and ‘mobster’ start appearing for men.\n",
      "This points to gendered associations similar to those that\n",
      "have previously been found for occupations (Schwemmer\n",
      "et al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\n",
      "\n",
      "At the higher 4% threshold, the labels with the highest prob-\n",
      "ability across both genders include “lawmaker”, “legislator”\n",
      "and “congressman”. However, the presence of these biases\n",
      "amongst lower probability labels nonetheless point to larger\n",
      "questions about what ‘sufficiently’ safe behaviour may look\n",
      "\n",
      "like for deploying such systems.\n",
      "\n",
      "When given the combined set of labels that Google Cloud\n",
      "Vision (GCV), Amazon Rekognition and Microsoft returned\n",
      "for all the images, similar to the biases Schwemmer et al.\n",
      "(2020) found in GCV systems, we found our system also\n",
      "disproportionately attached labels to do with hair and ap-\n",
      "pearance in general to women more than men. For ex-\n",
      "ample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\n",
      "appeared significantly more often for women. Additionally,\n",
      "CLIP attached some labels that described high status occu-\n",
      "pations disproportionately more often to men such as ‘ex-\n",
      "ecutive’ and ‘doctor’. Out of the only four occupations that\n",
      "it attached more often to women, three were ‘newscaster’,\n",
      "‘television presenter’ and ‘newsreader’ and the fourth was\n",
      "‘Judge’. This is again similar to the biases found in GCV\n",
      "and points to historical gendered differences (Schwemmer\n",
      "et al., 2020).\n",
      "\n",
      "Interestingly, when we lowered the threshold to 0.5% for\n",
      "this set of labels, we found that the labels disproportionately\n",
      "describing men also shifted to appearance oriented words\n",
      "such as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\n",
      "tion oriented words such as ‘military person’ and ‘executive’\n",
      "- which were not used to describe images of women at the\n",
      "higher 4% threshold - were used for both men and women\n",
      "at the lower 0.5% threshold, which could have caused the\n",
      "change in labels for men. The reverse was not true. Descrip-\n",
      "tive words used to describe women were still uncommon\n",
      "amongst men.\n",
      "\n",
      "Design decisions at every stage of building a model impact\n",
      "how biases manifest and this is especially true for CLIP\n",
      "given the flexibility it offers. In addition to choices about\n",
      "training data and model architecture, decisions about things\n",
      "like class designs and thresholding values can alter the labels\n",
      "a model outputs and as a result heighten or lower certain\n",
      "kinds of harm, such as those described by Crawford (2017).\n",
      "People designing and developing models and AI systems\n",
      "have considerable power. Decisions about things like class\n",
      "design are a key determiner not only of model performance,\n",
      "but also of how and in what contexts model biases manifest.\n",
      "\n",
      "These experiments are not comprehensive. They illus-\n",
      "trate potential issues stemming from class design and other\n",
      "sources of bias, and are intended to spark inquiry.\n",
      "\n",
      "7.2. Surveillance\n",
      "\n",
      "We next sought to characterize model performance in re-\n",
      "lation to a downstream task for which there is significant\n",
      "societal sensitivity: surveillance. Our analysis aims to better\n",
      "embody the characterization approach described above and\n",
      "to help orient the research community towards the potential\n",
      "future impacts of increasingly general purpose computer\n",
      "vision models and aid the development of norms and checks\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 24\n",
      "\n",
      "0 20 40 60 80 100\n",
      "Frequency (%)\n",
      "\n",
      "blouse\n",
      "purple\n",
      "\n",
      "newsreader\n",
      "bangs\n",
      "\n",
      "pink\n",
      "pixie cut\n",
      "\n",
      "black hair\n",
      "bob cut\n",
      "\n",
      "magenta\n",
      "hot\n",
      "\n",
      "laughing\n",
      "blazer\n",
      "\n",
      "spokesperson\n",
      "blonde\n",
      "\n",
      "public speaking\n",
      "senior citizen\n",
      "\n",
      "looking\n",
      "female\n",
      "\n",
      "lady\n",
      "woman\n",
      "\n",
      "Top labels,\n",
      "images of women\n",
      "\n",
      "Women\n",
      "Men\n",
      "\n",
      "0 20 40 60 80 100\n",
      "Frequency (%)\n",
      "\n",
      "yellow\n",
      "necktie\n",
      "\n",
      "kid\n",
      "frown\n",
      "\n",
      "shoulder\n",
      "tie\n",
      "\n",
      "display\n",
      "elder\n",
      "\n",
      "photograph\n",
      "walking\n",
      "\n",
      "military officer\n",
      "photo\n",
      "\n",
      "suit\n",
      "facial expression\n",
      "\n",
      "head\n",
      "black\n",
      "\n",
      "player\n",
      "face\n",
      "\n",
      "male\n",
      "man\n",
      "\n",
      "Top labels,\n",
      "images of men\n",
      "\n",
      "Women\n",
      "Men\n",
      "\n",
      "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\n",
      "Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\n",
      "identified with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\n",
      "certain label by gender.\n",
      "\n",
      "around such systems. Our inclusion of surveillance is not\n",
      "intended to indicate enthusiasm for this domain - rather, we\n",
      "think surveillance is an important domain to try to make\n",
      "predictions about given its societal implications (Zuboff,\n",
      "2015; Browne, 2015).\n",
      "\n",
      "We measure the model’s performance on classification of\n",
      "images from CCTV cameras and zero-shot celebrity identifi-\n",
      "cation. We first tested model performance on low-resolution\n",
      "images captured from surveillance cameras (e.g. CCTV\n",
      "cameras). We used the VIRAT dataset (Oh et al., 2011) and\n",
      "data captured by Varadarajan & Odobez (2009), which both\n",
      "consist of real world outdoor scenes with non-actors.\n",
      "\n",
      "Given CLIP’s flexible class construction, we tested 515\n",
      "surveillance images captured from 12 different video se-\n",
      "quences on self-constructed general classes for coarse and\n",
      "fine grained classification. Coarse classification required the\n",
      "model to correctly identify the main subject of the image (i.e.\n",
      "determine if the image was a picture of an empty parking\n",
      "lot, school campus, etc.). For fine-grained classification, the\n",
      "model had to choose between two options constructed to\n",
      "determine if the model could identify the presence/absence\n",
      "of smaller features in the image such as a person standing\n",
      "in the corner.\n",
      "\n",
      "For coarse classification, we constructed the classes by hand-\n",
      "captioning the images ourselves to describe the contents\n",
      "of the image and there were always at least 6 options for\n",
      "\n",
      "the model to choose from. Additionally, we carried out a\n",
      "‘stress test’ where the class set included at least one more\n",
      "caption for something that was ‘close’ to the image (for\n",
      "example, ‘parking lot with white car’ vs. ‘parking lot with\n",
      "red car’). We found that the model had a top-1 accuracy\n",
      "of 91.8% on the CCTV images for the initial evaluation.\n",
      "The accuracy dropped significantly to 51.1% for the second\n",
      "evaluation, with the model incorrectly choosing the ‘close’\n",
      "answer 40.7% of the time.\n",
      "\n",
      "For fine-grained detection, the zero-shot model performed\n",
      "poorly, with results near random. Note that this experiment\n",
      "was targeted only towards detecting the presence or absence\n",
      "of small objects in image sequences.\n",
      "\n",
      "We also tested CLIP’s zero-shot performance for ‘in the\n",
      "wild’ identity detection using the CelebA dataset8. We did\n",
      "this to evaluate the model’s performance for identity detec-\n",
      "tion using just the publicly available data it was pre-trained\n",
      "on. While we tested this on a dataset of celebrities who have\n",
      "a larger number of images on the internet, we hypothesize\n",
      "that the number of images in the pre-training data needed\n",
      "for the model to associate faces with names will keep de-\n",
      "creasing as models get more powerful (see Table 8), which\n",
      "has significant societal implications (Garvie, 2019). This\n",
      "\n",
      "8Note: The CelebA dataset is more representative of faces with\n",
      "lighter skin tones. Due to the nature of the dataset, we were not\n",
      "able to control for race, gender, age, etc.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 25\n",
      "\n",
      "Model 100 Classes 1k Classes 2k Classes\n",
      "\n",
      "CLIP L/14 59.2 43.3 42.2\n",
      "CLIP RN50x64 56.4 39.5 38.4\n",
      "CLIP RN50x16 52.7 37.4 36.3\n",
      "CLIP RN50x4 52.8 38.1 37.3\n",
      "\n",
      "Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy\n",
      "\n",
      "mirrors recent developments in natural language processing,\n",
      "in which recent large language models trained on Internet\n",
      "data often exhibit a surprising ability to provide informa-\n",
      "tion related to relatively minor public figures (Brown et al.,\n",
      "2020).\n",
      "\n",
      "We found that the model had 59.2% top-1 accuracy out\n",
      "of 100 possible classes for ‘in the wild’ 8k celebrity im-\n",
      "ages. However, this performance dropped to 43.3% when\n",
      "we increased our class sizes to 1k celebrity names. This\n",
      "performance is not competitive when compared to produc-\n",
      "tion level models such as Google’s Celebrity Recognition\n",
      "(Google). However, what makes these results noteworthy is\n",
      "that this analysis was done using only zero-shot identifica-\n",
      "tion capabilities based on names inferred from pre-training\n",
      "data - we didn’t use any additional task-specific dataset, and\n",
      "so the (relatively) strong results further indicate that before\n",
      "deploying multimodal models, people will need to carefully\n",
      "study them for behaviors in a given context and domain.\n",
      "\n",
      "CLIP offers significant benefit for tasks that have relatively\n",
      "little data given its zero-shot capabilities. However, large\n",
      "datasets and high performing supervised models exist for\n",
      "many in-demand surveillance tasks such as facial recogni-\n",
      "tion. As a result, CLIP’s comparative appeal for such uses\n",
      "is low. Additionally, CLIP is not designed for common\n",
      "surveillance-relevant tasks like object detection and seman-\n",
      "tic segmentation. This means it has limited use for certain\n",
      "surveillance tasks when models that are designed with these\n",
      "uses in mind such as Detectron2 (Wu et al., 2019) are widely\n",
      "available.\n",
      "\n",
      "However, CLIP does unlock a certain aspect of usability\n",
      "given how it removes the need for training data. Thus, CLIP\n",
      "and similar models could enable bespoke, niche surveillance\n",
      "use cases for which no well-tailored models or datasets exist,\n",
      "and could lower the skill requirements to build such appli-\n",
      "cations. As our experiments show, ZS CLIP displays non-\n",
      "trivial, but not exceptional, performance on a few surveil-\n",
      "lance relevant tasks today.\n",
      "\n",
      "7.3. Future Work\n",
      "\n",
      "This preliminary analysis is intended to illustrate some of\n",
      "the challenges that general purpose computer vision models\n",
      "pose and to give a glimpse into their biases and impacts.\n",
      "\n",
      "We hope that this work motivates future research on the\n",
      "characterization of the capabilities, shortcomings, and biases\n",
      "of such models, and we are excited to engage with the\n",
      "research community on such questions.\n",
      "\n",
      "We believe one good step forward is community exploration\n",
      "to further characterize the capabilities of models like CLIP\n",
      "and - crucially - identify application areas where they have\n",
      "promising performance and areas where they may have\n",
      "reduced performance9. This process of characterization can\n",
      "help researchers increase the likelihood models are used\n",
      "beneficially by:\n",
      "\n",
      "• Identifying potentially beneficial downstream uses of\n",
      "models early in the research process, enabling other\n",
      "researchers to think about applications.\n",
      "\n",
      "• Surfacing tasks with significant sensitivity and a large\n",
      "set of societal stakeholders, which may call for inter-\n",
      "vention by policymakers.\n",
      "\n",
      "• Better characterizing biases in models, alerting other\n",
      "researchers to areas of concern and areas for interven-\n",
      "tions.\n",
      "\n",
      "• Creating suites of tests to evaluate systems like CLIP\n",
      "on, so we can better characterize model capabilities\n",
      "earlier in the development cycle.\n",
      "\n",
      "• Identifying potential failure modes and areas for further\n",
      "work.\n",
      "\n",
      "We plan to contribute to this work, and hope this analysis\n",
      "provides some motivating examples for subsequent research.\n",
      "\n",
      "8. Related Work\n",
      "Any model that leverages written, spoken, signed or any\n",
      "other form of human language as part of its training signal\n",
      "is arguably using natural language as a source of supervi-\n",
      "sion. This is an admittedly extremely broad area and covers\n",
      "most work in the field of distributional semantics including\n",
      "topic models (Blei et al., 2003), word, sentence, and para-\n",
      "graph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\n",
      "Mikolov, 2014), and language models (Bengio et al., 2003).\n",
      "It also includes much of the broader field of NLP that deals\n",
      "with predicting or modeling sequences of natural language\n",
      "in some way. Work in NLP intentionally leveraging natural\n",
      "language supervision in the form of explanations, feedback,\n",
      "instructions, and advice for tasks such as classification (as\n",
      "opposed to the commonly used representation of supervision\n",
      "as a set of arbitrarily encoded discrete category labels) has\n",
      "\n",
      "9A model could be unfit for use due to inadequate performance\n",
      "or due to the inappropriateness of AI use in the application area\n",
      "itself.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 26\n",
      "\n",
      "been explored in many creative and advanced ways. Dialog\n",
      "based learning (Weston, 2016; Li et al., 2016; Hancock et al.,\n",
      "2019) develops techniques to learn from interactive natural\n",
      "language feedback in dialog. Several papers have leveraged\n",
      "semantic parsing to convert natural language explanations\n",
      "into features (Srivastava et al., 2017) or additional training\n",
      "labels (Hancock et al., 2018). More recently, ExpBERT\n",
      "(Murty et al., 2020) uses feature representations produced\n",
      "by conditioning a deep contextual language model on nat-\n",
      "ural language explanations and descriptions of relations to\n",
      "improve performance on the task of relation extraction.\n",
      "\n",
      "CLIP is an example of using natural language as a training\n",
      "signal for learning about a domain other than language. In\n",
      "this context, the earliest use of the term natural language\n",
      "supervision that we are aware of is the work of Ramanathan\n",
      "et al. (2013) which showed that natural language descrip-\n",
      "tions could be used along side other sources of supervision\n",
      "to improve performance on the task of video event under-\n",
      "standing. However, as mentioned in the introduction and\n",
      "approach section, methods of leveraging natural language\n",
      "descriptions in computer vision well predate the use of this\n",
      "specific term, especially for image retrieval (Mori et al.,\n",
      "1999) and object classification (Wang et al., 2009). Other\n",
      "early work leveraged tags (but not natural language) asso-\n",
      "ciated with images for the task of semantic segmentation\n",
      "(Barnard et al., 2003). More recently, He & Peng (2017)\n",
      "and Liang et al. (2020) demonstrated using natural language\n",
      "descriptions and explanations to improve fine-grained vi-\n",
      "sual classification of birds. Others have investigated how\n",
      "grounded language can be used to improve visual represen-\n",
      "tations and classifiers on the ShapeWorld dataset (Kuhnle\n",
      "& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\n",
      "Finally, techniques which combine natural language with\n",
      "reinforcement learning environments (Narasimhan et al.,\n",
      "2015) have demonstrated exciting emergent behaviors such\n",
      "as systematically accomplishing zero-shot tasks (Hill et al.,\n",
      "2019).\n",
      "\n",
      "CLIP’s pre-training task optimizes for text-image retrieval.\n",
      "This areas of research dates back to the mid-90s with the\n",
      "previously mentioned Mori et al. (1999) as representative of\n",
      "early work. While initial efforts focused primarily on predic-\n",
      "tive objectives over time research shifted towards learning\n",
      "joint multi-modal embedding spaces with techniques like\n",
      "kernel Canonical Correlation Analysis and various ranking\n",
      "objectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\n",
      "Hodosh et al., 2013). Over time work explored many combi-\n",
      "nations of training objective, transfer, and more expressive\n",
      "models and steadily improved performance (Frome et al.,\n",
      "2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\n",
      "2014; Faghri et al., 2017).\n",
      "\n",
      "Other work has leveraged natural language supervision for\n",
      "domains other than images. Stroud et al. (2020) explores\n",
      "\n",
      "large scale representation learning by training a system to\n",
      "pair descriptive text with videos instead of images. Several\n",
      "works have explored using dense spoken natural language\n",
      "supervision for videos (Miech et al., 2019; 2020b). When\n",
      "considered together with CLIP, these works suggest that\n",
      "large scale natural language supervision is a promising way\n",
      "to learn high quality perceptual systems for many domains.\n",
      "Alayrac et al. (2020) extended this line of work to an addi-\n",
      "tional modality by adding raw audio as an additional super-\n",
      "vision source and demonstrated benefits from combining all\n",
      "three sources of supervision.\n",
      "\n",
      "As part of our work on CLIP we also construct a new dataset\n",
      "of image-text pairs. Modern work on image-text retrieval\n",
      "has relied on a set of crowd-sourced sentence level im-\n",
      "age caption evaluation datasets like Pascal1K (Rashtchian\n",
      "et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\n",
      "(Young et al., 2014). However, these datasets are still rel-\n",
      "atively small and limit achievable performance. Several\n",
      "methods have been proposed to create larger datasets au-\n",
      "tomatically with Ordonez et al. (2011) as a notable early\n",
      "example. In the deep learning era, Mithun et al. (2018)\n",
      "demonstrated an additional set of (image, text) pairs col-\n",
      "lected from the internet could improve retrieval performance\n",
      "and several new automatically constructed datasets such as\n",
      "Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\n",
      "2020), and OCR-CC (Yang et al., 2020) have been created.\n",
      "However, these datasets still use significantly more aggres-\n",
      "sive filtering or are designed for a specific task such as OCR\n",
      "and as a result are still much smaller than WIT with between\n",
      "1 and 10 million training examples.\n",
      "\n",
      "A related idea to CLIP is webly supervised learning. This\n",
      "line of work queries image search engines to build image\n",
      "datasets by querying for terms and uses the queries as the\n",
      "labels for the returned images (Fergus et al., 2005). Classi-\n",
      "fiers trained on these large but noisily labeled datasets can\n",
      "be competitive with those trained on smaller carefully la-\n",
      "beled datasets. These image-query pairs are also often used\n",
      "to improve performance on standard datasets as additional\n",
      "training data (Chen & Gupta, 2015). CLIP also uses search\n",
      "queries as part of its dataset creation process. However\n",
      "CLIP only uses full text sequences co-occuring with images\n",
      "as supervision rather than just the queries, which are often\n",
      "only a single word or short n-gram. We also restrict this step\n",
      "in CLIP to text only querying for sub-string matches while\n",
      "most webly supervised work uses standard image search\n",
      "engines which have their own complex retrieval and filter-\n",
      "ing pipelines that often involve computer vision systems.\n",
      "Of this line of work, Learning Everything about Anything:\n",
      "Webly-Supervised Visual Concept Learning (Divvala et al.,\n",
      "2014) has a notably similar ambition and goal as CLIP.\n",
      "\n",
      "Finally, CLIP is related to a recent burst of activity on learn-\n",
      "ing joint models of vision and language (Lu et al., 2019; Tan\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 27\n",
      "\n",
      "& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\n",
      "2020). This line of work focuses on richly connecting vision\n",
      "and language in order to solve complex downstream tasks\n",
      "such as visual question answering, visual commonsense\n",
      "reasoning, or multimodal entailment. These approaches\n",
      "leverage impressively engineered models which combine 3\n",
      "(or more) pre-trained subsystems, typically an image feature\n",
      "model, a region proposal / object detection model, and a\n",
      "pre-trained masked language model such as BERT. These\n",
      "systems are then jointly fine-tuned via various training objec-\n",
      "tives on image-text pairs and applied to the aforementioned\n",
      "tasks and achieve impressive results. CLIP is instead fo-\n",
      "cused on learning visual models from scratch via natural\n",
      "language supervision and does not densely connect the two\n",
      "domains with a joint attention model. The only interaction\n",
      "in a CLIP model between the image and text domain is a\n",
      "single dot product in a learned joint embedding space. We\n",
      "are excited to see CLIP hybridized with this line of work.\n",
      "\n",
      "9. Conclusion\n",
      "We have investigated whether it is possible to transfer the\n",
      "success of task-agnostic web-scale pre-training in NLP to\n",
      "another domain. We find that adopting this formula re-\n",
      "sults in similar behaviors emerging in the field of computer\n",
      "vision and discuss the social implications of this line of\n",
      "research. In order to optimize their training objective, CLIP\n",
      "models learn to perform a wide variety of tasks during pre-\n",
      "training. This task learning can then be leveraged via natural\n",
      "language prompting to enable zero-shot transfer to many\n",
      "existing datasets. At sufficient scale, the performance of this\n",
      "approach can be competitive with task-specific supervised\n",
      "models although there is still room for much improvement.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "We’d like to thank the millions of people involved in creating\n",
      "the data CLIP is trained on. We’d also like to thank Susan\n",
      "Zhang for her work on image conditional language models\n",
      "while at OpenAI, Ishaan Gulrajani for catching an error in\n",
      "the pseudocode, and Irene Solaiman, Miles Brundage, and\n",
      "Gillian Hadfield for their thoughtful feedback on the broader\n",
      "impacts section of the paper. We are also grateful to the\n",
      "Acceleration and Supercomputing teams at OpenAI for their\n",
      "critical work on software and hardware infrastructure this\n",
      "project used. Finally, we’d also like to thank the developers\n",
      "of the many software packages used throughout this project\n",
      "including, but not limited, to Numpy (Harris et al., 2020),\n",
      "SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\n",
      "Flow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\n",
      "pandas (pandas development team, 2020), and scikit-learn\n",
      "(Pedregosa et al., 2011).\n",
      "\n",
      "References\n",
      "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\n",
      "\n",
      "J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\n",
      "Tensorflow: A system for large-scale machine learning. In\n",
      "12th {USENIX} symposium on operating systems design\n",
      "and implementation ({OSDI} 16), pp. 265–283, 2016.\n",
      "\n",
      "Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelović,\n",
      "R., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\n",
      "and Zisserman, A. Self-supervised multimodal versatile\n",
      "networks. arXiv preprint arXiv:2006.16228, 2020.\n",
      "\n",
      "Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\n",
      "S., and Nguyen, A. Strike (with) a pose: Neural networks\n",
      "are easily fooled by strange poses of familiar objects. In\n",
      "Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 4845–4854, 2019.\n",
      "\n",
      "Andreas, J., Klein, D., and Levine, S. Learning with latent\n",
      "language. arXiv preprint arXiv:1711.00482, 2017.\n",
      "\n",
      "Assiri, Y. Stochastic optimization of plain convolutional\n",
      "neural networks with simple methods. arXiv preprint\n",
      "arXiv:2001.08856, 2020.\n",
      "\n",
      "Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning\n",
      "representations by maximizing mutual information across\n",
      "views. In Advances in Neural Information Processing\n",
      "Systems, pp. 15535–15545, 2019.\n",
      "\n",
      "Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\n",
      "freund, D., Tenenbaum, J., and Katz, B. Objectnet: A\n",
      "large-scale bias-controlled dataset for pushing the lim-\n",
      "its of object recognition models. In Advances in Neural\n",
      "Information Processing Systems, pp. 9453–9463, 2019.\n",
      "\n",
      "Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\n",
      "D. M., and Jordan, M. I. Matching words and pictures.\n",
      "Journal of machine learning research, 3(Feb):1107–1135,\n",
      "2003.\n",
      "\n",
      "Bechmann, A. and Bowker, G. C. Unsupervised by any\n",
      "other name: Hidden layers of knowledge production in\n",
      "artificial intelligence on social media. Big Data & Society,\n",
      "6(1):205395171881956, January 2019. doi: 10.1177/\n",
      "2053951718819569. URL https://doi.org/10.\n",
      "1177/2053951718819569.\n",
      "\n",
      "Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\n",
      "neural probabilistic language model. Journal of machine\n",
      "learning research, 3(Feb):1137–1155, 2003.\n",
      "\n",
      "Bhargava, S. and Forsyth, D. Exposing and correcting the\n",
      "gender bias in image captioning datasets and models.\n",
      "arXiv preprint arXiv:1912.00578, 2019.\n",
      "\n",
      "https://doi.org/10.1177/2053951718819569\n",
      "https://doi.org/10.1177/2053951718819569\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 28\n",
      "\n",
      "Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\n",
      "allocation. Journal of machine Learning research, 3(Jan):\n",
      "993–1022, 2003.\n",
      "\n",
      "Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\n",
      "Kalai, A. T. Man is to computer programmer as woman\n",
      "is to homemaker? debiasing word embeddings. Advances\n",
      "in neural information processing systems, 29:4349–4357,\n",
      "2016.\n",
      "\n",
      "Bowker, G. C. and Star, S. L. Sorting things out: Classifica-\n",
      "tion and its consequences. MIT press, 2000.\n",
      "\n",
      "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\n",
      "J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n",
      "Askell, A., et al. Language models are few-shot learners.\n",
      "arXiv preprint arXiv:2005.14165, 2020.\n",
      "\n",
      "Browne, S. Dark Matters: Surveillance of Blackness. Duke\n",
      "University Press, 2015.\n",
      "\n",
      "Bulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\n",
      "visual representations with caption annotations. arXiv\n",
      "e-prints, pp. arXiv–2008, 2020.\n",
      "\n",
      "Buolamwini, J. and Gebru, T. Gender shades: Intersec-\n",
      "tional accuracy disparities in commercial gender classi-\n",
      "fication. In Conference on fairness, accountability and\n",
      "transparency, pp. 77–91, 2018.\n",
      "\n",
      "Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A\n",
      "short note on the kinetics-700 human action dataset. arXiv\n",
      "preprint arXiv:1907.06987, 2019.\n",
      "\n",
      "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\n",
      "D., and Sutskever, I. Generative pretraining from pixels.\n",
      "In International Conference on Machine Learning, pp.\n",
      "1691–1703. PMLR, 2020a.\n",
      "\n",
      "Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training\n",
      "deep nets with sublinear memory cost. arXiv preprint\n",
      "arXiv:1604.06174, 2016.\n",
      "\n",
      "Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\n",
      "simple framework for contrastive learning of visual rep-\n",
      "resentations. arXiv preprint arXiv:2002.05709, 2020b.\n",
      "\n",
      "Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and\n",
      "Hinton, G. Big self-supervised models are strong semi-\n",
      "supervised learners. arXiv preprint arXiv:2006.10029,\n",
      "2020c.\n",
      "\n",
      "Chen, X. and Gupta, A. Webly supervised learning of\n",
      "convolutional networks. In Proceedings of the IEEE\n",
      "International Conference on Computer Vision, pp. 1431–\n",
      "1439, 2015.\n",
      "\n",
      "Chen, X., Fan, H., Girshick, R., and He, K. Improved\n",
      "baselines with momentum contrastive learning. arXiv\n",
      "preprint arXiv:2003.04297, 2020d.\n",
      "\n",
      "Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\n",
      "Cheng, Y., and Liu, J. Uniter: Learning universal image-\n",
      "text representations. arXiv preprint arXiv:1909.11740,\n",
      "2019.\n",
      "\n",
      "Cheng, G., Han, J., and Lu, X. Remote sensing image scene\n",
      "classification: Benchmark and state of the art. Proceed-\n",
      "ings of the IEEE, 105(10):1865–1883, 2017.\n",
      "\n",
      "Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\n",
      "and Dahl, G. E. On empirical comparisons of optimiz-\n",
      "ers for deep learning. arXiv preprint arXiv:1910.05446,\n",
      "2019.\n",
      "\n",
      "Coates, A., Ng, A., and Lee, H. An analysis of single-\n",
      "layer networks in unsupervised feature learning. In Pro-\n",
      "ceedings of the fourteenth international conference on\n",
      "artificial intelligence and statistics, pp. 215–223, 2011.\n",
      "\n",
      "Crawford, K. The trouble with bias. NIPS 2017\n",
      "Keynote, 2017. URL https://www.youtube.com/\n",
      "watch?v=fMym_BKWQzk.\n",
      "\n",
      "Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.\n",
      "In Advances in neural information processing systems,\n",
      "pp. 3079–3087, 2015.\n",
      "\n",
      "D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\n",
      "panahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\n",
      "J., Hoffman, M. D., et al. Underspecification presents\n",
      "challenges for credibility in modern machine learning.\n",
      "arXiv preprint arXiv:2011.03395, 2020.\n",
      "\n",
      "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\n",
      "Fei, L. ImageNet: A Large-Scale Hierarchical Image\n",
      "Database. In CVPR09, 2009.\n",
      "\n",
      "Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\n",
      "and Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\n",
      "image-net.org/challenges/LSVRC/2012/.\n",
      "\n",
      "Desai, K. and Johnson, J. Virtex: Learning visual rep-\n",
      "resentations from textual annotations. arXiv preprint\n",
      "arXiv:2006.06666, 2020.\n",
      "\n",
      "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\n",
      "Pre-training of deep bidirectional transformers for lan-\n",
      "guage understanding. arXiv preprint arXiv:1810.04805,\n",
      "2018.\n",
      "\n",
      "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\n",
      "and Sutskever, I. Jukebox: A generative model for music.\n",
      "arXiv preprint arXiv:2005.00341, 2020.\n",
      "\n",
      "https://www.youtube.com/watch?v=fMym_BKWQzk\n",
      "https://www.youtube.com/watch?v=fMym_BKWQzk\n",
      "http://www.image-net.org/challenges/LSVRC/2012/\n",
      "http://www.image-net.org/challenges/LSVRC/2012/\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 29\n",
      "\n",
      "Divvala, S. K., Farhadi, A., and Guestrin, C. Learning\n",
      "everything about anything: Webly-supervised visual con-\n",
      "cept learning. In Proceedings of the IEEE Conference\n",
      "on Computer Vision and Pattern Recognition, pp. 3270–\n",
      "3277, 2014.\n",
      "\n",
      "Dodge, S. and Karam, L. A study and comparison of human\n",
      "and deep learning recognition performance under visual\n",
      "distortions. In 2017 26th international conference on\n",
      "computer communication and networks (ICCCN), pp. 1–\n",
      "7. IEEE, 2017.\n",
      "\n",
      "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\n",
      "D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\n",
      "Heigold, G., Gelly, S., et al. An image is worth 16x16\n",
      "words: Transformers for image recognition at scale. arXiv\n",
      "preprint arXiv:2010.11929, 2020.\n",
      "\n",
      "Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\n",
      "fier: Zero-shot learning using purely textual descriptions.\n",
      "In Proceedings of the IEEE International Conference on\n",
      "Computer Vision, pp. 2584–2591, 2013.\n",
      "\n",
      "Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\n",
      "proving visual-semantic embeddings with hard negatives.\n",
      "arXiv preprint arXiv:1707.05612, 2017.\n",
      "\n",
      "Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\n",
      "ing object categories from google’s image search. In\n",
      "Tenth IEEE International Conference on Computer Vision\n",
      "(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\n",
      "2005.\n",
      "\n",
      "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\n",
      "Ranzato, M., and Mikolov, T. Devise: A deep visual-\n",
      "semantic embedding model. In Advances in neural infor-\n",
      "mation processing systems, pp. 2121–2129, 2013.\n",
      "\n",
      "Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\n",
      "Large-scale adversarial training for vision-and-language\n",
      "representation learning. arXiv preprint arXiv:2006.06195,\n",
      "2020.\n",
      "\n",
      "Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-\n",
      "guage models better few-shot learners. arXiv preprint\n",
      "arXiv:2012.15723, 2020.\n",
      "\n",
      "Garvie, C., May 2019. URL https://www.\n",
      "flawedfacedata.com/.\n",
      "\n",
      "Geiger, A., Lenz, P., and Urtasun, R. Are we ready for\n",
      "autonomous driving? the kitti vision benchmark suite. In\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), 2012.\n",
      "\n",
      "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\n",
      "mann, F. A., and Brendel, W. Imagenet-trained cnns are\n",
      "\n",
      "biased towards texture; increasing shape bias improves ac-\n",
      "curacy and robustness. arXiv preprint arXiv:1811.12231,\n",
      "2018.\n",
      "\n",
      "Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\n",
      "Brendel, W., Bethge, M., and Wichmann, F. A. Short-\n",
      "cut learning in deep neural networks. arXiv preprint\n",
      "arXiv:2004.07780, 2020.\n",
      "\n",
      "Gomez, L., Patel, Y., Rusiñol, M., Karatzas, D., and Jawahar,\n",
      "C. Self-supervised learning of visual features through\n",
      "embedding images into text topic spaces. In Proceedings\n",
      "of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pp. 4230–4239, 2017.\n",
      "\n",
      "Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\n",
      "ing and harnessing adversarial examples. arXiv preprint\n",
      "arXiv:1412.6572, 2014.\n",
      "\n",
      "Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\n",
      "Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\n",
      "D., Lee, D.-H., et al. Challenges in representation learn-\n",
      "ing: A report on three machine learning contests. Neural\n",
      "Networks, 64:59–63, 2015.\n",
      "\n",
      "Google. Google cloud api: Celebrity recognition. URL\n",
      "https://cloud.google.com/vision/docs/\n",
      "celebrity-recognition.\n",
      "\n",
      "Griewank, A. and Walther, A. Algorithm 799: revolve: an\n",
      "implementation of checkpointing for the reverse or ad-\n",
      "joint mode of computational differentiation. ACM Trans-\n",
      "actions on Mathematical Software (TOMS), 26(1):19–45,\n",
      "2000.\n",
      "\n",
      "Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,\n",
      "P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\n",
      "Z. D., Azar, M. G., et al. Bootstrap your own latent: A\n",
      "new approach to self-supervised learning. arXiv preprint\n",
      "arXiv:2006.07733, 2020.\n",
      "\n",
      "Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv\n",
      "preprint arXiv:1609.09106, 2016.\n",
      "\n",
      "Hancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\n",
      "S., and Ré, C. Training classifiers with natural language\n",
      "explanations. In Proceedings of the conference. Associ-\n",
      "ation for Computational Linguistics. Meeting, volume\n",
      "2018, pp. 1884. NIH Public Access, 2018.\n",
      "\n",
      "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\n",
      "Learning from dialogue after deployment: Feed yourself,\n",
      "chatbot! arXiv preprint arXiv:1901.05415, 2019.\n",
      "\n",
      "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\n",
      "R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\n",
      "Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\n",
      "Kerkwijk, M. H., Brett, M., Haldane, A., Fernández del\n",
      "\n",
      "https://www.flawedfacedata.com/\n",
      "https://www.flawedfacedata.com/\n",
      "https://cloud.google.com/vision/docs/celebrity-recognition\n",
      "https://cloud.google.com/vision/docs/celebrity-recognition\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 30\n",
      "\n",
      "Rı́o, J., Wiebe, M., Peterson, P., Gérard-Marchant, P.,\n",
      "Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\n",
      "Gohlke, C., and Oliphant, T. E. Array programming\n",
      "with NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\n",
      "s41586-020-2649-2.\n",
      "\n",
      "Hays, J. and Efros, A. A. Im2gps: estimating geographic\n",
      "information from a single image. In 2008 ieee confer-\n",
      "ence on computer vision and pattern recognition, pp. 1–8.\n",
      "IEEE, 2008.\n",
      "\n",
      "He, K., Zhang, X., Ren, S., and Sun, J. Delving deep\n",
      "into rectifiers: Surpassing human-level performance on\n",
      "imagenet classification. In Proceedings of the IEEE inter-\n",
      "national conference on computer vision, pp. 1026–1034,\n",
      "2015.\n",
      "\n",
      "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\n",
      "ing for image recognition. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition,\n",
      "pp. 770–778, 2016a.\n",
      "\n",
      "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\n",
      "ing for image recognition. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition,\n",
      "pp. 770–778, 2016b.\n",
      "\n",
      "He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\n",
      "mentum contrast for unsupervised visual representation\n",
      "learning. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pp. 9729–\n",
      "9738, 2020.\n",
      "\n",
      "He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\n",
      "Bag of tricks for image classification with convolutional\n",
      "neural networks. In Proceedings of the IEEE Conference\n",
      "on Computer Vision and Pattern Recognition, pp. 558–\n",
      "567, 2019.\n",
      "\n",
      "He, X. and Peng, Y. Fine-grained image classification via\n",
      "combining vision and language. In Proceedings of the\n",
      "IEEE Conference on Computer Vision and Pattern Recog-\n",
      "nition, pp. 5994–6002, 2017.\n",
      "\n",
      "Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\n",
      "A novel dataset and deep learning benchmark for land\n",
      "use and land cover classification. IEEE Journal of Se-\n",
      "lected Topics in Applied Earth Observations and Remote\n",
      "Sensing, 12(7):2217–2226, 2019.\n",
      "\n",
      "Henaff, O. Data-efficient image recognition with contrastive\n",
      "predictive coding. In International Conference on Ma-\n",
      "chine Learning, pp. 4182–4192. PMLR, 2020.\n",
      "\n",
      "Hendrycks, D. and Dietterich, T. Benchmarking neural\n",
      "network robustness to common corruptions and perturba-\n",
      "tions. arXiv preprint arXiv:1903.12261, 2019.\n",
      "\n",
      "Hendrycks, D. and Gimpel, K. Gaussian error linear units\n",
      "(gelus). arXiv preprint arXiv:1606.08415, 2016.\n",
      "\n",
      "Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\n",
      "Song, D. Natural adversarial examples. arXiv preprint\n",
      "arXiv:1907.07174, 2019.\n",
      "\n",
      "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\n",
      "Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\n",
      "et al. The many faces of robustness: A critical analy-\n",
      "sis of out-of-distribution generalization. arXiv preprint\n",
      "arXiv:2006.16241, 2020a.\n",
      "\n",
      "Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\n",
      "R., and Song, D. Pretrained transformers improve out-of-\n",
      "distribution robustness. arXiv preprint arXiv:2004.06100,\n",
      "2020b.\n",
      "\n",
      "Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\n",
      "Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\n",
      "Y. Deep learning scaling is predictable, empirically. arXiv\n",
      "preprint arXiv:1712.00409, 2017.\n",
      "\n",
      "Hill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\n",
      "M., McClelland, J. L., and Santoro, A. Environmental\n",
      "drivers of systematicity and generalization in a situated\n",
      "agent. In International Conference on Learning Repre-\n",
      "sentations, 2019.\n",
      "\n",
      "Hodosh, M., Young, P., and Hockenmaier, J. Framing image\n",
      "description as a ranking task: Data, models and evaluation\n",
      "metrics. Journal of Artificial Intelligence Research, 47:\n",
      "853–899, 2013.\n",
      "\n",
      "Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\n",
      "Enhancing image geolocalization by combinatorial parti-\n",
      "tioning of maps. In Proceedings of the European Confer-\n",
      "ence on Computer Vision (ECCV), pp. 536–551, 2018.\n",
      "\n",
      "Howard, J. and Ruder, S. Universal language model\n",
      "fine-tuning for text classification. arXiv preprint\n",
      "arXiv:1801.06146, 2018.\n",
      "\n",
      "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\n",
      "B., and Madry, A. Adversarial examples are not bugs,\n",
      "they are features. In Advances in Neural Information\n",
      "Processing Systems, pp. 125–136, 2019.\n",
      "\n",
      "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating\n",
      "deep network training by reducing internal covariate shift.\n",
      "arXiv preprint arXiv:1502.03167, 2015.\n",
      "\n",
      "Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\n",
      "A. Deep structured output learning for unconstrained text\n",
      "recognition. arXiv preprint arXiv:1412.5903, 2014.\n",
      "\n",
      "Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\n",
      "transformer networks. Advances in neural information\n",
      "processing systems, 28:2017–2025, 2015.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 31\n",
      "\n",
      "Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\n",
      "Lawrence Zitnick, C., and Girshick, R. Clevr: A diag-\n",
      "nostic dataset for compositional language and elementary\n",
      "visual reasoning. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp.\n",
      "2901–2910, 2017.\n",
      "\n",
      "Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\n",
      "Learning visual features from large weakly supervised\n",
      "data. In European Conference on Computer Vision, pp.\n",
      "67–84. Springer, 2016.\n",
      "\n",
      "Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\n",
      "modeling in 3d cnn architectures with bert for action\n",
      "recognition. arXiv preprint arXiv:2008.01232, 2020.\n",
      "\n",
      "Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\n",
      "Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and\n",
      "Amodei, D. Scaling laws for neural language models.\n",
      "arXiv preprint arXiv:2001.08361, 2020.\n",
      "\n",
      "Karpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\n",
      "embeddings for bidirectional image sentence mapping.\n",
      "In Advances in neural information processing systems,\n",
      "pp. 1889–1897, 2014.\n",
      "\n",
      "Keyes, O. The misgendering machines: Trans/hci implica-\n",
      "tions of automatic gender recognition. Proceedings of the\n",
      "ACM on Human-Computer Interaction, 2(CSCW):1–22,\n",
      "2018.\n",
      "\n",
      "Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\n",
      "Ringshia, P., and Testuggine, D. The hateful memes\n",
      "challenge: Detecting hate speech in multimodal memes.\n",
      "arXiv preprint arXiv:2005.04790, 2020.\n",
      "\n",
      "Kingma, D. P. and Ba, J. Adam: A method for stochastic\n",
      "optimization. arXiv preprint arXiv:1412.6980, 2014.\n",
      "\n",
      "Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\n",
      "visual-semantic embeddings with multimodal neural lan-\n",
      "guage models. arXiv preprint arXiv:1411.2539, 2014.\n",
      "\n",
      "Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\n",
      "R., Torralba, A., and Fidler, S. Skip-thought vectors.\n",
      "Advances in neural information processing systems, 28:\n",
      "3294–3302, 2015.\n",
      "\n",
      "Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\n",
      "J., Gelly, S., and Houlsby, N. Large scale learning of\n",
      "general visual representations for transfer. arXiv preprint\n",
      "arXiv:1912.11370, 2019.\n",
      "\n",
      "Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\n",
      "models transfer better? In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition,\n",
      "pp. 2661–2671, 2019.\n",
      "\n",
      "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\n",
      "Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\n",
      "D. A., et al. Visual genome: Connecting language and\n",
      "vision using crowdsourced dense image annotations. In-\n",
      "ternational journal of computer vision, 123(1):32–73,\n",
      "2017.\n",
      "\n",
      "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\n",
      "classification with deep convolutional neural networks.\n",
      "In Advances in neural information processing systems,\n",
      "pp. 1097–1105, 2012.\n",
      "\n",
      "Kuhnle, A. and Copestake, A. Shapeworld-a new test\n",
      "methodology for multimodal language understanding.\n",
      "arXiv preprint arXiv:1704.04517, 2017.\n",
      "\n",
      "Kärkkäinen, K. and Joo, J. Fairface: Face attribute dataset\n",
      "for balanced race, gender, and age, 2019.\n",
      "\n",
      "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\n",
      "man, S. J. Building machines that learn and think like\n",
      "people, 2016.\n",
      "\n",
      "Lampert, C. H., Nickisch, H., and Harmeling, S. Learning\n",
      "to detect unseen object classes by between-class attribute\n",
      "transfer. In 2009 IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 951–958. IEEE, 2009.\n",
      "\n",
      "Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\n",
      "of new tasks. 2008.\n",
      "\n",
      "Le, Q. and Mikolov, T. Distributed representations of sen-\n",
      "tences and documents. In International conference on\n",
      "machine learning, pp. 1188–1196, 2014.\n",
      "\n",
      "LeCun, Y. The mnist database of handwritten digits.\n",
      "http://yann. lecun. com/exdb/mnist/.\n",
      "\n",
      "Lee, D.-H. Pseudo-label: The simple and efficient semi-\n",
      "supervised learning method for deep neural networks.\n",
      "\n",
      "Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\n",
      "zero-shot convolutional neural networks using textual\n",
      "descriptions. In Proceedings of the IEEE International\n",
      "Conference on Computer Vision, pp. 4247–4255, 2015.\n",
      "\n",
      "Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\n",
      "visual n-grams from web data. In Proceedings of the\n",
      "IEEE International Conference on Computer Vision, pp.\n",
      "4183–4192, 2017.\n",
      "\n",
      "Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\n",
      "Unicoder-vl: A universal encoder for vision and language\n",
      "by cross-modal pre-training. 2020a.\n",
      "\n",
      "Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\n",
      "Learning through dialogue interactions by asking ques-\n",
      "tions. arXiv preprint arXiv:1612.04936, 2016.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 32\n",
      "\n",
      "Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\n",
      "L., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\n",
      "semantics aligned pre-training for vision-language tasks.\n",
      "arXiv preprint arXiv:2004.06165, 2020b.\n",
      "\n",
      "Liang, W., Zou, J., and Yu, Z. Alice: Active learning with\n",
      "contrastive natural language explanations. arXiv preprint\n",
      "arXiv:2009.10259, 2020.\n",
      "\n",
      "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\n",
      "manan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:\n",
      "Common objects in context. In European conference on\n",
      "computer vision, pp. 740–755. Springer, 2014.\n",
      "\n",
      "Linzen, T. How can we accelerate progress towards\n",
      "human-like linguistic generalization? arXiv preprint\n",
      "arXiv:2005.00955, 2020.\n",
      "\n",
      "Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\n",
      "toniou, G., Shutova, E., and Yannakoudakis, H. A mul-\n",
      "timodal framework for the detection of hateful memes.\n",
      "arXiv preprint arXiv:2012.12871, 2020.\n",
      "\n",
      "Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\n",
      "ssi, R., Kaiser, L., and Shazeer, N. Generating\n",
      "wikipedia by summarizing long sequences. arXiv preprint\n",
      "arXiv:1801.10198, 2018.\n",
      "\n",
      "Locatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S.,\n",
      "Schölkopf, B., and Bachem, O. A sober look at the\n",
      "unsupervised learning of disentangled representations\n",
      "and their evaluation. arXiv preprint arXiv:2010.14766,\n",
      "2020.\n",
      "\n",
      "Loshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\n",
      "dient descent with warm restarts. arXiv preprint\n",
      "arXiv:1608.03983, 2016.\n",
      "\n",
      "Loshchilov, I. and Hutter, F. Decoupled weight decay regu-\n",
      "larization. arXiv preprint arXiv:1711.05101, 2017.\n",
      "\n",
      "Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\n",
      "task-agnostic visiolinguistic representations for vision-\n",
      "and-language tasks. In Advances in Neural Information\n",
      "Processing Systems, pp. 13–23, 2019.\n",
      "\n",
      "Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\n",
      "weakly supervised data and pose representation for action\n",
      "recognition, 2020. URL https://www.youtube.\n",
      "com/watch?v=KOQFxbPPLOE&t=1390s.\n",
      "\n",
      "Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\n",
      "quet, O. Are gans created equal? a large-scale study.\n",
      "Advances in neural information processing systems, 31:\n",
      "700–709, 2018.\n",
      "\n",
      "Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\n",
      "M., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\n",
      "ploring the limits of weakly supervised pretraining. In\n",
      "\n",
      "Proceedings of the European Conference on Computer\n",
      "Vision (ECCV), pp. 181–196, 2018.\n",
      "\n",
      "McCann, B., Bradbury, J., Xiong, C., and Socher, R.\n",
      "Learned in translation: Contextualized word vectors. In\n",
      "Advances in neural information processing systems, pp.\n",
      "6294–6305, 2017.\n",
      "\n",
      "McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\n",
      "natural language decathlon: Multitask learning as ques-\n",
      "tion answering. arXiv preprint arXiv:1806.08730, 2018.\n",
      "\n",
      "Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\n",
      "E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\n",
      "Venkatesh, G., et al. Mixed precision training. arXiv\n",
      "preprint arXiv:1710.03740, 2017.\n",
      "\n",
      "Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\n",
      "I., and Sivic, J. Howto100m: Learning a text-video em-\n",
      "bedding by watching hundred million narrated video clips.\n",
      "In Proceedings of the IEEE international conference on\n",
      "computer vision, pp. 2630–2640, 2019.\n",
      "\n",
      "Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\n",
      "man, A. Rareact: A video dataset of unusual interactions.\n",
      "arXiv preprint arXiv:2008.01018, 2020a.\n",
      "\n",
      "Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\n",
      "and Zisserman, A. End-to-end learning of visual represen-\n",
      "tations from uncurated instructional videos. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 9879–9889, 2020b.\n",
      "\n",
      "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\n",
      "Dean, J. Distributed representations of words and phrases\n",
      "and their compositionality. Advances in neural informa-\n",
      "tion processing systems, 26:3111–3119, 2013.\n",
      "\n",
      "Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\n",
      "of natural distribution shift on question answering models.\n",
      "arXiv preprint arXiv:2004.14444, 2020.\n",
      "\n",
      "Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\n",
      "tion using higher order language priors. 2012.\n",
      "\n",
      "Mithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\n",
      "Chowdhury, A. K. Webly supervised joint embedding for\n",
      "cross-modal image-text retrieval. In Proceedings of the\n",
      "26th ACM international conference on Multimedia, pp.\n",
      "1856–1864, 2018.\n",
      "\n",
      "Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\n",
      "formation based on dividing and vector quantizing images\n",
      "with words. Citeseer, 1999.\n",
      "\n",
      "Mu, J., Liang, P., and Goodman, N. Shaping visual represen-\n",
      "tations with language for few-shot classification. arXiv\n",
      "preprint arXiv:1911.02683, 2019.\n",
      "\n",
      "https://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\n",
      "https://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 33\n",
      "\n",
      "Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\n",
      "cation estimation of photos using a hierarchical model\n",
      "and scene classification. In Proceedings of the European\n",
      "Conference on Computer Vision (ECCV), pp. 563–579,\n",
      "2018.\n",
      "\n",
      "Murty, S., Koh, P. W., and Liang, P. Expbert: Representation\n",
      "engineering with natural language explanations. arXiv\n",
      "preprint arXiv:2005.01932, 2020.\n",
      "\n",
      "Narasimhan, K., Kulkarni, T., and Barzilay, R. Language\n",
      "understanding for text-based games using deep reinforce-\n",
      "ment learning. arXiv preprint arXiv:1506.08941, 2015.\n",
      "\n",
      "Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\n",
      "and Ng, A. Y. Reading digits in natural images with\n",
      "unsupervised feature learning. 2011.\n",
      "\n",
      "Noble, S. U. Algorithms of oppression: How search engines\n",
      "reinforce racism. 2018.\n",
      "\n",
      "Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\n",
      "ing implicit group attitudes and beliefs from a demonstra-\n",
      "tion web site. Group Dynamics: Theory, Research, and\n",
      "Practice, 6(1):101, 2002.\n",
      "\n",
      "Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\n",
      "J. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\n",
      "A large-scale benchmark dataset for event recognition in\n",
      "surveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\n",
      "2011.\n",
      "\n",
      "Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\n",
      "fellow, I. Realistic evaluation of deep semi-supervised\n",
      "learning algorithms. Advances in neural information pro-\n",
      "cessing systems, 31:3235–3246, 2018.\n",
      "\n",
      "Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\n",
      "ing with contrastive predictive coding. arXiv preprint\n",
      "arXiv:1807.03748, 2018.\n",
      "\n",
      "Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\n",
      "images using 1 million captioned photographs. Advances\n",
      "in neural information processing systems, 24:1143–1151,\n",
      "2011.\n",
      "\n",
      "pandas development team, T. pandas-dev/pandas: Pan-\n",
      "das, February 2020. URL https://doi.org/10.\n",
      "5281/zenodo.3509134.\n",
      "\n",
      "Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\n",
      "C. V. Cats and dogs. In IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, 2012.\n",
      "\n",
      "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\n",
      "Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\n",
      "L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\n",
      "M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\n",
      "\n",
      "Bai, J., and Chintala, S. Pytorch: An imperative style,\n",
      "high-performance deep learning library. In Advances\n",
      "in Neural Information Processing Systems 32, pp. 8024–\n",
      "8035, 2019.\n",
      "\n",
      "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\n",
      "Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\n",
      "Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\n",
      "napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\n",
      "Scikit-learn: Machine learning in Python. Journal of\n",
      "Machine Learning Research, 12:2825–2830, 2011.\n",
      "\n",
      "Pennington, J., Socher, R., and Manning, C. D. Glove:\n",
      "Global vectors for word representation. In Proceedings\n",
      "of the 2014 conference on empirical methods in natural\n",
      "language processing (EMNLP), pp. 1532–1543, 2014.\n",
      "\n",
      "Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\n",
      "C., Lee, K., and Zettlemoyer, L. Deep contextualized\n",
      "word representations. arXiv preprint arXiv:1802.05365,\n",
      "2018.\n",
      "\n",
      "Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\n",
      "A. Imagebert: Cross-modal pre-training with large-\n",
      "scale weak-supervised image-text data. arXiv preprint\n",
      "arXiv:2001.07966, 2020.\n",
      "\n",
      "Quattoni, A., Collins, M., and Darrell, T. Learning visual\n",
      "representations using images with captions. In 2007 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 1–8. IEEE, 2007.\n",
      "\n",
      "Radford, A., Narasimhan, K., Salimans, T., and Sutskever,\n",
      "I. Improving language understanding by generative pre-\n",
      "training, 2018.\n",
      "\n",
      "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\n",
      "Sutskever, I. Language models are unsupervised multitask\n",
      "learners. 2019.\n",
      "\n",
      "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\n",
      "Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\n",
      "the limits of transfer learning with a unified text-to-text\n",
      "transformer. arXiv preprint arXiv:1910.10683, 2019.\n",
      "\n",
      "Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\n",
      "J., and Denton, E. Saving face: Investigating the ethical\n",
      "concerns of facial recognition auditing, 2020.\n",
      "\n",
      "Ramanathan, V., Liang, P., and Fei-Fei, L. Video event\n",
      "understanding using natural language descriptions. In\n",
      "Proceedings of the IEEE International Conference on\n",
      "Computer Vision, pp. 905–912, 2013.\n",
      "\n",
      "Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\n",
      "Collecting image annotations using amazon’s mechanical\n",
      "turk. In Proceedings of the NAACL HLT 2010 Workshop\n",
      "on Creating Speech and Language Data with Amazon’s\n",
      "Mechanical Turk, pp. 139–147, 2010.\n",
      "\n",
      "https://doi.org/10.5281/zenodo.3509134\n",
      "https://doi.org/10.5281/zenodo.3509134\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 34\n",
      "\n",
      "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\n",
      "agenet classifiers generalize to imagenet? arXiv preprint\n",
      "arXiv:1902.10811, 2019.\n",
      "\n",
      "Salimans, T. and Kingma, D. P. Weight normalization: A\n",
      "simple reparameterization to accelerate training of deep\n",
      "neural networks. In Advances in neural information pro-\n",
      "cessing systems, pp. 901–909, 2016.\n",
      "\n",
      "Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\n",
      "computers see gender: An evaluation of gender classifica-\n",
      "tion in commercial facial analysis services. Proceedings\n",
      "of the ACM on Human-Computer Interaction, 3(CSCW):\n",
      "1–33, 2019.\n",
      "\n",
      "Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\n",
      "S., Schoonvelde, M., and Lockhart, J. W. Diagnosing\n",
      "gender bias in image recognition systems. Socius, 6:\n",
      "2378023120967171, 2020.\n",
      "\n",
      "Sennrich, R., Haddow, B., and Birch, A. Neural machine\n",
      "translation of rare words with subword units. arXiv\n",
      "preprint arXiv:1508.07909, 2015.\n",
      "\n",
      "Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\n",
      "and Schmidt, L. Do image classifiers generalize across\n",
      "time? arXiv preprint arXiv:1906.02168, 2019.\n",
      "\n",
      "Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\n",
      "ceptual captions: A cleaned, hypernymed, image alt-text\n",
      "dataset for automatic image captioning. In Proceedings\n",
      "of the 56th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers), pp. 2556–\n",
      "2565, 2018.\n",
      "\n",
      "Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\n",
      "Batra, D., Parikh, D., and Rohrbach, M. Towards vqa\n",
      "models that can read. In Proceedings of the IEEE Con-\n",
      "ference on Computer Vision and Pattern Recognition, pp.\n",
      "8317–8326, 2019.\n",
      "\n",
      "Socher, R. and Fei-Fei, L. Connecting modalities: Semi-\n",
      "supervised segmentation and annotation of images using\n",
      "unaligned text corpora. In 2010 IEEE Computer Society\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 966–973. IEEE, 2010.\n",
      "\n",
      "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\n",
      "C. D., Ng, A. Y., and Potts, C. Recursive deep models for\n",
      "semantic compositionality over a sentiment treebank. In\n",
      "Proceedings of the 2013 conference on empirical methods\n",
      "in natural language processing, pp. 1631–1642, 2013.\n",
      "\n",
      "Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\n",
      "A. Y. Grounded compositional semantics for finding and\n",
      "describing images with sentences. Transactions of the\n",
      "Association for Computational Linguistics, 2:207–218,\n",
      "2014.\n",
      "\n",
      "Sohn, K. Improved deep metric learning with multi-class\n",
      "n-pair loss objective. In Advances in neural information\n",
      "processing systems, pp. 1857–1865, 2016.\n",
      "\n",
      "Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\n",
      "Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\n",
      "Kreps, S., McCain, M., Newhouse, A., Blazakis, J.,\n",
      "McGuffie, K., and Wang, J. Release strategies and the\n",
      "social impacts of language models, 2019.\n",
      "\n",
      "Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\n",
      "of 101 human actions classes from videos in the wild.\n",
      "arXiv preprint arXiv:1212.0402, 2012.\n",
      "\n",
      "Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/\n",
      "10.5281/zenodo.2591652. Version 5.5.\n",
      "\n",
      "Srivastava, N. and Salakhutdinov, R. Multimodal learning\n",
      "with deep boltzmann machines. In NIPS, 2012.\n",
      "\n",
      "Srivastava, S., Labutov, I., and Mitchell, T. Joint concept\n",
      "learning and semantic parsing from natural language ex-\n",
      "planations. In Proceedings of the 2017 conference on\n",
      "empirical methods in natural language processing, pp.\n",
      "1527–1536, 2017.\n",
      "\n",
      "Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\n",
      "German Traffic Sign Recognition Benchmark: A multi-\n",
      "class classification competition. In IEEE International\n",
      "Joint Conference on Neural Networks, pp. 1453–1460,\n",
      "2011.\n",
      "\n",
      "Stroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\n",
      "and Schmid, C. Learning video representations from tex-\n",
      "tual web supervision. arXiv preprint arXiv:2007.14937,\n",
      "2020.\n",
      "\n",
      "Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\n",
      "A. Inception-v4, inception-resnet and the impact\n",
      "of residual connections on learning. arXiv preprint\n",
      "arXiv:1602.07261, 2016.\n",
      "\n",
      "Tan, H. and Bansal, M. Lxmert: Learning cross-modality\n",
      "encoder representations from transformers. arXiv preprint\n",
      "arXiv:1908.07490, 2019.\n",
      "\n",
      "Tan, M. and Le, Q. V. Efficientnet: Rethinking model\n",
      "scaling for convolutional neural networks. arXiv preprint\n",
      "arXiv:1905.11946, 2019.\n",
      "\n",
      "Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\n",
      "and Schmidt, L. Measuring robustness to natural dis-\n",
      "tribution shifts in image classification. arXiv preprint\n",
      "arXiv:2007.00644, 2020.\n",
      "\n",
      "Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\n",
      "K., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\n",
      "new data in multimedia research. Communications of the\n",
      "ACM, 59(2):64–73, 2016.\n",
      "\n",
      "https://doi.org/10.5281/zenodo.2591652\n",
      "https://doi.org/10.5281/zenodo.2591652\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 35\n",
      "\n",
      "Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview\n",
      "coding. arXiv preprint arXiv:1906.05849, 2019.\n",
      "\n",
      "Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\n",
      "Isola, P. Rethinking few-shot image classification: a\n",
      "good embedding is all you need? arXiv preprint\n",
      "arXiv:2003.11539, 2020.\n",
      "\n",
      "Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\n",
      "images: A large data set for nonparametric object and\n",
      "scene recognition. IEEE transactions on pattern analysis\n",
      "and machine intelligence, 30(11):1958–1970, 2008.\n",
      "\n",
      "Touvron, H., Vedaldi, A., Douze, M., and Jégou, H. Fix-\n",
      "ing the train-test resolution discrepancy. In Advances in\n",
      "neural information processing systems, pp. 8252–8262,\n",
      "2019.\n",
      "\n",
      "Varadarajan, J. and Odobez, J.-M. Topic models for scene\n",
      "analysis and abnormality detection. In 2009 IEEE 12th\n",
      "International Conference on Computer Vision Workshops,\n",
      "ICCV Workshops, pp. 1338–1345. IEEE, 2009.\n",
      "\n",
      "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\n",
      "L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\n",
      "tion is all you need. In Advances in neural information\n",
      "processing systems, pp. 5998–6008, 2017.\n",
      "\n",
      "Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\n",
      "Welling, M. Rotation equivariant CNNs for digital pathol-\n",
      "ogy. June 2018.\n",
      "\n",
      "Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\n",
      "Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\n",
      "Weckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\n",
      "Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\n",
      "Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, İ.,\n",
      "Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\n",
      "Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\n",
      "Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\n",
      "F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n",
      "1.0: Fundamental Algorithms for Scientific Computing\n",
      "in Python. Nature Methods, 17:261–272, 2020. doi:\n",
      "10.1038/s41592-019-0686-2.\n",
      "\n",
      "Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\n",
      "deep learning era. In Proceedings of the IEEE Interna-\n",
      "tional Conference on Computer Vision, pp. 2621–2630,\n",
      "2017.\n",
      "\n",
      "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\n",
      "Bowman, S. R. Glue: A multi-task benchmark and anal-\n",
      "ysis platform for natural language understanding. arXiv\n",
      "preprint arXiv:1804.07461, 2018.\n",
      "\n",
      "Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\n",
      "bust global representations by penalizing local predictive\n",
      "power. In Advances in Neural Information Processing\n",
      "Systems, pp. 10506–10518, 2019.\n",
      "\n",
      "Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\n",
      "M., Wang, Y., and Liu, W. All you need is boundary: To-\n",
      "ward arbitrary-shaped text spotting. In Proceedings of the\n",
      "AAAI Conference on Artificial Intelligence, volume 34,\n",
      "pp. 12160–12167, 2020.\n",
      "\n",
      "Wang, J., Markert, K., and Everingham, M. Learning mod-\n",
      "els for object recognition from natural language descrip-\n",
      "tions. In BMVC, volume 1, pp. 2, 2009.\n",
      "\n",
      "Weston, J., Bengio, S., and Usunier, N. Large scale im-\n",
      "age annotation: learning to rank with joint word-image\n",
      "embeddings. Machine learning, 81(1):21–35, 2010.\n",
      "\n",
      "Weston, J. E. Dialog-based language learning. In Advances\n",
      "in Neural Information Processing Systems, pp. 829–837,\n",
      "2016.\n",
      "\n",
      "Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\n",
      "cation with convolutional neural networks. In European\n",
      "Conference on Computer Vision, pp. 37–55. Springer,\n",
      "2016.\n",
      "\n",
      "Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\n",
      "shick, R. Detectron2. https://github.com/\n",
      "facebookresearch/detectron2, 2019.\n",
      "\n",
      "Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\n",
      "learning via non-parametric instance-level discrimination.\n",
      "arXiv preprint arXiv:1805.01978, 2018.\n",
      "\n",
      "Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\n",
      "with noisy student improves imagenet classification. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pp. 10687–10698, 2020.\n",
      "\n",
      "y Arcas, B. A., Mitchell, M., and Todorov,\n",
      "A. Physiognomy’s new clothes. 2017.\n",
      "URL https://medium.com/@blaisea/\n",
      "physiognomys-new-clothes-f2d4b59fdd6a.\n",
      "\n",
      "Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\n",
      "L., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\n",
      "pre-training for text-vqa and text-caption. arXiv preprint\n",
      "arXiv:2012.04638, 2020.\n",
      "\n",
      "Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\n",
      "T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\n",
      "Yu, L., Dyer, C., et al. Learning and evaluating general\n",
      "linguistic intelligence. arXiv preprint arXiv:1901.11373,\n",
      "2019.\n",
      "\n",
      "Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\n",
      "image descriptions to visual denotations: New similarity\n",
      "metrics for semantic inference over event descriptions.\n",
      "Transactions of the Association for Computational Lin-\n",
      "guistics, 2:67–78, 2014.\n",
      "\n",
      "https://github.com/facebookresearch/detectron2\n",
      "https://github.com/facebookresearch/detectron2\n",
      "https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\n",
      "https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 36\n",
      "\n",
      "Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\n",
      "and Wang, H. Ernie-vil: Knowledge enhanced vision-\n",
      "language representations through scene graph. arXiv\n",
      "preprint arXiv:2006.16934, 2020.\n",
      "\n",
      "Zeiler, M. D. and Fergus, R. Visualizing and understand-\n",
      "ing convolutional networks. In European conference on\n",
      "computer vision, pp. 818–833. Springer, 2014.\n",
      "\n",
      "Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\n",
      "Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\n",
      "mann, M., Dosovitskiy, A., et al. A large-scale study of\n",
      "representation learning with the visual task adaptation\n",
      "benchmark. arXiv preprint arXiv:1910.04867, 2019.\n",
      "\n",
      "Zhang, R. Making convolutional networks shift-invariant\n",
      "again. arXiv preprint arXiv:1904.11486, 2019.\n",
      "\n",
      "Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\n",
      "glotz, C. P. Contrastive learning of medical visual repre-\n",
      "sentations from paired images and text. arXiv preprint\n",
      "arXiv:2010.00747, 2020.\n",
      "\n",
      "Zuboff, S. Big other: surveillance capitalism and the\n",
      "prospects of an information civilization. Journal of Infor-\n",
      "mation Technology, 30(1):75–89, 2015.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 37\n",
      "\n",
      "A. Linear-probe evaluation\n",
      "We provide additional details for linear probe experiments\n",
      "presented in this paper, including the list of the datasets and\n",
      "models used for evaluation.\n",
      "\n",
      "A.1. Datasets\n",
      "\n",
      "We use the 12 datasets from the well-studied evaluation\n",
      "suite introduced by (Kornblith et al., 2019) and add 15\n",
      "additional datasets in order to assess the performance of\n",
      "models on a wider variety of distributions and tasks. These\n",
      "datasets include MNIST, the Facial Expression Recognition\n",
      "2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\n",
      "et al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\n",
      "RESISC45 dataset (Cheng et al., 2017), the German Traf-\n",
      "fic Sign Recognition Benchmark (GTSRB) dataset (Stal-\n",
      "lkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\n",
      "PatchCamelyon (Veeling et al., 2018), the UCF101 action\n",
      "recognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\n",
      "reira et al., 2019), 2,500 random samples of the CLEVR\n",
      "dataset (Johnson et al., 2017), the Hateful Memes dataset\n",
      "(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\n",
      "et al., 2012). For the two video datasets (UCF101 and Ki-\n",
      "netics700), we use the middle frame of each video clip as\n",
      "the input image. STL-10 and UCF101 have multiple pre-\n",
      "defined train/validation/test splits, 10 and 3 respectively, and\n",
      "we report the average over all splits. Details on each dataset\n",
      "and the corresponding evaluation metrics are provided in\n",
      "Table 9.\n",
      "\n",
      "Additionally, we created two datasets that we call Coun-\n",
      "try211 and Rendered SST2. The Country211 dataset is\n",
      "designed to assess the geolocation capability of visual rep-\n",
      "resentations. We filtered the YFCC100m dataset (Thomee\n",
      "et al., 2016) to find 211 countries (defined as having an\n",
      "ISO-3166 country code) that have at least 300 photos with\n",
      "GPS coordinates, and we built a balanced dataset with 211\n",
      "categories, by sampling 200 photos for training and 100\n",
      "photos for testing, for each country.\n",
      "\n",
      "The Rendered SST2 dataset is designed to measure the opti-\n",
      "cal character recognition capability of visual representations.\n",
      "To do so, we used the sentences from the Stanford Sentiment\n",
      "Treebank dataset (Socher et al., 2013) and rendered them\n",
      "into images, with black texts on a white background, in a\n",
      "448×448 resolution. Two example images from this dataset\n",
      "are shown in Figure 19.\n",
      "\n",
      "A.2. Models\n",
      "\n",
      "In combination with the datasets listed above, we evaluate\n",
      "the following series of models using linear probes.\n",
      "\n",
      "LM RN50 This is a multimodal model that uses an au-\n",
      "toregressive loss instead of a contrastive loss, while using\n",
      "\n",
      "the ResNet-50 architecture as in the smallest contrastive\n",
      "model. To do so, the output from the CNN is projected into\n",
      "four tokens, which are then fed as a prefix to a language\n",
      "model autoregressively predicting the text tokens. Apart\n",
      "from the training objective, the model was trained on the\n",
      "same dataset for the same number of epochs as other CLIP\n",
      "models.\n",
      "\n",
      "CLIP-RN Five ResNet-based contrastive CLIP models\n",
      "are included. As discussed in the paper, the first two models\n",
      "follow ResNet-50 and ResNet-101, and we use EfficientNet-\n",
      "style (Tan & Le, 2019) scaling for the next three models\n",
      "which simultaneously scale the model width, the number\n",
      "of layers, and the input resolution to obtain models with\n",
      "roughly 4x, 16x, and 64x computation.\n",
      "\n",
      "CLIP-ViT We include four CLIP models that use the Vi-\n",
      "sion Transformer (Dosovitskiy et al., 2020) architecture as\n",
      "the image encoder. We include three models trained on 224-\n",
      "by-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\n",
      "the ViT-L/14 model fine-tuned on 336-by-336 pixel input\n",
      "images.\n",
      "\n",
      "EfficietNet We use the nine models (B0-B8) from the\n",
      "original EfficientNet paper (Tan & Le, 2019), as well as\n",
      "the noisy-student variants (B0-B7, L2-475, and L2-800)\n",
      "(Tan & Le, 2019). The largest models (L2-475 and L2-800)\n",
      "take the input resolutions of 475x475 and 800x800 pixels,\n",
      "respectively.\n",
      "\n",
      "Instagram-pretrained ResNeXt We use the four models\n",
      "(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\n",
      "et al., 2018), as well as their two FixRes variants which use\n",
      "higher input resolutions (Touvron et al., 2019).\n",
      "\n",
      "Big Transfer (BiT) We use BiT-S and BiT-M models\n",
      "(Kolesnikov et al., 2019), trained on the ImageNet-1k and\n",
      "ImageNet-21k datasets. The model weights for BiT-L is not\n",
      "publicly available.\n",
      "\n",
      "Vision Transformer (ViT) We also include four ViT\n",
      "(Dosovitskiy et al., 2020) checkpoints pretrained on the\n",
      "ImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\n",
      "L/16, and ViT-H/14. We note that their best-performing\n",
      "models, trained on the JFT-300M dataset, are not available\n",
      "publicly.\n",
      "\n",
      "SimCLRv2 The SimCLRv2 (Chen et al., 2020c) project\n",
      "released pre-trained and fine-tuned models in various set-\n",
      "tings. We use the seven pretrain-only checkpoints with\n",
      "selective kernels.\n",
      "\n",
      "BYOL We use the recently released model weights of\n",
      "BYOL (Grill et al., 2020), specifically their 50x1 and 200x2\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 38\n",
      "\n",
      "Figure 19. Two example images from the Rendered SST2 dataset\n",
      "\n",
      "checkpoints.\n",
      "\n",
      "Momentum Contrast (MoCo) We include the MoCo-v1\n",
      "(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\n",
      "checkpoints.\n",
      "\n",
      "VirTex We use the pretrained model of VirTex (Desai &\n",
      "Johnson, 2020). We note that VirTex has a similar model\n",
      "design to CLIP-AR but is trained on a 1000x smaller dataset\n",
      "of high-quality captions from MSCOCO.\n",
      "\n",
      "ResNet We add the original ResNet checkpoints released\n",
      "by (He et al., 2016b), namely ResNet-50, ResNet-101, and\n",
      "ResNet152.\n",
      "\n",
      "A.3. Evaluation\n",
      "\n",
      "We use image features taken from the penultimate layer of\n",
      "each model, ignoring any classification layer provided. For\n",
      "CLIP-ViT models, we used the features before the linear\n",
      "projection to the embedding space, which corresponds to\n",
      "I f in Figure 3. We train a logistic regression classifier\n",
      "using scikit-learn’s L-BFGS implementation, with maxi-\n",
      "mum 1,000 iterations, and report the corresponding met-\n",
      "ric for each dataset. We determine the L2 regularization\n",
      "strength λ using a hyperparameter sweep on the validation\n",
      "sets over the range between 10−6 and 106, with 96 log-\n",
      "arithmically spaced steps. To save compute required for\n",
      "the sweeps, we perform a parametric binary search that\n",
      "starts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\n",
      "eratively halves the interval around the peak until it reaches\n",
      "a resolution of 8 steps per decade. The hyperparameter\n",
      "sweeps are performed on a validation split of each dataset.\n",
      "For the datasets that contain a validation split in addition to\n",
      "\n",
      "a test split, we use the provided validation set to perform\n",
      "the hyperparameter search, and for the datasets that do not\n",
      "provide a validation split or have not published labels for\n",
      "the test data, we split the training dataset to perform the\n",
      "hyperparameter search. For the final result, we combine the\n",
      "validation split back with the training split and report the\n",
      "performance on the unused split.\n",
      "\n",
      "A.4. Results\n",
      "\n",
      "The individual linear probe scores are provided in Table 10\n",
      "and plotted in Figure 20. The best-performing CLIP model,\n",
      "using ViT-L/14 archiecture and 336-by-336 pixel images,\n",
      "achieved the state of the art in 21 of the 27 datasets, i.e.\n",
      "included in the Clopper-Pearson 99.5% confidence interval\n",
      "around each dataset’s top score. For many datasets, CLIP\n",
      "performs significantly better than other models, demonstrat-\n",
      "ing the advantage of natural language supervision over tradi-\n",
      "tional pre-training approaches based on image classification.\n",
      "See Section 3.2 for more discussions on the linear probe\n",
      "results.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 39\n",
      "\n",
      "Dataset Classes Train size Test size Evaluation metric\n",
      "\n",
      "Food-101 102 75,750 25,250 accuracy\n",
      "CIFAR-10 10 50,000 10,000 accuracy\n",
      "CIFAR-100 100 50,000 10,000 accuracy\n",
      "Birdsnap 500 42,283 2,149 accuracy\n",
      "SUN397 397 19,850 19,850 accuracy\n",
      "Stanford Cars 196 8,144 8,041 accuracy\n",
      "FGVC Aircraft 100 6,667 3,333 mean per class\n",
      "Pascal VOC 2007 Classification 20 5,011 4,952 11-point mAP\n",
      "Describable Textures 47 3,760 1,880 accuracy\n",
      "Oxford-IIIT Pets 37 3,680 3,669 mean per class\n",
      "Caltech-101 102 3,060 6,085 mean-per-class\n",
      "Oxford Flowers 102 102 2,040 6,149 mean per class\n",
      "\n",
      "MNIST 10 60,000 10,000 accuracy\n",
      "Facial Emotion Recognition 2013 8 32,140 3,574 accuracy\n",
      "STL-10 10 1000 8000 accuracy\n",
      "EuroSAT 10 10,000 5,000 accuracy\n",
      "RESISC45 45 3,150 25,200 accuracy\n",
      "GTSRB 43 26,640 12,630 accuracy\n",
      "KITTI 4 6,770 711 accuracy\n",
      "Country211 211 43,200 21,100 accuracy\n",
      "PatchCamelyon 2 294,912 32,768 accuracy\n",
      "UCF101 101 9,537 1,794 accuracy\n",
      "Kinetics700 700 494,801 31,669 mean(top1, top5)\n",
      "CLEVR Counts 8 2,000 500 accuracy\n",
      "Hateful Memes 2 8,500 500 ROC AUC\n",
      "Rendered SST2 2 7,792 1,821 accuracy\n",
      "ImageNet 1000 1,281,167 50,000 accuracy\n",
      "\n",
      "Table 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\n",
      "available online at the time of this writing.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 40\n",
      "\n",
      "Fo\n",
      "od\n",
      "\n",
      "10\n",
      "1\n",
      "\n",
      "C\n",
      "IF\n",
      "\n",
      "A\n",
      "R\n",
      "\n",
      "10\n",
      "\n",
      "C\n",
      "IF\n",
      "\n",
      "A\n",
      "R\n",
      "\n",
      "10\n",
      "0\n",
      "\n",
      "B\n",
      "ir\n",
      "\n",
      "ds\n",
      "na\n",
      "\n",
      "p\n",
      "\n",
      "SU\n",
      "N\n",
      "\n",
      "39\n",
      "7\n",
      "\n",
      "C\n",
      "ar\n",
      "\n",
      "s\n",
      "\n",
      "A\n",
      "ir\n",
      "\n",
      "cr\n",
      "af\n",
      "\n",
      "t\n",
      "\n",
      "V\n",
      "O\n",
      "\n",
      "C\n",
      "20\n",
      "\n",
      "07\n",
      "\n",
      "D\n",
      "T\n",
      "\n",
      "D\n",
      "\n",
      "Pe\n",
      "ts\n",
      "\n",
      "C\n",
      "al\n",
      "\n",
      "te\n",
      "ch\n",
      "\n",
      "10\n",
      "1\n",
      "\n",
      "Fl\n",
      "ow\n",
      "\n",
      "er\n",
      "s\n",
      "\n",
      "M\n",
      "N\n",
      "\n",
      "IS\n",
      "T\n",
      "\n",
      "FE\n",
      "R\n",
      "\n",
      "20\n",
      "13\n",
      "\n",
      "ST\n",
      "L\n",
      "\n",
      "10\n",
      "?\n",
      "\n",
      "E\n",
      "ur\n",
      "\n",
      "oS\n",
      "A\n",
      "\n",
      "T\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "SI\n",
      "SC\n",
      "\n",
      "45\n",
      "\n",
      "G\n",
      "T\n",
      "\n",
      "SR\n",
      "B\n",
      "\n",
      "K\n",
      "IT\n",
      "\n",
      "T\n",
      "I\n",
      "\n",
      "C\n",
      "ou\n",
      "\n",
      "nt\n",
      "ry\n",
      "\n",
      "21\n",
      "1\n",
      "\n",
      "PC\n",
      "A\n",
      "\n",
      "M\n",
      "\n",
      "U\n",
      "C\n",
      "\n",
      "F1\n",
      "01\n",
      "\n",
      "K\n",
      "in\n",
      "\n",
      "et\n",
      "ic\n",
      "\n",
      "s7\n",
      "00\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "E\n",
      "V\n",
      "\n",
      "R\n",
      "\n",
      "H\n",
      "at\n",
      "\n",
      "ef\n",
      "ul\n",
      "\n",
      "M\n",
      "em\n",
      "\n",
      "es\n",
      "\n",
      "SS\n",
      "T\n",
      "\n",
      "Im\n",
      "ag\n",
      "\n",
      "eN\n",
      "et\n",
      "\n",
      "LM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "IP\n",
      "-R\n",
      "\n",
      "N\n",
      "\n",
      "50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\n",
      "101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\n",
      "50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\n",
      "\n",
      "50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\n",
      "50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "IP\n",
      "-V\n",
      "\n",
      "iT B/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\n",
      "B/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\n",
      "L/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\n",
      "\n",
      "L/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4\n",
      "\n",
      "E\n",
      "ffi\n",
      "\n",
      "ci\n",
      "en\n",
      "\n",
      "tN\n",
      "et\n",
      "\n",
      "B0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\n",
      "B1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\n",
      "B2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\n",
      "B3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\n",
      "B4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\n",
      "B5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\n",
      "B6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\n",
      "B7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\n",
      "B8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3\n",
      "\n",
      "E\n",
      "ffi\n",
      "\n",
      "ci\n",
      "en\n",
      "\n",
      "tN\n",
      "et\n",
      "\n",
      "N\n",
      "oi\n",
      "\n",
      "sy\n",
      "St\n",
      "\n",
      "ud\n",
      "en\n",
      "\n",
      "t B0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\n",
      "B1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\n",
      "B2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\n",
      "B3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\n",
      "B4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\n",
      "B5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\n",
      "B6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\n",
      "B7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\n",
      "\n",
      "L2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\n",
      "L2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4\n",
      "\n",
      "In\n",
      "st\n",
      "\n",
      "ag\n",
      "ra\n",
      "\n",
      "m\n",
      "\n",
      "32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\n",
      "32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\n",
      "32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\n",
      "32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\n",
      "\n",
      "FixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\n",
      "FixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0\n",
      "\n",
      "B\n",
      "iT\n",
      "\n",
      "-S\n",
      "\n",
      "R50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\n",
      "R50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\n",
      "\n",
      "R101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\n",
      "R101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\n",
      "R152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\n",
      "R152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5\n",
      "\n",
      "B\n",
      "iT\n",
      "\n",
      "-M\n",
      "\n",
      "R50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\n",
      "R50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\n",
      "\n",
      "R101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\n",
      "R101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\n",
      "R152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\n",
      "R152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5\n",
      "\n",
      "V\n",
      "iT\n",
      "\n",
      "B/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\n",
      "B/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\n",
      "L/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\n",
      "H/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9\n",
      "\n",
      "Si\n",
      "m\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "R\n",
      "v2\n",
      "\n",
      "R50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\n",
      "R50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\n",
      "\n",
      "R101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\n",
      "R101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\n",
      "R152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\n",
      "R152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\n",
      "R152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0\n",
      "\n",
      "B\n",
      "Y\n",
      "\n",
      "O\n",
      "L 50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\n",
      "\n",
      "200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2\n",
      "\n",
      "M\n",
      "oC\n",
      "\n",
      "o v1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\n",
      "v2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1\n",
      "\n",
      "VirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7\n",
      "\n",
      "R\n",
      "es\n",
      "\n",
      "N\n",
      "et 50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\n",
      "\n",
      "101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\n",
      "152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1\n",
      "\n",
      "Table 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson confidence\n",
      "interval of each dataset’s top score are shown in bold.\n",
      "\n",
      "?We updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 41\n",
      "\n",
      "100 101 102\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "95\n",
      "ac\n",
      "\n",
      "cu\n",
      "ra\n",
      "\n",
      "cy\n",
      "Food101\n",
      "\n",
      "100 101 102\n",
      "\n",
      "90\n",
      "\n",
      "92\n",
      "\n",
      "94\n",
      "\n",
      "96\n",
      "\n",
      "98\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CIFAR10\n",
      "\n",
      "100 101 102\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CIFAR100\n",
      "\n",
      "100 101 102\n",
      "\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "Birdsnap\n",
      "\n",
      "100 101 102\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "SUN397\n",
      "\n",
      "100 101 102\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "StanfordCars\n",
      "\n",
      "100 101 102\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "FGVCAircraft\n",
      "\n",
      "100 101 102\n",
      "\n",
      "83\n",
      "\n",
      "84\n",
      "\n",
      "85\n",
      "\n",
      "86\n",
      "\n",
      "87\n",
      "\n",
      "88\n",
      "\n",
      "89\n",
      "\n",
      "90\n",
      "\n",
      "11\n",
      "-p\n",
      "\n",
      "oi\n",
      "nt\n",
      "\n",
      " m\n",
      "AP\n",
      "\n",
      " o\n",
      "ve\n",
      "\n",
      "r 2\n",
      "0 \n",
      "\n",
      "cla\n",
      "ss\n",
      "\n",
      "es\n",
      "\n",
      "PascalVOC2007\n",
      "\n",
      "100 101 102\n",
      "\n",
      "72\n",
      "\n",
      "74\n",
      "\n",
      "76\n",
      "\n",
      "78\n",
      "\n",
      "80\n",
      "\n",
      "82\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "DescribableTextures\n",
      "\n",
      "100 101 102\n",
      "84\n",
      "\n",
      "86\n",
      "\n",
      "88\n",
      "\n",
      "90\n",
      "\n",
      "92\n",
      "\n",
      "94\n",
      "\n",
      "96\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "OxfordPets\n",
      "\n",
      "100 101 102\n",
      "\n",
      "90\n",
      "\n",
      "91\n",
      "\n",
      "92\n",
      "\n",
      "93\n",
      "\n",
      "94\n",
      "\n",
      "95\n",
      "\n",
      "96\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n-\n",
      "pe\n",
      "\n",
      "r-c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "Caltech101\n",
      "\n",
      "100 101 102\n",
      "\n",
      "90\n",
      "\n",
      "92\n",
      "\n",
      "94\n",
      "\n",
      "96\n",
      "\n",
      "98\n",
      "\n",
      "100\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "Flowers102\n",
      "\n",
      "100 101 102\n",
      "\n",
      "97.25\n",
      "\n",
      "97.50\n",
      "\n",
      "97.75\n",
      "\n",
      "98.00\n",
      "\n",
      "98.25\n",
      "\n",
      "98.50\n",
      "\n",
      "98.75\n",
      "\n",
      "99.00\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "MNIST\n",
      "\n",
      "100 101 102\n",
      "\n",
      "55.0\n",
      "\n",
      "57.5\n",
      "\n",
      "60.0\n",
      "\n",
      "62.5\n",
      "\n",
      "65.0\n",
      "\n",
      "67.5\n",
      "\n",
      "70.0\n",
      "\n",
      "72.5\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "FacialEmotionRecognition2013\n",
      "\n",
      "100 101 102\n",
      "\n",
      "96.0\n",
      "\n",
      "96.5\n",
      "\n",
      "97.0\n",
      "\n",
      "97.5\n",
      "\n",
      "98.0\n",
      "\n",
      "98.5\n",
      "\n",
      "99.0\n",
      "\n",
      "99.5\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "STL10\n",
      "\n",
      "100 101 102\n",
      "\n",
      "95.5\n",
      "\n",
      "96.0\n",
      "\n",
      "96.5\n",
      "\n",
      "97.0\n",
      "\n",
      "97.5\n",
      "\n",
      "98.0\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "EuroSAT\n",
      "\n",
      "100 101 102\n",
      "82\n",
      "\n",
      "84\n",
      "\n",
      "86\n",
      "\n",
      "88\n",
      "\n",
      "90\n",
      "\n",
      "92\n",
      "\n",
      "94\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "RESISC45\n",
      "\n",
      "100 101 102\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "GTSRB\n",
      "\n",
      "100 101 102\n",
      "\n",
      "57.5\n",
      "\n",
      "60.0\n",
      "\n",
      "62.5\n",
      "\n",
      "65.0\n",
      "\n",
      "67.5\n",
      "\n",
      "70.0\n",
      "\n",
      "72.5\n",
      "\n",
      "75.0\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "KITTI\n",
      "\n",
      "100 101 102\n",
      "\n",
      "81\n",
      "\n",
      "82\n",
      "\n",
      "83\n",
      "\n",
      "84\n",
      "\n",
      "85\n",
      "\n",
      "86\n",
      "\n",
      "87\n",
      "ac\n",
      "\n",
      "cu\n",
      "ra\n",
      "\n",
      "cy\n",
      "PatchCamelyon\n",
      "\n",
      "100 101 102\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "UCF101\n",
      "\n",
      "100 101 102\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n(\n",
      "to\n",
      "\n",
      "p1\n",
      ", t\n",
      "\n",
      "op\n",
      "5)\n",
      "\n",
      "Kinetics700\n",
      "\n",
      "100 101 102\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CLEVRCounts\n",
      "\n",
      "100 101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "Country211\n",
      "\n",
      "100 101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "RO\n",
      "CA\n",
      "\n",
      "UC\n",
      "\n",
      "HatefulMemes\n",
      "\n",
      "100 101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "SST2\n",
      "\n",
      "100 101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "70.0\n",
      "\n",
      "72.5\n",
      "\n",
      "75.0\n",
      "\n",
      "77.5\n",
      "\n",
      "80.0\n",
      "\n",
      "82.5\n",
      "\n",
      "85.0\n",
      "\n",
      "87.5\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "ImageNet\n",
      "CLIP-ViT\n",
      "CLIP-ResNet\n",
      "EfficientNet-NoisyStudent\n",
      "EfficientNet\n",
      "Instagram-pretrained\n",
      "SimCLRv2\n",
      "BYOL\n",
      "MoCo\n",
      "ViT (ImageNet-21k)\n",
      "BiT-M\n",
      "BiT-S\n",
      "ResNet\n",
      "\n",
      "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 42\n",
      "\n",
      "correct label: red and white triangle with exclamation mark warning\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\n",
      "\n",
      "a zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.\n",
      "\n",
      "a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\n",
      "\n",
      "a zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.\n",
      "\n",
      "a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.\n",
      "\n",
      "correct rank: 1/43    correct probability: 45.75%\n",
      "German Traffic Sign Recognition Benchmark (GTSRB)\n",
      "\n",
      "correct label: positive\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a positive review of a movie.\n",
      "\n",
      "a negative review of a movie.\n",
      "\n",
      "correct rank: 1/2    correct probability: 78.21%\n",
      "Stanford Sentiment Treebank\n",
      "\n",
      "correct label: meme\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a meme.\n",
      "\n",
      "a hatespeech meme.\n",
      "\n",
      "correct rank: 1/2    correct probability: 99.20%\n",
      "Hateful Memes\n",
      "\n",
      "correct label: barn\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a barn.\n",
      "\n",
      "a photo of a church.\n",
      "\n",
      "a photo of a threshing machine.\n",
      "\n",
      "a photo of a sawmill.\n",
      "\n",
      "a photo of a prison.\n",
      "\n",
      "correct rank: 1/1000    correct probability: 79.56%\n",
      "ImageNet Sketch\n",
      "\n",
      "correct label(s): antelope\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a antelope.\n",
      "\n",
      "a photo of a zebra.\n",
      "\n",
      "a photo of a car.\n",
      "\n",
      "a photo of a cattle.\n",
      "\n",
      "a photo of a elephant.\n",
      "\n",
      "correct rank: 1/30    correct probability: 99.77%\n",
      "ImageNet Vid\n",
      "\n",
      "correct label: 158\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a street sign of the number: \"1157\".\n",
      "\n",
      "a street sign of the number: \"1165\".\n",
      "\n",
      "a street sign of the number: \"1164\".\n",
      "\n",
      "a street sign of the number: \"1155\".\n",
      "\n",
      "a street sign of the number: \"1364\".\n",
      "\n",
      "correct rank: 83/2000    correct probability: 0.27%\n",
      "Street View House Numbers (SVHN)\n",
      "\n",
      "correct label: 7\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of the number: \"7\".\n",
      "\n",
      "a photo of the number: \"2\".\n",
      "\n",
      "a photo of the number: \"1\".\n",
      "\n",
      "a photo of the number: \"6\".\n",
      "\n",
      "a photo of the number: \"4\".\n",
      "\n",
      "correct rank: 1/10    correct probability: 85.32%\n",
      "MNIST\n",
      "\n",
      "correct label(s): motorcycle\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a motorcycle.\n",
      "\n",
      "a photo of a bicycle.\n",
      "\n",
      "a photo of a car.\n",
      "\n",
      "a photo of a horse.\n",
      "\n",
      "a photo of a dining table.\n",
      "\n",
      "correct rank: 1/20    correct probability: 99.69%\n",
      "PASCAL VOC 2007\n",
      "\n",
      "correct label: perforated\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a polka-dotted texture.\n",
      "\n",
      "a photo of a perforated texture.\n",
      "\n",
      "a photo of a dotted texture.\n",
      "\n",
      "a photo of a studded texture.\n",
      "\n",
      "a photo of a freckled texture.\n",
      "\n",
      "correct rank: 2/47    correct probability: 20.50%\n",
      "Describable Textures Dataset (DTD)\n",
      "\n",
      "correct label: marimba\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a marimba.\n",
      "\n",
      "a photo of a abacus.\n",
      "\n",
      "a photo of a steel drum.\n",
      "\n",
      "a photo of a computer keyboard.\n",
      "\n",
      "a photo of a pool table.\n",
      "\n",
      "correct rank: 1/1000    correct probability: 79.54%\n",
      "ImageNet Blurry\n",
      "\n",
      "correct label: Pill bottle\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a pill bottle.\n",
      "\n",
      "a photo of a bottle cap.\n",
      "\n",
      "a photo of a beer bottle.\n",
      "\n",
      "a photo of a pillow.\n",
      "\n",
      "a photo of a wine bottle.\n",
      "\n",
      "correct rank: 1/113    correct probability: 98.34%\n",
      "ObjectNet ImageNet Overlap\n",
      "\n",
      "correct label: building\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a building.\n",
      "\n",
      "a photo of a carriage.\n",
      "\n",
      "a photo of a statue.\n",
      "\n",
      "a photo of a bag.\n",
      "\n",
      "a photo of a mug.\n",
      "\n",
      "correct rank: 1/12    correct probability: 97.69%\n",
      "aYahoo\n",
      "\n",
      "correct label: Black chinned Hummingbird\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a broad tailed hummingbird, a type of bird.\n",
      "\n",
      "a photo of a calliope hummingbird, a type of bird.\n",
      "\n",
      "a photo of a costas hummingbird, a type of bird.\n",
      "\n",
      "a photo of a black chinned hummingbird, a type of bird.\n",
      "\n",
      "a photo of a annas hummingbird, a type of bird.\n",
      "\n",
      "correct rank: 4/500    correct probability: 12.00%\n",
      "Birdsnap\n",
      "\n",
      "correct label: King Charles Spaniel\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a king charles spaniel.\n",
      "\n",
      "a photo of a brittany dog.\n",
      "\n",
      "a photo of a cocker spaniel.\n",
      "\n",
      "a photo of a papillon.\n",
      "\n",
      "a photo of a sussex spaniel.\n",
      "\n",
      "correct rank: 1/1000    correct probability: 91.61%\n",
      "ImageNet\n",
      "\n",
      "correct label: great masterwort\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a great masterwort, a type of flower.\n",
      "\n",
      "a photo of a bishop of llandaff, a type of flower.\n",
      "\n",
      "a photo of a pincushion flower, a type of flower.\n",
      "\n",
      "a photo of a globe flower, a type of flower.\n",
      "\n",
      "a photo of a prince of wales feathers, a type of flower.\n",
      "\n",
      "correct rank: 1/102    correct probability: 74.25%\n",
      "Flowers-102\n",
      "\n",
      "correct label: country line dancing\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of country line dancing.\n",
      "\n",
      "a photo of square dancing.\n",
      "\n",
      "a photo of swing dancing.\n",
      "\n",
      "a photo of dancing charleston.\n",
      "\n",
      "a photo of salsa dancing.\n",
      "\n",
      "correct rank: 1/700    correct probability: 98.98%\n",
      "Kinetics-700\n",
      "\n",
      "correct label: kennel indoor\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a kennel indoor.\n",
      "\n",
      "a photo of a kennel outdoor.\n",
      "\n",
      "a photo of a jail cell.\n",
      "\n",
      "a photo of a jail indoor.\n",
      "\n",
      "a photo of a veterinarians office.\n",
      "\n",
      "correct rank: 1/723    correct probability: 98.63%\n",
      "SUN\n",
      "\n",
      "correct label: 2012 Honda Accord Coupe\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a 2012 honda accord coupe.\n",
      "\n",
      "a photo of a 2012 honda accord sedan.\n",
      "\n",
      "a photo of a 2012 acura tl sedan.\n",
      "\n",
      "a photo of a 2012 acura tsx sedan.\n",
      "\n",
      "a photo of a 2008 acura tl type-s.\n",
      "\n",
      "correct rank: 1/196    correct probability: 63.30%\n",
      "Stanford Cars\n",
      "\n",
      "correct label: roundabout\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "satellite imagery of roundabout.\n",
      "\n",
      "satellite imagery of intersection.\n",
      "\n",
      "satellite imagery of church.\n",
      "\n",
      "satellite imagery of medium residential.\n",
      "\n",
      "satellite imagery of chaparral.\n",
      "\n",
      "correct rank: 1/45    correct probability: 96.39%\n",
      "RESISC45\n",
      "\n",
      "correct label: Belize\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo i took in french guiana.\n",
      "\n",
      "a photo i took in gabon.\n",
      "\n",
      "a photo i took in cambodia.\n",
      "\n",
      "a photo i took in guyana.\n",
      "\n",
      "a photo i took in belize.\n",
      "\n",
      "correct rank: 5/211    correct probability: 3.92%\n",
      "Country211\n",
      "\n",
      "correct label: Boeing 717\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a mcdonnell douglas md-90, a type of aircraft.\n",
      "\n",
      "a photo of a boeing 717, a type of aircraft.\n",
      "\n",
      "a photo of a fokker 100, a type of aircraft.\n",
      "\n",
      "a photo of a mcdonnell douglas dc-9-30, a type of aircraft.\n",
      "\n",
      "a photo of a boeing 727-200, a type of aircraft.\n",
      "\n",
      "correct rank: 2/100    correct probability: 9.91%\n",
      "FGVC Aircraft\n",
      "\n",
      "correct label: beer bottle\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a beer bottle.\n",
      "\n",
      "a photo of a pirate ship.\n",
      "\n",
      "a photo of a chocolate syrup.\n",
      "\n",
      "a photo of a product packet / packaging.\n",
      "\n",
      "a photo of a wine bottle.\n",
      "\n",
      "correct rank: 1/1000    correct probability: 88.27%\n",
      "ImageNetV2 Matched Frequency\n",
      "\n",
      "correct label: snake\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a snake.\n",
      "\n",
      "a photo of a sweet pepper.\n",
      "\n",
      "a photo of a flatfish.\n",
      "\n",
      "a photo of a turtle.\n",
      "\n",
      "a photo of a lizard.\n",
      "\n",
      "correct rank: 1/100    correct probability: 38.02%\n",
      "CIFAR-100\n",
      "\n",
      "correct label: Maine Coon\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a maine coon, a type of pet.\n",
      "\n",
      "a photo of a persian, a type of pet.\n",
      "\n",
      "a photo of a ragdoll, a type of pet.\n",
      "\n",
      "a photo of a birman, a type of pet.\n",
      "\n",
      "a photo of a siamese, a type of pet.\n",
      "\n",
      "correct rank: 1/37    correct probability: 99.99%\n",
      "Oxford-IIIT Pets\n",
      "\n",
      "correct label: Siberian Husky\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a siberian husky.\n",
      "\n",
      "a photo of a german shepherd dog.\n",
      "\n",
      "a photo of a collie.\n",
      "\n",
      "a photo of a border collie.\n",
      "\n",
      "a photo of a rottweiler.\n",
      "\n",
      "correct rank: 1/200    correct probability: 76.02%\n",
      "ImageNet-R (Rendition)\n",
      "\n",
      "correct label: kangaroo\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a kangaroo.\n",
      "\n",
      "a photo of a gerenuk.\n",
      "\n",
      "a photo of a emu.\n",
      "\n",
      "a photo of a wild cat.\n",
      "\n",
      "a photo of a scorpion.\n",
      "\n",
      "correct rank: 1/102    correct probability: 99.81%\n",
      "Caltech-101\n",
      "\n",
      "correct label: Volleyball Spiking\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a person volleyball spiking.\n",
      "\n",
      "a photo of a person jump rope.\n",
      "\n",
      "a photo of a person long jump.\n",
      "\n",
      "a photo of a person soccer penalty.\n",
      "\n",
      "a photo of a person table tennis shot.\n",
      "\n",
      "correct rank: 1/101    correct probability: 99.30%\n",
      "UCF101\n",
      "\n",
      "correct label: angry\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a happy looking face.\n",
      "\n",
      "a photo of a neutral looking face.\n",
      "\n",
      "a photo of a surprised looking face.\n",
      "\n",
      "a photo of a fearful looking face.\n",
      "\n",
      "a photo of a angry looking face.\n",
      "\n",
      "correct rank: 5/7    correct probability: 8.16%\n",
      "Facial Emotion Recognition 2013 (FER2013)\n",
      "\n",
      "correct label: 4\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of 3 objects.\n",
      "\n",
      "a photo of 4 objects.\n",
      "\n",
      "a photo of 5 objects.\n",
      "\n",
      "a photo of 6 objects.\n",
      "\n",
      "a photo of 10 objects.\n",
      "\n",
      "correct rank: 2/8    correct probability: 17.11%\n",
      "CLEVR Count\n",
      "\n",
      "correct label: bird\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a bird.\n",
      "\n",
      "a photo of a cat.\n",
      "\n",
      "a photo of a deer.\n",
      "\n",
      "a photo of a frog.\n",
      "\n",
      "a photo of a dog.\n",
      "\n",
      "correct rank: 1/10    correct probability: 40.86%\n",
      "CIFAR-10\n",
      "\n",
      "correct label: lynx\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a fox squirrel.\n",
      "\n",
      "a photo of a mongoose.\n",
      "\n",
      "a photo of a skunk.\n",
      "\n",
      "a photo of a red fox.\n",
      "\n",
      "a photo of a lynx.\n",
      "\n",
      "correct rank: 5/200    correct probability: 4.18%\n",
      "ImageNet-A (Adversarial)\n",
      "\n",
      "correct label: healthy lymph node tissue\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "this is a photo of lymph node tumor tissue\n",
      "\n",
      "this is a photo of healthy lymph node tissue\n",
      "\n",
      "correct rank: 2/2    correct probability: 22.81%\n",
      "PatchCamelyon (PCam)\n",
      "\n",
      "correct label: annual crop land\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a centered satellite photo of permanent crop land.\n",
      "\n",
      "a centered satellite photo of pasture land.\n",
      "\n",
      "a centered satellite photo of highway or road.\n",
      "\n",
      "a centered satellite photo of annual crop land.\n",
      "\n",
      "a centered satellite photo of brushland or shrubland.\n",
      "\n",
      "correct rank: 4/10    correct probability: 12.90%\n",
      "EuroSAT\n",
      "\n",
      "correct label(s): airplane,person\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a airplane.\n",
      "\n",
      "a photo of a bird.\n",
      "\n",
      "a photo of a bear.\n",
      "\n",
      "a photo of a giraffe.\n",
      "\n",
      "a photo of a car.\n",
      "\n",
      "correct rank: 1/23    correct probability: 88.98%\n",
      "Youtube-BB\n",
      "\n",
      "correct label: television studio\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of a television studio.\n",
      "\n",
      "a photo of a podium indoor.\n",
      "\n",
      "a photo of a conference room.\n",
      "\n",
      "a photo of a lecture room.\n",
      "\n",
      "a photo of a control room.\n",
      "\n",
      "correct rank: 1/397    correct probability: 90.22%\n",
      "SUN397\n",
      "\n",
      "correct label: guacamole\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "a photo of guacamole, a type of food.\n",
      "\n",
      "a photo of ceviche, a type of food.\n",
      "\n",
      "a photo of edamame, a type of food.\n",
      "\n",
      "a photo of tuna tartare, a type of food.\n",
      "\n",
      "a photo of hummus, a type of food.\n",
      "\n",
      "correct rank: 1/101    correct probability: 90.15%\n",
      "Food101\n",
      "\n",
      "Figure 21. Visualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting\n",
      "Hateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\n",
      "the class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect\n",
      "prediction is colored orange.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 43\n",
      "\n",
      "Fo\n",
      "od\n",
      "\n",
      "10\n",
      "1\n",
      "\n",
      "C\n",
      "IF\n",
      "\n",
      "A\n",
      "R\n",
      "\n",
      "10\n",
      "\n",
      "C\n",
      "IF\n",
      "\n",
      "A\n",
      "R\n",
      "\n",
      "10\n",
      "0\n",
      "\n",
      "B\n",
      "ir\n",
      "\n",
      "ds\n",
      "na\n",
      "\n",
      "p\n",
      "\n",
      "SU\n",
      "N\n",
      "\n",
      "39\n",
      "7\n",
      "\n",
      "St\n",
      "an\n",
      "\n",
      "fo\n",
      "rd\n",
      "\n",
      "C\n",
      "ar\n",
      "\n",
      "s\n",
      "\n",
      "FG\n",
      "V\n",
      "\n",
      "C\n",
      "A\n",
      "\n",
      "ir\n",
      "cr\n",
      "\n",
      "af\n",
      "t\n",
      "\n",
      "V\n",
      "O\n",
      "\n",
      "C\n",
      "20\n",
      "\n",
      "07\n",
      "\n",
      "D\n",
      "T\n",
      "\n",
      "D\n",
      "\n",
      "O\n",
      "xf\n",
      "\n",
      "or\n",
      "d\n",
      "\n",
      "Pe\n",
      "ts\n",
      "\n",
      "C\n",
      "al\n",
      "\n",
      "te\n",
      "ch\n",
      "\n",
      "10\n",
      "1\n",
      "\n",
      "Fl\n",
      "ow\n",
      "\n",
      "er\n",
      "s1\n",
      "\n",
      "02\n",
      "\n",
      "M\n",
      "N\n",
      "\n",
      "IS\n",
      "T\n",
      "\n",
      "FE\n",
      "R\n",
      "\n",
      "20\n",
      "13\n",
      "\n",
      "ST\n",
      "L\n",
      "\n",
      "10\n",
      "\n",
      "E\n",
      "ur\n",
      "\n",
      "oS\n",
      "A\n",
      "\n",
      "T\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "SI\n",
      "SC\n",
      "\n",
      "45\n",
      "\n",
      "G\n",
      "T\n",
      "\n",
      "SR\n",
      "B\n",
      "\n",
      "K\n",
      "IT\n",
      "\n",
      "T\n",
      "I\n",
      "\n",
      "C\n",
      "ou\n",
      "\n",
      "nt\n",
      "ry\n",
      "\n",
      "21\n",
      "1\n",
      "\n",
      "PC\n",
      "am\n",
      "\n",
      "U\n",
      "C\n",
      "\n",
      "F1\n",
      "01\n",
      "\n",
      "K\n",
      "in\n",
      "\n",
      "et\n",
      "ic\n",
      "\n",
      "s7\n",
      "00\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "E\n",
      "V\n",
      "\n",
      "R\n",
      "\n",
      "H\n",
      "at\n",
      "\n",
      "ef\n",
      "ul\n",
      "\n",
      "M\n",
      "em\n",
      "\n",
      "es\n",
      "\n",
      "R\n",
      "en\n",
      "\n",
      "de\n",
      "re\n",
      "\n",
      "d\n",
      "SS\n",
      "\n",
      "T\n",
      "2\n",
      "\n",
      "Im\n",
      "ag\n",
      "\n",
      "eN\n",
      "et\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "IP\n",
      "-R\n",
      "\n",
      "es\n",
      "N\n",
      "\n",
      "et RN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\n",
      "RN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\n",
      "\n",
      "RN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\n",
      "RN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\n",
      "RN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6\n",
      "\n",
      "C\n",
      "L\n",
      "\n",
      "IP\n",
      "-V\n",
      "\n",
      "iT B/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\n",
      "B/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\n",
      "L/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\n",
      "\n",
      "L/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\n",
      "\n",
      "Table 11. Zero-shot performance of CLIP models over 27 datasets.\n",
      "\n",
      "101 102\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "Food101\n",
      "\n",
      "101 102\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "95\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CIFAR10\n",
      "\n",
      "101 102\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CIFAR100\n",
      "\n",
      "101 102\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "Birdsnap\n",
      "\n",
      "101 102\n",
      "\n",
      "60\n",
      "\n",
      "62\n",
      "\n",
      "64\n",
      "\n",
      "66\n",
      "\n",
      "68\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "SUN397\n",
      "\n",
      "101 102\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "StanfordCars\n",
      "\n",
      "101 102\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "FGVCAircraft\n",
      "\n",
      "101 102\n",
      "82.0\n",
      "\n",
      "82.5\n",
      "\n",
      "83.0\n",
      "\n",
      "83.5\n",
      "\n",
      "84.0\n",
      "\n",
      "84.5\n",
      "\n",
      "11\n",
      "-p\n",
      "\n",
      "oi\n",
      "nt\n",
      "\n",
      " m\n",
      "AP\n",
      "\n",
      " o\n",
      "ve\n",
      "\n",
      "r 2\n",
      "0 \n",
      "\n",
      "cla\n",
      "ss\n",
      "\n",
      "es PascalVOC2007\n",
      "\n",
      "101 102\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "DescribableTextures\n",
      "\n",
      "101 102\n",
      "\n",
      "86\n",
      "\n",
      "88\n",
      "\n",
      "90\n",
      "\n",
      "92\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "OxfordPets\n",
      "\n",
      "101 102\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n-\n",
      "pe\n",
      "\n",
      "r-c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "Caltech101\n",
      "\n",
      "101 102\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "90\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n \n",
      "pe\n",
      "\n",
      "r c\n",
      "la\n",
      "\n",
      "ss\n",
      "\n",
      "Flowers102\n",
      "\n",
      "101 102\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "MNIST\n",
      "\n",
      "101 102\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "FacialEmotionRecognition2013\n",
      "\n",
      "101 102\n",
      "\n",
      "95\n",
      "\n",
      "96\n",
      "\n",
      "97\n",
      "\n",
      "98\n",
      "\n",
      "99\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "STL10\n",
      "\n",
      "101 102\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "EuroSAT\n",
      "\n",
      "101 102\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "RESISC45\n",
      "\n",
      "101 102\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "GTSRB\n",
      "\n",
      "101 102\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "KITTI\n",
      "\n",
      "101 102\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "PatchCamelyon\n",
      "\n",
      "101 102\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "UCF101\n",
      "\n",
      "101 102\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "m\n",
      "ea\n",
      "\n",
      "n(\n",
      "to\n",
      "\n",
      "p1\n",
      ", t\n",
      "\n",
      "op\n",
      "5)\n",
      "\n",
      "Kinetics700\n",
      "\n",
      "101 102\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "CLEVRCounts\n",
      "\n",
      "101 102\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "Country211\n",
      "\n",
      "101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "54\n",
      "\n",
      "56\n",
      "\n",
      "58\n",
      "\n",
      "60\n",
      "\n",
      "62\n",
      "\n",
      "RO\n",
      "CA\n",
      "\n",
      "UC\n",
      "\n",
      "HatefulMemes\n",
      "\n",
      "101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "SST2\n",
      "\n",
      "101 102\n",
      "\n",
      "GFLOPs/image\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "ac\n",
      "cu\n",
      "\n",
      "ra\n",
      "cy\n",
      "\n",
      "ImageNet\n",
      "CLIP-ViT\n",
      "CLIP-ResNet\n",
      "ResNet\n",
      "\n",
      "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 44\n",
      "\n",
      "B. Zero-Shot Prediction\n",
      "To provide a qualitative summary / overview of CLIP’s zero-\n",
      "shot performance we visualize a randomly selected predic-\n",
      "tion for 36 different zero-shot CLIP classifiers in Figure\n",
      "21. In addition, Table 11 and Figure 22 show the individual\n",
      "zero-shot performance scores for each dataset.\n",
      "\n",
      "C. Duplicate Detector\n",
      "Our early attempts at duplicate detection and analysis used\n",
      "nearest neighbors in the model’s learned embedding space.\n",
      "While it is intuitive to use a model’s own notion of similar-\n",
      "ity, we encountered issues. We found the model’s feature\n",
      "space is weighted very heavily towards semantic similar-\n",
      "ity. Many false positives occurred due to distinct objects\n",
      "that would be described similarly (soccer balls, flowers of\n",
      "the same species, etc...) having almost perfect similarity.\n",
      "We also observed the model was quite poor at assigning\n",
      "certain kinds of near-duplicates high similarity scores. We\n",
      "noticed repeatedly that images with high-frequency textures\n",
      "(such as fur or stripe patterns) pre-processed by different\n",
      "resizing algorithms (nearest neighbor vs bi-linear) could\n",
      "have surprisingly low similarity. This resulted in many false\n",
      "negatives.\n",
      "\n",
      "We built our own near-duplicate detector to fix this issue.\n",
      "We created a synthetic data augmentation pipeline that com-\n",
      "bined a variety of common image manipulations. The aug-\n",
      "mentation pipeline combines random cropping and zooming,\n",
      "aspect ratio distortion, downsizing and upscaling to different\n",
      "resolutions, minor rotations, jpeg compression, and HSV\n",
      "color jitter. The pipeline also randomly selects from differ-\n",
      "ent interpolation algorithms for all relevant steps. We then\n",
      "trained a model to maximize the similarity of an image and\n",
      "its transformed variant while minimizing similarity to all\n",
      "other images in a training batch. We used the same n-pair /\n",
      "InfoNCE loss as CLIP but with a fixed temperature of 0.07.\n",
      "\n",
      "We selected a ResNet-50 as the model architecture. We\n",
      "modified the base ResNet-50 with the anti-alias improve-\n",
      "ments from (Zhang, 2019) and used weight norm (Sali-\n",
      "mans & Kingma, 2016) instead of batch norm (Ioffe &\n",
      "Szegedy, 2015) to avoid leaking information about dupli-\n",
      "cates via batch statistics - a problem previously noted in\n",
      "(Henaff, 2020). We also found the GELU activation func-\n",
      "tion (Hendrycks & Gimpel, 2016) to perform better for this\n",
      "task. We trained the model with a total batch size of 1,712\n",
      "for approximately 30 million images sampled from our pre-\n",
      "training dataset. At the end of training it achieves nearly\n",
      "100% accuracy on its proxy training task.\n",
      "\n",
      "Linear Classifier Zero Shot\n",
      "Dataset YFCC WIT ∆ YFCC WIT ∆\n",
      "\n",
      "Birdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\n",
      "Country211 23.1 17.3 +5.8 5.2 5.3 +0.1\n",
      "Flowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\n",
      "GTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1\n",
      "UCF101 69.2 74.9 −5.7 22.9 32.0 −9.1\n",
      "Stanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1\n",
      "\n",
      "ImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7\n",
      "Dataset Average 65.5 66.6 −1.1 29.6 30.0 −0.4\n",
      "Dataset “Wins” 10 15 −5 19 18 +1\n",
      "\n",
      "Table 12. CLIP performs similarly when trained on only\n",
      "YFCC100M. Comparing a ResNet-50 trained on only\n",
      "YFCC100M with a same sized subset of WIT shows simi-\n",
      "lar average performance and number of wins on zero shot and\n",
      "linear classifier evals. However, large differences in dataset\n",
      "specific performance occur. We include performance on the 3\n",
      "datasets where YFCC does best and worst compared to WIT\n",
      "according to a linear probe in order to highlight this as well as\n",
      "aggregate performance across all linear and zero-shot evals and\n",
      "the canonical ImageNet dataset.\n",
      "\n",
      "D. Dataset Ablation on YFCC100M\n",
      "To study whether our custom dataset is critical to the perfor-\n",
      "mance of CLIP, we trained a model on a filtered subset of\n",
      "the YFCC100M dataset (details described in Section 2.2)\n",
      "and compared its performance to the same model trained\n",
      "on an equally sized subset of WIT. We train each model for\n",
      "32 epochs at which point transfer performance begins to\n",
      "plateau due to overfitting. Results are shown in Table 12.\n",
      "Across our whole eval suite, YFCC and WIT perform simi-\n",
      "larly on average for both zero-shot and linear probe settings.\n",
      "However, performance on specific fine-grained classifica-\n",
      "tion datasets can vary widely - sometimes by over 10%.\n",
      "Our speculation is that these differences in performance re-\n",
      "flect the relative density of relevant data in each pre-training\n",
      "dataset. For instance, pre-training on YFCC100M, which\n",
      "might contain many photos of birds and flowers (common\n",
      "subjects for photographers), results in better performance on\n",
      "Birdsnap and Flowers102, while pre-training on WIT results\n",
      "in better car and pet classifiers (which appear common in\n",
      "our dataset).\n",
      "\n",
      "Overall, these results are encouraging as they suggest our\n",
      "approach can use any reasonably filtered collection of paired\n",
      "(text, image) data. This mirrors recent work which reported\n",
      "positive results using the same contrastive pre-training ob-\n",
      "jective on the relatively different domain of medical imaging\n",
      "(Zhang et al., 2020). It also is similar to the findings of noisy\n",
      "student self-training which reported only slight improve-\n",
      "ments when using their JFT300M dataset over YFCC100M\n",
      "(Xie et al., 2020). We suspect the major advantage of our\n",
      "dataset over the already existing YFCC100M is its much\n",
      "larger size.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 45\n",
      "\n",
      "Finally, we caution that WIT includes this filtered subset\n",
      "of YFCC100M. This could result in our ablation under-\n",
      "estimating the size of performance differences between\n",
      "YFCC100M and the rest of WIT. We do not think this is\n",
      "likely as YFCC100M is only 3.7% of the overall WIT data\n",
      "blend and it did not noticeably change the performance of\n",
      "models when it was added to the existing data blend during\n",
      "the creation of WIT.\n",
      "\n",
      "E. Selected Task and Dataset Results\n",
      "Due to the large variety of datasets and experiments consid-\n",
      "ered in this work, the main body focuses on summarizing\n",
      "and analyzing overall results. In the following subsections\n",
      "we report details of performance for specific groups of tasks,\n",
      "datasets, and evaluation settings.\n",
      "\n",
      "E.1. Image and Text Retrieval\n",
      "\n",
      "CLIP pre-trains for the task of image-text retrieval on our\n",
      "noisy web-scale dataset. Although the focus of this paper\n",
      "is on representation learning and task learning for the pur-\n",
      "pose of transfer to a wide variety of downstream datasets,\n",
      "validating that CLIP is able to achieve high transfer perfor-\n",
      "mance transfer on exactly what it is pre-trained for is an\n",
      "important sanity check / proof of concept. In Table 13 we\n",
      "check the zero-shot transfer performance of CLIP for both\n",
      "text and image retrieval on the Flickr30k and MSCOCO\n",
      "datsets. Zero-shot CLIP matches or outperforms all prior\n",
      "zero-shot results on these two datasets. Zero-shot CLIP is\n",
      "also competitive with the current overall SOTA for the task\n",
      "of text retrieval on Flickr30k. On image retrieval, CLIP’s\n",
      "performance relative to the overall state of the art is notice-\n",
      "ably lower. However, zero-shot CLIP is still competitive\n",
      "with a fine-tuned Unicoder-VL. On the larger MS-COCO\n",
      "dataset fine-tuning improves performance significantly and\n",
      "zero-shot CLIP is not competitive with the most recent work.\n",
      "For both these datasets we prepend the prompt “a photo\n",
      "of” to the description of each image which we found boosts\n",
      "CLIP’s zero-shot R@1 performance between 1 and 2 points.\n",
      "\n",
      "E.2. Optical Character Recognition\n",
      "\n",
      "Although visualizations have shown that ImageNet models\n",
      "contain features that respond to the presence of text in an\n",
      "image (Zeiler & Fergus, 2014), these representations are\n",
      "not sufficiently fine-grained to use for the task of optical\n",
      "character recognition (OCR). To compensate, models are\n",
      "augmented with the outputs of custom OCR engines and\n",
      "features to boost performance on tasks where this capability\n",
      "is required (Singh et al., 2019; Yang et al., 2020). Early dur-\n",
      "ing the development of CLIP, we noticed that CLIP began to\n",
      "learn primitive OCR capabilities which appeared to steadily\n",
      "improve over the course of the project. To evaluate this\n",
      "qualitatively noticed behavior, we measured performance\n",
      "\n",
      "on 5 datasets requiring the direct and indirect use of OCR.\n",
      "Three of these datasets MNIST (LeCun), SVHN (Netzer\n",
      "et al., 2011), and IIIT5K (Mishra et al., 2012) directly check\n",
      "the ability of a model to perform low-level character and\n",
      "word recognition, while Hateful Memes (Kiela et al., 2020)\n",
      "and SST-2 (Socher et al., 2013) check the ability of a model\n",
      "to use OCR to perform a semantic task. Results are reported\n",
      "in Table 14.\n",
      "\n",
      "CLIP’s performance is still highly variable and appears to\n",
      "be sensitive to some combination of the domain (rendered or\n",
      "natural images) and the type of text to be recognized (num-\n",
      "bers or words). CLIP’s OCR performance is strongest Hate-\n",
      "ful Memes and SST-2 - datasets where the text is digitally\n",
      "rendered and consists mostly of words. On IIIT5K, which\n",
      "is natural images of individually cropped words, zero-shot\n",
      "CLIP performs a bit more respectively and its performance\n",
      "is similar to Jaderberg et al. (2014) early work combining\n",
      "deep learning and structured prediction to perform open-\n",
      "vocabulary OCR. However, performance is noticeably lower\n",
      "on two datasets involving recognition of hand written and\n",
      "street view numbers. CLIP’s 51% accuracy on full number\n",
      "SVHN is well below any published results. Inspection sug-\n",
      "gests CLIP struggles with repeated characters as well as the\n",
      "low resolution and blurry images of SVHN. CLIP’s zero-\n",
      "shot MNIST performance is also poor and is outperformed\n",
      "by supervised logistic regression on raw pixels, one of the\n",
      "simplest possible machine learning baselines.\n",
      "\n",
      "SST-2 is a sentence level NLP dataset which we render into\n",
      "images. We include SST-2 in order to check whether CLIP\n",
      "is able to convert low level OCR capability into a higher\n",
      "level representation. Fitting a linear classifier on CLIP’s rep-\n",
      "resentation of rendered sentences achives 80.5% accuracy.\n",
      "This is on par with the 80% accuracy of a continuous bag\n",
      "of words baseline using GloVe word vectors pre-trained on\n",
      "840 billion tokens (Pennington et al., 2014). While this is a\n",
      "simple NLP baseline by today’s standard, and well below\n",
      "the 97.5% of the current SOTA, it is encouraging to see\n",
      "that CLIP is able to turn an image of rendered text into a\n",
      "non-trivial sentence level representation. Fully supervised\n",
      "CLIP is also surprisingly strong on Hateful Meme detec-\n",
      "tion, where CLIP is only 0.7 points behind the current single\n",
      "model SOTA and several points above the best baseline from\n",
      "the original paper. Similar to SST-2, these other results on\n",
      "Hateful Memes use the ground truth text which CLIP does\n",
      "not have access to. Finally, we note that zero-shot CLIP\n",
      "outperforms the best results using fully supervised linear\n",
      "probes across all other 56 models included in our evaluation\n",
      "suite. This suggests CLIP’s OCR capability is at least some-\n",
      "what unique compared to existing work on self-supervised\n",
      "and supervised representation learning.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 46\n",
      "\n",
      "Text Retrieval Image Retrieval\n",
      "Flickr30k MSCOCO Flickr30k MSCOCO\n",
      "\n",
      "R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\n",
      "\n",
      "Fi\n",
      "ne\n",
      "\n",
      "tu\n",
      "ne\n",
      "\n",
      "Unicoder-VLa 86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\n",
      "Uniterb 87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\n",
      "VILLAc 87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\n",
      "Oscard - - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\n",
      "ERNIE-ViLe 88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -\n",
      "\n",
      "Z\n",
      "er\n",
      "\n",
      "o-\n",
      "Sh\n",
      "\n",
      "ot Visual N-Gramsf 15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\n",
      "ImageBERTg - - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\n",
      "Unicoder-VLa 64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - -\n",
      "Uniterb 83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\n",
      "CLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2\n",
      "\n",
      "Table 13. CLIP improves zero-shot retrieval and is competitive with the best fine-tuned result on Flickr30k text retrieval. Bold\n",
      "indicates best overall performance while an underline indicates best in category performance (zero-shot or fine-tuned). For all other\n",
      "models, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\n",
      "a(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f (Li et al., 2017) g(Qi et al., 2020)\n",
      "\n",
      "IIIT5K Hateful\n",
      "MNIST SVHN 1k Memes SST-2\n",
      "\n",
      "Fi\n",
      "ne\n",
      "\n",
      "tu\n",
      "ne SOTA 99.8a 96.4b 98.9c 78.0d 97.5e\n",
      "\n",
      "JOINTf - - 89.6 - -\n",
      "CBoWg - - - - 80.0\n",
      "\n",
      "L\n",
      "in\n",
      "\n",
      "ea\n",
      "r Raw Pixels 92.5 - - - -\n",
      "\n",
      "ES Best 98.9h - - 58.6h 59.0i\n",
      "\n",
      "CLIP 99.2 - - 77.3 80.5\n",
      "\n",
      "Z\n",
      "S\n",
      "\n",
      "CLIP 88.4 51.0 90.0 63.3 67.9\n",
      "\n",
      "Table 14. OCR performance on 5 datasets. All metrics are accuracy\n",
      "on the test set except for Hateful Memes which reports ROC AUC\n",
      "on the dev set. Single model SOTA reported to best of knowledge.\n",
      "ES Best reports the best performance across the 56 non-CLIP\n",
      "models in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al.,\n",
      "2015) c(Wang et al., 2020) d(Lippe et al., 2020) f (Jaderberg et al.,\n",
      "2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\n",
      "2018)\n",
      "\n",
      "E.3. Action Recognition in Videos\n",
      "\n",
      "For the purpose of learning, a potentially important aspect\n",
      "of natural language is its ability to express, and therefore su-\n",
      "pervise, an extremely wide set of concepts. A CLIP model,\n",
      "since it is trained to pair semi-arbitrary text with images, is\n",
      "likely to receive supervision for a wide range of visual con-\n",
      "cepts involving both common and proper nouns, verbs, and\n",
      "adjectives. ImageNet-1K, by contrast, only labels common\n",
      "nouns. Does the lack of broader supervision in ImageNet\n",
      "result in weaker transfer of ImageNet models to tasks involv-\n",
      "ing the recognition of visual concepts that are not nouns?\n",
      "\n",
      "To investigate this, we measure and compare the perfor-\n",
      "mance of CLIP and ImageNet models on several video\n",
      "\n",
      "UCF101 K700 RareAct\n",
      "Top-1 AVG mWAP mWSAP\n",
      "\n",
      "Fi\n",
      "ne\n",
      "\n",
      "tu\n",
      "ne R(2+1)D-BERTa 98.7 - - -\n",
      "\n",
      "NS ENet-L2b - 84.8 - -\n",
      "HT100M S3Dd 91.3 - - -\n",
      "Baseline I3De - 70.2 - -\n",
      "\n",
      "L\n",
      "in\n",
      "\n",
      "ea\n",
      "r MMV FACf 91.8 - - -\n",
      "\n",
      "NS ENet-L2c 89.4c 68.2c - -\n",
      "CLIP 92.0 73.0 - -\n",
      "\n",
      "Z\n",
      "S HT100M S3Dd - - 30.5 34.8\n",
      "\n",
      "CLIP 80.3 69.6 40.7 44.8\n",
      "\n",
      "Table 15. Action recognition performance on 3 video datasets. Sin-\n",
      "gle model SOTA reported to best of knowledge. Note that linear\n",
      "CLIP and linear NS ENet-L2 are trained and evaluated on a single\n",
      "frame subsampled version of each dataset and not directly compa-\n",
      "rable to prior work. On Kinetics-700, we report the ActivityNet\n",
      "competition metric which is the average of top-1 and top-5 per-\n",
      "formance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al.,\n",
      "2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f (Alayrac\n",
      "et al., 2020)\n",
      "\n",
      "action classification datasets which measure the ability of a\n",
      "model to recognize verbs. In Table 15 we report results on\n",
      "UCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\n",
      "et al., 2019), two common datasets for the task. Unfortu-\n",
      "nately, our CPU based linear classifier takes a prohibitively\n",
      "long time to evaluate on a video dataset due to the very large\n",
      "number of training frames. To deal with this, we aggres-\n",
      "sively sub-sample each video to only a single center frame,\n",
      "effectively turning it into an image classification dataset.\n",
      "As a result, our reported performance in a linear evaluation\n",
      "setting likely under estimates performance by a moderate\n",
      "amount.\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 47\n",
      "\n",
      "IN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\n",
      "Top-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10\n",
      "\n",
      "NS EfficientNet-L2a 88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\n",
      "FixResNeXt101-32x48d V2b 86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\n",
      "Linear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\n",
      "Zero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5\n",
      "\n",
      "Table 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)\n",
      "\n",
      "Despite this handicap, CLIP features transfer surprisingly\n",
      "well to this task. CLIP matches the best prior result on UCF-\n",
      "101 in a linear probe evaluation setting and also outperforms\n",
      "all other models in our evaluation suite. On Kinetics-700,\n",
      "CLIP also outperforms the fine-tuned I3D baseline from the\n",
      "original paper. Since it does not require a training stage,\n",
      "we report CLIP’s zero-shot performance when averaging\n",
      "predictions across all frames. CLIP also performs well in\n",
      "this setting and on Kinetics-700 its performance is within\n",
      "1% of the fully supervised I3D baseline which is trained\n",
      "on 545000 labeled videos. Encouraged by these results, we\n",
      "also measure CLIP’s performance on the recently introduced\n",
      "RareAct dataset (Miech et al., 2020a) which was designed\n",
      "to measure zero-shot recognition of unusual actions like\n",
      "“hammering a phone” and “drilling an egg”. CLIP improves\n",
      "over the prior state of the art, a S3D model trained on auto-\n",
      "matically extracted captions from 100 million instructional\n",
      "videos, by 10 points.\n",
      "\n",
      "While CLIP has encouragingly strong performance on the\n",
      "task of action recognition, we note that there are many differ-\n",
      "ences between the models being compared beyond just their\n",
      "form of supervision such as model architecture, training\n",
      "data distribution, dataset size, and compute used. Further\n",
      "work is needed to more precisely determine what specific\n",
      "design decisions contribute to achieving high performance\n",
      "on this task.\n",
      "\n",
      "1km 25km 200km 750km 2500km\n",
      "\n",
      "ISNsa 16.9 43.0 51.9 66.7 80.2\n",
      "CPlaNetb 16.5 37.1 46.4 62.0 78.5\n",
      "CLIP 13.9 32.9 43.0 62.0 79.3\n",
      "Deep-Ret+c 14.4 33.3 47.7 61.6 73.4\n",
      "PlaNetd 8.4 24.5 37.6 53.6 71.3\n",
      "\n",
      "Table 17. Geolocalization performance on the IM2GPS test set.\n",
      "Metric is percent of images localized within a given radius. Models\n",
      "are ordered by average performance. a(Muller-Budack et al., 2018)\n",
      "b(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al.,\n",
      "2016)\n",
      "\n",
      "E.4. Geolocalization\n",
      "\n",
      "Another behavior we noticed during the development of\n",
      "CLIP was its ability to recognize many places and locations.\n",
      "To quantify this we created the Country211 dataset as de-\n",
      "scribed in Appendix A and report results on it throughout\n",
      "the paper. However it is a new benchmark so to compare\n",
      "with prior work on geolocalization we also report results\n",
      "on the IM2GPS test set from Hays & Efros (2008) in Table\n",
      "17. Since IM2GPS is a regression benchmark, we guess the\n",
      "GPS coordinates of the nearest image in a set of reference\n",
      "images using CLIP’s embedding space. This is not a zero-\n",
      "shot result since it uses nearest-neighbor regression. Despite\n",
      "querying only 1 million images, which is much less than\n",
      "prior work, CLIP performs similarly to several task specific\n",
      "models. It is not, however, competitive with the current state\n",
      "of the art.\n",
      "\n",
      "E.5. Robustness to Distribution Shift\n",
      "\n",
      "Section 3.3 provides a high level summary and analysis of\n",
      "ImageNet-related robustness results. We briefly provide\n",
      "some additional numerical details in this appendix. Per-\n",
      "formance results per dataset are provided in Table 16 and\n",
      "compared with the current state of the art results reported\n",
      "in Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\n",
      "proves the state of the art on 5 of the 7 datasets, ImageNet-R,\n",
      "ObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\n",
      "BB. CLIP’s improvements are largest on ImageNet-Vid and\n",
      "Youtube-BB due to its flexible zero-shot capability and on\n",
      "ImageNet-R, which likely reflects CLIP’s pre-training dis-\n",
      "tribution including significant amounts of creative content.\n",
      "A similar behavior has been documented for the Instagram\n",
      "pre-trained ResNeXt models as discussed in Taori et al.\n",
      "(2020).\n",
      "\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision 48\n",
      "\n",
      "F. Model Hyperparameters\n",
      "\n",
      "Hyperparameter Value\n",
      "\n",
      "Batch size 32768\n",
      "Vocabulary size 49408\n",
      "Training epochs 32\n",
      "Maximum temperature 100.0\n",
      "Weight decay 0.2\n",
      "Warm-up iterations 2000\n",
      "Adam β1 0.9\n",
      "Adam β2 0.999 (ResNet), 0.98 (ViT)\n",
      "Adam ε 10−8 (ResNet), 10−6 (ViT)\n",
      "\n",
      "Table 18. Common CLIP hyperparameters\n",
      "\n",
      "Learning Embedding Input ResNet Text Transformer\n",
      "Model rate dimension resolution blocks width layers width heads\n",
      "\n",
      "RN50 5× 10−4 1024 224 (3, 4, 6, 3) 2048 12 512 8\n",
      "RN101 5× 10−4 512 224 (3, 4, 23, 3) 2048 12 512 8\n",
      "RN50x4 5× 10−4 640 288 (4, 6, 10, 6) 2560 12 640 10\n",
      "RN50x16 4× 10−4 768 384 (6, 8, 18, 8) 3072 12 768 12\n",
      "RN50x64 3.6× 10−4 1024 448 (3, 15, 36, 10) 4096 12 1024 16\n",
      "\n",
      "Table 19. CLIP-ResNet hyperparameters\n",
      "\n",
      "Learning Embedding Input Vision Transformer Text Transformer\n",
      "Model rate dimension resolution layers width heads layers width heads\n",
      "\n",
      "ViT-B/32 5× 10−4 512 224 12 768 12 12 512 8\n",
      "ViT-B/16 5× 10−4 512 224 12 768 12 12 512 8\n",
      "ViT-L/14 4× 10−4 768 224 24 1024 16 12 768 12\n",
      "ViT-L/14-336px 2× 10−5 768 336 24 1024 16 12 768 12\n",
      "\n",
      "Table 20. CLIP-ViT hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run NLP-Lesson-01___search-corpus.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search index of sentence embeddings\n",
    "\n",
    "Let us now create the search index of sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.047653Z",
     "start_time": "2023-03-25T10:55:56.205787Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = embedder.encode(sentences, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Let us check the tokens in each of the sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1521, 1056, 17311, 7987, 8591, 8004, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2009, 2001, 1996, 2190, 1997, 2335, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 1045, 4299, 2017, 2000, 2113, 2008, 2017...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 1037, 6919, 2755, 2000, 8339, 2588, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 1996, 3899, 2003, 1037, 10170, 1025, 104...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[101, 2065, 1037, 2158, 2004, 20781, 2015, 287...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[101, 26922, 3709, 2819, 1998, 26922, 3709, 44...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[101, 3585, 12850, 2869, 2024, 2025, 13680, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[101, 2065, 2017, 1521, 2128, 5341, 1010, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[101, 2026, 2767, 6316, 2038, 1037, 3399, 2008...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[101, 2065, 2017, 2123, 1521, 1056, 2903, 2008...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[101, 1037, 2518, 1997, 5053, 2003, 1037, 6569...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[101, 1996, 7444, 5537, 9099, 16256, 4275, 202...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[101, 1999, 3698, 4083, 1010, 2067, 21572, 450...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[101, 2016, 1040, 8545, 7096, 2426, 1996, 4895...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[101, 2054, 2003, 2023, 2166, 2065, 1010, 2440...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[101, 10556, 8197, 2140, 16475, 8223, 13837, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[101, 17266, 10606, 8223, 9953, 7166, 5313, 66...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[101, 18454, 25526, 2319, 12267, 1006, 2141, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[101, 6819, 8609, 12849, 27766, 1006, 9269, 15...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>62348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[101, 2110, 2235, 2449, 4923, 6349, 2951, 2713...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            input_ids  \\\n",
       "0   [101, 1521, 1056, 17311, 7987, 8591, 8004, 101...   \n",
       "1   [101, 2009, 2001, 1996, 2190, 1997, 2335, 1010...   \n",
       "2   [101, 1045, 4299, 2017, 2000, 2113, 2008, 2017...   \n",
       "3   [101, 1037, 6919, 2755, 2000, 8339, 2588, 1010...   \n",
       "4   [101, 1996, 3899, 2003, 1037, 10170, 1025, 104...   \n",
       "5   [101, 2065, 1037, 2158, 2004, 20781, 2015, 287...   \n",
       "6   [101, 26922, 3709, 2819, 1998, 26922, 3709, 44...   \n",
       "7   [101, 3585, 12850, 2869, 2024, 2025, 13680, 20...   \n",
       "8   [101, 2065, 2017, 1521, 2128, 5341, 1010, 1037...   \n",
       "9   [101, 2026, 2767, 6316, 2038, 1037, 3399, 2008...   \n",
       "10  [101, 2065, 2017, 2123, 1521, 1056, 2903, 2008...   \n",
       "11  [101, 1037, 2518, 1997, 5053, 2003, 1037, 6569...   \n",
       "12  [101, 1996, 7444, 5537, 9099, 16256, 4275, 202...   \n",
       "13  [101, 1999, 3698, 4083, 1010, 2067, 21572, 450...   \n",
       "14  [101, 2016, 1040, 8545, 7096, 2426, 1996, 4895...   \n",
       "15  [101, 2054, 2003, 2023, 2166, 2065, 1010, 2440...   \n",
       "16  [101, 10556, 8197, 2140, 16475, 8223, 13837, 2...   \n",
       "17  [101, 17266, 10606, 8223, 9953, 7166, 5313, 66...   \n",
       "18  [101, 18454, 25526, 2319, 12267, 1006, 2141, 1...   \n",
       "19  [101, 6819, 8609, 12849, 27766, 1006, 9269, 15...   \n",
       "20  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "21  [101, 2110, 2235, 2449, 4923, 6349, 2951, 2713...   \n",
       "\n",
       "                                       token_type_ids  \\\n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       attention_mask  Length  \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     297  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     147  \n",
       "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     168  \n",
       "3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "4   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "5   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "6   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     125  \n",
       "7   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     120  \n",
       "8   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      25  \n",
       "9   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "11  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     248  \n",
       "12  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     242  \n",
       "13  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     236  \n",
       "14  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      90  \n",
       "15  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     131  \n",
       "16  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     475  \n",
       "17  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     710  \n",
       "18  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     495  \n",
       "19  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     672  \n",
       "20  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   62348  \n",
       "21  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     446  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(sentences)\n",
    "df=pd.DataFrame(dict(tokens))\n",
    "#df['input_ids'].iloc[0]\n",
    "df['Length']=df['input_ids'].apply(len)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we chose to get the embeddings as `pytorch` tensors -- this will help us later in doing high-performance searches over the GPU/TPU hardware. What do these embeddings look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.055548Z",
     "start_time": "2023-03-25T10:55:58.050080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T15:19:59.964384Z",
     "start_time": "2023-03-18T15:19:59.950392Z"
    }
   },
   "source": [
    "Clearly, there are 16 embeddings, each of a 768 dimensional vector. Let us glance at a sentence, and its embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.112601Z",
     "start_time": "2023-03-25T10:55:58.059746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "’Twas brillig, and the slithy toves\n",
      "      Did gyre and gimble in the wabe:\n",
      "All mimsy were the borogoves,\n",
      "      And the mome raths outgrabe.\n",
      "\n",
      "“Beware the Jabberwock, my son!\n",
      "      The jaws that bite, the claws that catch!\n",
      "Beware the Jubjub bird, and shun\n",
      "      The frumious Bandersnatch!”\n",
      "\n",
      "He took his vorpal sword in hand;\n",
      "      Long time the manxome foe he sought—\n",
      "So rested he by the Tumtum tree\n",
      "      And stood awhile in thought.\n",
      "\n",
      "And, as in uffish thought he stood,\n",
      "      The Jabberwock, with eyes of flame,\n",
      "Came whiffling through the tulgey wood,\n",
      "      And burbled as it came!\n",
      "\n",
      "One, two! One, two! And through and through\n",
      "      The vorpal blade went snicker-snack!\n",
      "He left it dead, and with its head\n",
      "      He went galumphing back.\n",
      "\n",
      "“And hast thou slain the Jabberwock?\n",
      "      Come to my arms, my beamish boy!\n",
      "O frabjous day! Callooh! Callay!”\n",
      "      He chortled in his joy.\n",
      "\n",
      "’Twas brillig, and the slithy toves\n",
      "      Did gyre and gimble in the wabe:\n",
      "All mimsy were the borogoves,\n",
      "      And the mome raths outgrabe.\n",
      "\n",
      "  tensor([ 1.7917e-02,  3.2575e-01, -5.2674e-01, -4.5594e-01, -1.9075e-01,\n",
      "         1.2122e-01,  6.5697e-01,  3.6187e-01, -3.1310e-01,  4.4082e-01,\n",
      "         1.4342e-02,  2.6244e-01,  5.3394e-01,  1.0590e-01, -4.5185e-01,\n",
      "         1.2903e-01, -6.4802e-02, -1.5091e-01,  7.3926e-01, -5.1614e-01,\n",
      "         2.7852e-01,  2.7218e-02,  1.5634e-01,  3.4818e-01,  5.5325e-01,\n",
      "         3.8035e-01,  1.8087e-01,  4.8250e-01, -2.6185e-01, -1.3253e-01,\n",
      "        -3.6970e-01,  3.3219e-01,  1.6746e-01,  2.1851e-02,  2.8724e-01,\n",
      "         7.9400e-02, -3.2609e-01, -2.0288e-01, -5.9856e-01,  8.8381e-02,\n",
      "         2.5152e-01,  3.5598e-01,  4.8215e-02, -7.2605e-02,  1.9663e-01,\n",
      "        -3.8163e-02,  4.1078e-01,  1.4597e-01,  3.5012e-01, -4.9210e-01,\n",
      "         4.4801e-01,  9.7352e-03,  5.4350e-01, -2.7849e-01, -2.8385e-01,\n",
      "        -7.2804e-02,  2.6930e-01, -9.2223e-03,  2.5226e-01,  3.9880e-01,\n",
      "        -4.8472e-01, -5.0370e-01,  2.1162e-01,  4.4004e-01, -5.7341e-01,\n",
      "         3.1301e-01, -1.4083e-01,  2.2410e-01, -4.4437e-01, -1.8528e-01,\n",
      "         1.4232e-01, -4.4814e-01, -5.3232e-02,  1.1122e-01, -3.7607e-02,\n",
      "        -3.6722e-01, -7.7722e-03, -4.3455e-01, -1.4202e-01,  1.2595e-01,\n",
      "        -7.6095e-02, -2.7487e-01,  2.0302e-01,  4.0621e-01, -4.9197e-02,\n",
      "        -7.9096e-01,  2.9097e-01,  6.0802e-01, -3.7323e-01,  2.4245e-01,\n",
      "        -6.5203e-02, -5.9707e-01, -1.9272e-01,  3.3305e-01, -2.1034e-01,\n",
      "         2.2604e-01, -5.0660e-01,  2.1707e-01,  1.8715e-01,  3.8112e-01,\n",
      "         2.5473e-01,  1.6069e-01,  2.1520e-01,  2.9177e-01, -4.9132e-01,\n",
      "         4.0516e-02, -2.2139e-01, -4.2588e-02,  1.2716e-01,  1.5987e-01,\n",
      "         2.5544e-01, -2.9770e-01, -1.3194e-01,  1.5549e-02,  5.0884e-01,\n",
      "         4.0749e-01, -2.1452e-01, -3.0160e-01, -3.1604e-01, -1.7183e-01,\n",
      "         8.1062e-01, -5.2953e-02,  2.7274e-01, -5.1342e-01,  4.2006e-01,\n",
      "        -3.6531e-01, -7.3516e-02, -1.2259e-02, -1.7089e-01, -1.3319e-01,\n",
      "         1.3879e-01,  9.8377e-01,  3.1317e-02,  1.2674e-01,  6.2552e-02,\n",
      "         1.0961e-02,  5.3247e-01, -1.7293e-01, -4.7855e-01, -1.8883e-01,\n",
      "        -3.1886e-02,  2.3250e-02, -6.8861e-02,  6.1711e-01,  3.1547e-01,\n",
      "        -1.0787e-01, -4.6169e-01, -3.2052e-01, -9.7470e-02,  5.3235e-01,\n",
      "        -1.9616e-01,  3.4645e-02, -3.6577e-01,  2.9087e-01,  2.2671e-01,\n",
      "         2.1960e-01,  1.8154e-01,  4.0864e-01,  3.7546e-01,  1.1522e-01,\n",
      "        -3.8927e-01, -1.4973e-01,  7.6048e-02, -6.7651e-02,  1.7376e-01,\n",
      "        -1.3123e-01, -3.1742e-01, -2.8624e-01, -1.5841e-02, -1.4196e-01,\n",
      "         1.0881e-01, -5.8047e-01,  4.4365e-01,  2.4115e-01,  5.4083e-02,\n",
      "        -5.3161e-01,  1.6474e-01, -2.9610e-01, -4.0890e-01, -2.1082e-01,\n",
      "        -5.2382e-01, -5.5760e-01,  1.2510e-01, -1.3824e-02,  4.2348e-02,\n",
      "         2.1822e-01, -7.5218e-02, -2.3979e-01, -5.7186e-01,  8.4367e-02,\n",
      "        -1.2576e-01, -4.2886e-02,  5.5928e-01,  1.0654e-01,  8.9992e-02,\n",
      "        -2.3590e-01,  1.9082e-01, -5.8835e-01,  1.7601e-01, -3.4115e-01,\n",
      "         2.2822e-02, -5.3441e-02, -3.5236e-01,  1.1238e-01,  6.1241e-02,\n",
      "         1.2119e-01, -3.2559e-01,  2.0884e-01,  3.3278e-02,  9.3340e-03,\n",
      "         4.4278e-01, -1.8672e-01, -1.2111e-01, -5.6137e-02, -1.2680e-01,\n",
      "         1.6150e-01,  2.3153e-01,  1.8705e-01,  1.1947e-01, -2.5572e-01,\n",
      "        -9.2670e-01, -6.9634e-02,  8.8565e-02, -5.9111e-01,  2.7490e-01,\n",
      "         8.0624e-01, -3.2596e-01, -6.9461e-02, -6.4609e-01,  1.6149e-01,\n",
      "        -3.6439e-01,  3.9385e-02, -7.1674e-02,  1.9714e-01,  4.1333e-01,\n",
      "        -2.1130e-01, -2.6286e-01, -2.9745e-01, -1.6346e-01, -1.1724e-01,\n",
      "        -4.7544e-01, -4.3545e-01,  3.7929e-02,  3.0695e-01, -1.3773e-02,\n",
      "        -2.0251e-02,  4.0708e-01, -1.1444e-03,  3.1819e-01,  1.8085e-01,\n",
      "         4.6539e-01,  2.1454e-02, -4.0807e-02, -2.8014e-01, -2.7622e-01,\n",
      "         6.6178e-02, -1.8675e-01, -7.5373e-01,  2.1418e-02,  3.1014e-01,\n",
      "         3.9646e-01,  4.0270e-01,  4.3578e-01,  1.8946e-01, -5.6522e-02,\n",
      "         7.1969e-01,  2.6049e-01,  1.7067e-01,  2.4791e-01, -2.3705e-01,\n",
      "        -5.5930e-02,  1.7302e-02,  1.6413e-01,  4.0594e-01,  1.6504e-01,\n",
      "         7.5399e-02, -1.1570e-01, -2.4189e-01, -7.6809e-02,  6.3857e-02,\n",
      "        -3.0081e-01,  7.4117e-01, -6.9915e-01, -1.4235e-01, -1.9157e-01,\n",
      "        -8.2039e-02,  5.7404e-01,  3.2158e-01,  2.7205e-01, -2.1700e-01,\n",
      "         2.9987e-02,  7.9279e-01,  6.1427e-01, -5.1988e-02, -5.8224e-01,\n",
      "        -8.6427e-04,  1.5717e-01,  1.0860e-01,  2.2685e-01,  3.0562e-01,\n",
      "        -2.3346e-01,  1.5908e-01, -5.7816e-01,  1.6771e-01, -3.8282e-01,\n",
      "        -2.4983e-01, -1.9433e-01, -1.5392e-01, -1.7070e+00,  2.6598e-01,\n",
      "         1.1801e-01, -2.2074e-01,  6.3294e-01,  3.6595e-01, -9.0837e-02,\n",
      "         1.1078e-01,  5.9197e-03, -1.8070e-02, -2.4574e-01, -5.4478e-01,\n",
      "         6.0092e-01, -2.2873e-01,  2.3063e-01, -6.8054e-01, -2.9629e-01,\n",
      "        -3.2019e-01,  3.4550e-01, -6.7519e-01,  1.9262e-01,  2.3329e-04,\n",
      "         9.4906e-02, -5.4171e-01, -2.9485e-01,  1.7788e-01,  4.3582e-01,\n",
      "        -1.5583e-01, -4.0829e-02,  1.7331e-01, -2.2900e-01,  4.2604e-01,\n",
      "         2.2739e-01,  2.9901e-02,  1.2949e-02, -3.8811e-02,  5.0931e-01,\n",
      "        -6.9911e-02, -6.8597e-02, -1.0675e-01, -6.8140e-03,  2.3347e-01,\n",
      "        -5.0954e-01,  2.2618e-01,  3.1839e-01, -4.5076e-02, -5.2840e-01,\n",
      "        -3.6181e-01,  6.9245e-01, -4.7530e-03, -3.2137e-01, -2.2106e-01,\n",
      "        -2.1965e-01, -1.4922e-01,  1.5394e-01, -3.2445e-02, -1.9035e-01,\n",
      "        -7.6169e-03,  5.0208e-02, -5.5047e-01, -3.2600e-02,  2.8282e-01,\n",
      "         2.7266e-02, -2.8430e-01,  4.8848e-02,  3.3134e-01,  3.7562e-02,\n",
      "         2.4394e-01,  8.3381e-03, -1.2271e-02, -1.4589e-02, -3.1226e-01,\n",
      "        -4.0162e-02, -5.0416e-01,  3.6569e-01,  1.0153e+00,  1.9812e-02,\n",
      "         7.8752e-04,  2.7361e-01, -4.2070e-01, -3.8222e-01, -4.2397e-01,\n",
      "         2.5626e-01, -5.2781e-01, -3.4485e-01, -5.6268e-01,  1.2262e-01,\n",
      "        -3.9935e-01,  5.1999e-02, -5.5470e-01,  8.0130e-02, -2.2590e-02,\n",
      "        -7.6713e-01,  7.7694e-01,  3.2428e-01,  1.9949e-01,  5.5646e-01,\n",
      "         2.2863e-02,  6.8619e-02,  4.1801e-01,  3.9969e-01, -6.4930e-02,\n",
      "        -3.4573e-02, -7.8673e-02,  4.0344e-01, -1.6961e-01, -2.0260e-03,\n",
      "        -3.9753e-02, -2.6265e-01,  2.7204e-01,  1.5871e-02, -2.3830e-01,\n",
      "         1.8041e-01,  5.5454e-01, -5.0322e-01, -2.1440e-01, -3.6038e-01,\n",
      "        -2.8141e-01,  2.3926e-01,  1.5952e-01,  1.3700e-01,  1.9691e-01,\n",
      "         1.0487e-01,  1.2944e-01,  1.6427e-01, -4.3116e-01, -1.4120e-01,\n",
      "        -1.9606e-01, -2.3791e-01, -2.8282e-01, -3.0180e-01, -2.0097e-01,\n",
      "        -2.8438e-01, -7.9645e-02,  4.1995e-01,  5.7060e-02, -7.0792e-01,\n",
      "        -6.1364e-01,  2.9859e-01, -3.2533e-01, -7.5628e-02, -3.6611e-02,\n",
      "         4.6359e-01,  2.8782e-01,  3.4186e-02,  3.3679e-01,  1.0766e-01,\n",
      "        -1.0526e-01, -2.7351e-01,  3.6651e-01, -5.9686e-01,  5.0702e-02,\n",
      "         2.0827e-01, -3.3547e-01,  1.3994e-01,  7.5079e-02, -2.9489e-01,\n",
      "        -1.8911e-01,  4.0929e-01,  5.2650e-01, -8.5071e-02,  3.8760e-01,\n",
      "         2.3798e-01,  7.0253e-01,  1.3223e-01, -4.7596e-01,  1.5318e-01,\n",
      "        -2.2118e-01, -2.0029e-02, -1.9628e-01, -2.6742e-01,  5.7637e-02,\n",
      "        -1.6170e-01,  8.3750e-01, -3.9614e-01,  4.1609e-01, -3.2807e-02,\n",
      "         5.1722e-02, -4.4778e-01,  3.5607e-01,  1.9781e-01,  3.0608e-01,\n",
      "        -4.2100e-01,  4.2799e-01,  3.2833e-01, -1.2995e-01, -2.2661e-01,\n",
      "        -1.9074e-01, -5.5466e-01,  2.6481e-01, -2.7469e-01,  5.6598e-02,\n",
      "         4.0208e-01, -1.7044e-01, -5.2993e-01,  6.1365e-02, -2.6243e-01,\n",
      "        -2.6288e-02, -2.4756e-01, -7.5068e-02,  4.9017e-01, -2.1801e-01,\n",
      "        -1.2747e-01, -3.4202e-02, -2.5892e-01, -3.5188e-01, -6.7000e-02,\n",
      "        -4.2172e-01,  1.4296e-01, -4.1184e-01, -9.4230e-04, -6.4984e-01,\n",
      "        -1.2443e-01, -2.2978e-02,  3.1080e-01, -5.3202e-01,  9.2308e-02,\n",
      "         5.1738e-01,  1.6357e-01,  4.1306e-02, -7.2864e-02, -1.8374e-01,\n",
      "        -9.1456e-02,  8.7034e-02, -5.2140e-01,  8.4393e-01, -2.6658e-01,\n",
      "         4.1124e-01,  2.2357e-01, -1.9583e-01,  3.4919e-01, -6.0420e-01,\n",
      "        -3.4275e-01,  1.7922e-01,  7.1702e-01, -6.5519e-02,  3.3226e-01,\n",
      "         1.9494e-01,  3.7074e-01,  4.8286e-01,  8.6542e-01,  1.9256e-01,\n",
      "         1.8797e-02,  8.1855e-01, -4.3893e-01,  2.1546e-01, -3.0848e-01,\n",
      "         2.5845e-01, -2.6430e-01,  2.7694e-01,  1.3315e-01,  9.5527e-01,\n",
      "         1.5193e-01, -1.4842e-01,  3.9241e-01,  3.5782e-01, -5.9792e-01,\n",
      "         1.1726e-01,  1.3109e-01, -3.5268e-01, -3.0282e-01,  4.1248e-02,\n",
      "         3.6656e-01, -1.8283e-02,  9.7255e-03, -2.4284e-01,  1.1514e-01,\n",
      "         1.3559e-01, -1.8612e-01,  3.1848e-01,  1.9259e-01, -3.0652e-01,\n",
      "        -5.2179e-01, -1.8819e-01, -1.0093e-01,  2.0414e-01, -3.2664e-01,\n",
      "        -3.6530e-01,  1.5873e-01, -5.5727e-01, -1.9239e-01,  4.5441e-02,\n",
      "        -1.7976e-01, -3.1875e-01, -7.1592e-01,  2.6829e-02,  2.9293e-01,\n",
      "         2.5407e-02, -9.0681e-02, -3.0221e-01,  1.2896e-01, -4.5249e-02,\n",
      "        -2.7396e-01,  2.4500e-02,  1.0130e-01,  1.0020e-01,  2.7701e-01,\n",
      "        -3.0467e-01, -1.0260e-01,  2.8172e-01,  2.3085e-02,  5.7836e-02,\n",
      "         6.0795e-01, -6.0961e-01, -2.4893e-02, -5.0576e-02,  5.5406e-02,\n",
      "         1.2906e-02, -2.0817e-01,  2.4534e-01, -2.2500e-01,  2.9426e-02,\n",
      "        -2.7480e-01,  7.4730e-01,  1.8980e-01,  2.5081e-01,  1.5851e-01,\n",
      "        -4.0235e-01, -1.6969e-01, -3.7136e-01,  7.6890e-02, -8.5676e-01,\n",
      "         1.2352e-01, -2.4541e-01,  1.3273e-01,  1.0323e-03, -1.5147e-01,\n",
      "         2.7888e-02, -2.3935e-02, -3.5993e-01, -3.2950e-01,  2.2789e-01,\n",
      "         2.5614e-02, -4.4394e-02, -1.2734e-01,  5.5925e-01,  2.4509e-01,\n",
      "        -3.9563e-01,  4.0246e-01,  8.5918e-02,  2.3930e-01,  5.5555e-02,\n",
      "         1.5976e-01,  3.1305e-01, -6.9706e-02, -2.4159e-01,  5.3633e-01,\n",
      "         2.5338e-02,  2.1870e-01,  7.3817e-02, -5.9284e-01,  3.9119e-01,\n",
      "        -2.6954e-01, -3.6437e-03,  4.2445e-01, -3.6671e-01, -2.2968e-01,\n",
      "         2.2761e-01,  2.0409e-01,  9.7983e-02,  3.0123e-01,  4.2016e-01,\n",
      "        -2.9227e-01,  1.8621e-01, -7.0845e-02,  5.2205e-02, -6.8265e-01,\n",
      "         2.2897e-01, -1.5892e-02,  2.4238e-01,  1.0389e+00,  6.2297e-01,\n",
      "         3.1147e-01, -3.1041e-01, -6.5775e-01,  8.6018e-02, -3.3829e-02,\n",
      "        -7.7501e-02, -1.6662e-01, -3.8203e-01,  5.9171e-02,  3.4376e-01,\n",
      "        -1.3236e-01,  3.9207e-01,  4.5186e-01, -2.7197e-01, -4.3461e-02,\n",
      "        -1.0711e-02,  1.9660e-01, -3.6862e-01,  3.2771e-02, -3.8254e-01,\n",
      "        -2.6337e-01,  7.3938e-02, -2.2048e-01, -3.3578e-01,  3.9128e-01,\n",
      "         5.8405e-01, -9.8725e-02,  6.2789e-01,  3.0186e-01,  2.6918e-01,\n",
      "        -6.3461e-01,  2.4154e-01,  4.7397e-01,  3.4264e-01,  6.6855e-02,\n",
      "        -1.0252e-01,  8.5944e-02, -1.4061e-01,  5.3254e-01,  5.5319e-01,\n",
      "        -3.1104e-03, -5.7683e-02, -9.4841e-02, -5.5374e-01, -1.0431e-01,\n",
      "        -6.4141e-01, -3.6587e-01,  3.1125e-02, -4.1912e-01, -8.1660e-02,\n",
      "         1.0081e-01,  4.0771e-02, -1.4054e-01, -1.4227e-01,  1.3890e-01,\n",
      "         3.7653e-01,  1.2723e-01, -4.6420e-01,  1.1746e-01,  7.3600e-02,\n",
      "         5.3363e-01, -6.5410e-01,  4.5491e-01,  3.2288e-01, -4.9433e-01,\n",
      "        -2.1253e-01,  2.3369e-01, -7.7339e-01,  5.9050e-01, -4.0324e-01,\n",
      "         3.8141e-01,  3.2117e-01, -9.5685e-01, -4.8296e-01,  5.1497e-01,\n",
      "        -2.8550e-02,  1.2886e-01, -2.1897e-02,  1.9964e-02,  5.5359e-02,\n",
      "        -4.9937e-01,  4.3293e-01,  2.4391e-01,  2.6040e-01, -4.2517e-02,\n",
      "         5.2210e-01,  4.4372e-01,  8.3466e-02])\n"
     ]
    }
   ],
   "source": [
    "print (f'{sentences[0]}  {embeddings[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, search for something!\n",
    "\n",
    "Let us find the closest match to the the query: \"a friendship with animals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.126169Z",
     "start_time": "2023-03-25T10:55:58.114669Z"
    }
   },
   "outputs": [],
   "source": [
    "query_text = \"kapil\"\n",
    "query = embedder.encode(query_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.134626Z",
     "start_time": "2023-03-25T10:55:58.127819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 16, 'score': 0.4423687756061554},\n",
       "  {'corpus_id': 19, 'score': 0.319680392742157},\n",
       "  {'corpus_id': 17, 'score': 0.21497485041618347}]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "search_results = util.semantic_search(query, embeddings, top_k = 3)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.139477Z",
     "start_time": "2023-03-25T10:55:58.136192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 0, Relevance score: 0.4423687756061554 \n",
      "\n",
      "Kapil Dev Ramlal Nikhanj (Pronunciation: [kəpiːl deːʋ] born 6 January 1959) is an Indian former cricketer. One of the greatest all-rounders in the history of cricket, he was a fast-medium bowler and a hard-hitting middle-order batsman. Dev is the only player in the history of cricket to have taken more than 400 wickets (434 wickets) and scored more than 5,000 runs in Test.[4]\n",
      "\n",
      "Dev captained the Indian cricket team that won the 1983 Cricket World Cup,[5] becoming the first Indian captain to win the Cricket World Cup. He is still the youngest captain (at the age of 24) to win the World Cup for any team.[6] He retired in 1994, as the first player to take 200 ODI wickets,[7] and holding the world record for the highest number of wickets taken in Test cricket, a record subsequently broken by Courtney Walsh in 2000.[8] Kapil Dev still holds the record for the highest individual score i.e. 175* scored by a batsman batting at number 5 or lower in ODIs.[9]\n",
      "\n",
      "After retiring, he coached the Indian national team between September 1999 and September 2000.[10][11]\n",
      "\n",
      "In 1982, Dev was awarded the Padma Shri, and in 1991 the Padma Bhushan. In 2002, he was named by Wisden as the Indian Cricketer of the Century. On 11 March 2010, Dev was inducted into the ICC Cricket Hall of Fame.[12] In 2013, he received the C. K. Nayudu Lifetime Achievement Award, the highest honour conferred by BCCI on a former player.[13]\n",
      "\n",
      "Early life\n",
      "Kapildev Ramlal Nikhanj[14] was born in a[15] family from Punjab to his father Ram Lal Nikhanj, a teak merchant and his wife, Rajkumari in Chandigarh on 6 January 1959. His family moved to Fazilka after the Partition before eventually moving to Chandigarh. His paternal family is from Montgomery (now known as Sahiwal) and his mother was born in Pakpattan, Okara.[16][17][18] Dev was a student at D.A.V.College.[19][20]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 1, Relevance score: 0.319680392742157 \n",
      "\n",
      "Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] i; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays for Royal Challengers Bangalore in the IPL and Delhi in domestic cricket. Considered to be one of the best cricketers in the world, he is widely regarded as one of the greatest batsmen in the history of the sport.[4] Nicknamed \"The King\", due to his dominant style of play and popularity, Kohli holds numerous records in his career across all formats. In 2020, the International Cricket Council named him the male cricketer of the decade. Kohli has also contributed to India's successes, captaining the team from 2014 to 2022, and winning the 2011 World Cup and the 2013 Champions trophy. He is among the only four Indian cricketers who have played over 500 matches for India.[5]\n",
      "\n",
      "Born and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team. He made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011. In 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time. During 2014 T20 World Cup, he set a record for the most runs scored in the tournament. In 2018, he achieved yet another milestone, becoming the world's top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game. His form continued in 2019, when he became the first player to score 20,000 international runs in a single decade. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\n",
      "\n",
      "He has received many accolades for his performances on the cricket field. He was recognized as the ICC ODI Player of the Year in 2012 and has won the Sir Garfield Sobers Trophy, given to the ICC Cricketer of the Year, on two occasions, in 2017 and 2018 respectively. Subsequently, Kohli also won ICC Test Player of the Year and ICC ODI Player of the Year awards in 2018, becoming the first player to win both awards in the same year. Additionally, he was named the Wisden Leading Cricketer in the World for three consecutive years, from 2016 to 2018. At the national level, Kohli was honoured with the Arjuna Award in 2013, the Padma Shri under the sports category in 2017 and the Khel Ratna award, India's highest sporting honour, in 2018.\n",
      "\n",
      "In 2016, he was ranked as one of the world's most famous athletes by ESPN, and one of the most valuable athlete brands by Forbes. In 2018, Time magazine included him on its list of the 100 most influential people in the world. In 2020, he was ranked 66th in Forbes list of the top 100 highest-paid athletes in the world for the year 2020 with estimated earnings of over $26 million. Kohli has been deemed one of the most commercially viable cricketers, with estimated earnings of ₹165 crore (US$21 million) in the year 2022.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 2, Relevance score: 0.21497485041618347 \n",
      "\n",
      "Sachin Ramesh Tendulkar, (/ˌsʌtʃɪn tɛnˈduːlkər/ i; pronounced [sətɕin teːɳɖulkəɾ]; born 24 April 1973) is an Indian former international cricketer who captained the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket.[4] He is the all-time highest run-scorer in both ODI and Test cricket with more than 18,000 runs and 15,000 runs, respectively.[5] He also holds the record for receiving the most man-of-the-match awards in international cricket.[6] Tendulkar was a Member of Parliament, Rajya Sabha by nomination from 2012 to 2018.[7][8]\n",
      "\n",
      "Tendulkar took up cricket at the age of eleven, made his Test match debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for over 24 years.[9] In 2002, halfway through his career, Wisden ranked him the second-greatest Test batsman of all time, behind Don Bradman, and the second-greatest ODI batsman of all time, behind Viv Richards.[10] The same year, Tendulkar was a part of the team that was one of the joint-winners of the 2002 ICC Champions Trophy. Later in his career, Tendulkar was part of the Indian team that won the 2011 Cricket World Cup, his first win in six World Cup appearances for India.[11] He had previously been named \"Player of the Tournament\" at the 2003 World Cup.\n",
      "\n",
      "Tendulkar has received several awards from the government of India: the Arjuna Award (1994), the Khel Ratna Award (1997), the Padma Shri (1998), and the Padma Vibhushan (2008).[12][13] After Tendulkar played his last match in November 2013, the Prime Minister's Office announced the decision to award him the Bharat Ratna, India's highest civilian award.[14][15] He was the first sportsperson to receive the reward and, as of 2023, is the youngest recipient.[16][17][18] In 2010, Time included Tendulkar in its annual list of the most influential people in the world.[19] Tendulkar was awarded the Sir Garfield Sobers Trophy for cricketer of the year at the 2010 International Cricket Council (ICC) Awards.[20]\n",
      "\n",
      "Having retired from ODI cricket in 2012,[21][22] he retired from all forms of cricket in November 2013 after playing his 200th Test match.[23] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[24] In 2013, Tendulkar was included in an all-time Test World XI to mark the 150th anniversary of Wisden Cricketers' Almanack, and he was the only specialist batsman of the post–World War II era, along with Viv Richards, to get featured in the team.[25] In 2019, he was inducted into the ICC Cricket Hall of Fame.[26] On 24 April 2023, the Sydney Cricket Ground unveiled a set of gates named after Tendulkar and Brian Lara on the occasion of Tendulkar's 50th birthday and the 30th anniversary of Lara's inning of 277 at the ground.[27][28][29]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, result in enumerate(search_results[0]):\n",
    "    print('-'*80)\n",
    "    print(f'Search Rank: {index}, Relevance score: {result[\"score\"]} ')\n",
    "    print(sentences[result['corpus_id']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 379, which is longer than the specified 256\n",
      "Created a chunk of size 581, which is longer than the specified 256\n",
      "Created a chunk of size 344, which is longer than the specified 256\n",
      "Created a chunk of size 588, which is longer than the specified 256\n",
      "Created a chunk of size 781, which is longer than the specified 256\n",
      "Created a chunk of size 710, which is longer than the specified 256\n",
      "Created a chunk of size 720, which is longer than the specified 256\n",
      "Created a chunk of size 312, which is longer than the specified 256\n",
      "Created a chunk of size 514, which is longer than the specified 256\n",
      "Created a chunk of size 863, which is longer than the specified 256\n",
      "Created a chunk of size 956, which is longer than the specified 256\n",
      "Created a chunk of size 740, which is longer than the specified 256\n",
      "Created a chunk of size 1491, which is longer than the specified 256\n",
      "Created a chunk of size 266, which is longer than the specified 256\n",
      "Created a chunk of size 671, which is longer than the specified 256\n",
      "Created a chunk of size 508, which is longer than the specified 256\n",
      "Created a chunk of size 1282, which is longer than the specified 256\n",
      "Created a chunk of size 437, which is longer than the specified 256\n",
      "Created a chunk of size 499, which is longer than the specified 256\n",
      "Created a chunk of size 1094, which is longer than the specified 256\n",
      "Created a chunk of size 445, which is longer than the specified 256\n",
      "Created a chunk of size 1422, which is longer than the specified 256\n",
      "Created a chunk of size 472, which is longer than the specified 256\n",
      "Created a chunk of size 609, which is longer than the specified 256\n",
      "Created a chunk of size 605, which is longer than the specified 256\n",
      "Created a chunk of size 298, which is longer than the specified 256\n",
      "Created a chunk of size 853, which is longer than the specified 256\n",
      "Created a chunk of size 902, which is longer than the specified 256\n",
      "Created a chunk of size 662, which is longer than the specified 256\n",
      "Created a chunk of size 622, which is longer than the specified 256\n",
      "Created a chunk of size 496, which is longer than the specified 256\n",
      "Created a chunk of size 1108, which is longer than the specified 256\n",
      "Created a chunk of size 828, which is longer than the specified 256\n",
      "Created a chunk of size 1465, which is longer than the specified 256\n",
      "Created a chunk of size 638, which is longer than the specified 256\n",
      "Created a chunk of size 325, which is longer than the specified 256\n",
      "Created a chunk of size 390, which is longer than the specified 256\n",
      "Created a chunk of size 949, which is longer than the specified 256\n",
      "Created a chunk of size 352, which is longer than the specified 256\n",
      "Created a chunk of size 521, which is longer than the specified 256\n",
      "Created a chunk of size 2168, which is longer than the specified 256\n",
      "Created a chunk of size 1499, which is longer than the specified 256\n",
      "Created a chunk of size 887, which is longer than the specified 256\n",
      "Created a chunk of size 417, which is longer than the specified 256\n",
      "Created a chunk of size 455, which is longer than the specified 256\n",
      "Created a chunk of size 1859, which is longer than the specified 256\n",
      "Created a chunk of size 571, which is longer than the specified 256\n",
      "Created a chunk of size 267, which is longer than the specified 256\n",
      "Created a chunk of size 984, which is longer than the specified 256\n",
      "Created a chunk of size 510, which is longer than the specified 256\n",
      "Created a chunk of size 533, which is longer than the specified 256\n",
      "Created a chunk of size 420, which is longer than the specified 256\n",
      "Created a chunk of size 617, which is longer than the specified 256\n",
      "Created a chunk of size 558, which is longer than the specified 256\n",
      "Created a chunk of size 876, which is longer than the specified 256\n",
      "Created a chunk of size 1138, which is longer than the specified 256\n",
      "Created a chunk of size 669, which is longer than the specified 256\n",
      "Created a chunk of size 268, which is longer than the specified 256\n",
      "Created a chunk of size 1321, which is longer than the specified 256\n",
      "Created a chunk of size 476, which is longer than the specified 256\n",
      "Created a chunk of size 961, which is longer than the specified 256\n",
      "Created a chunk of size 466, which is longer than the specified 256\n",
      "Created a chunk of size 897, which is longer than the specified 256\n",
      "Created a chunk of size 624, which is longer than the specified 256\n",
      "Created a chunk of size 657, which is longer than the specified 256\n",
      "Created a chunk of size 781, which is longer than the specified 256\n",
      "Created a chunk of size 546, which is longer than the specified 256\n",
      "Created a chunk of size 567, which is longer than the specified 256\n",
      "Created a chunk of size 819, which is longer than the specified 256\n",
      "Created a chunk of size 365, which is longer than the specified 256\n",
      "Created a chunk of size 835, which is longer than the specified 256\n",
      "Created a chunk of size 829, which is longer than the specified 256\n",
      "Created a chunk of size 395, which is longer than the specified 256\n",
      "Created a chunk of size 1477, which is longer than the specified 256\n",
      "Created a chunk of size 835, which is longer than the specified 256\n",
      "Created a chunk of size 1378, which is longer than the specified 256\n",
      "Created a chunk of size 862, which is longer than the specified 256\n",
      "Created a chunk of size 804, which is longer than the specified 256\n",
      "Created a chunk of size 768, which is longer than the specified 256\n",
      "Created a chunk of size 336, which is longer than the specified 256\n",
      "Created a chunk of size 1145, which is longer than the specified 256\n",
      "Created a chunk of size 265, which is longer than the specified 256\n",
      "Created a chunk of size 1006, which is longer than the specified 256\n",
      "Created a chunk of size 290, which is longer than the specified 256\n",
      "Created a chunk of size 1167, which is longer than the specified 256\n",
      "Created a chunk of size 489, which is longer than the specified 256\n",
      "Created a chunk of size 876, which is longer than the specified 256\n",
      "Created a chunk of size 382, which is longer than the specified 256\n",
      "Created a chunk of size 787, which is longer than the specified 256\n",
      "Created a chunk of size 293, which is longer than the specified 256\n",
      "Created a chunk of size 674, which is longer than the specified 256\n",
      "Created a chunk of size 641, which is longer than the specified 256\n",
      "Created a chunk of size 687, which is longer than the specified 256\n",
      "Created a chunk of size 964, which is longer than the specified 256\n",
      "Created a chunk of size 469, which is longer than the specified 256\n",
      "Created a chunk of size 850, which is longer than the specified 256\n",
      "Created a chunk of size 826, which is longer than the specified 256\n",
      "Created a chunk of size 838, which is longer than the specified 256\n",
      "Created a chunk of size 823, which is longer than the specified 256\n",
      "Created a chunk of size 321, which is longer than the specified 256\n",
      "Created a chunk of size 564, which is longer than the specified 256\n",
      "Created a chunk of size 406, which is longer than the specified 256\n",
      "Created a chunk of size 735, which is longer than the specified 256\n",
      "Created a chunk of size 524, which is longer than the specified 256\n",
      "Created a chunk of size 502, which is longer than the specified 256\n",
      "Created a chunk of size 360, which is longer than the specified 256\n",
      "Created a chunk of size 787, which is longer than the specified 256\n",
      "Created a chunk of size 401, which is longer than the specified 256\n",
      "Created a chunk of size 319, which is longer than the specified 256\n",
      "Created a chunk of size 298, which is longer than the specified 256\n",
      "Created a chunk of size 577, which is longer than the specified 256\n",
      "Created a chunk of size 932, which is longer than the specified 256\n",
      "Created a chunk of size 1096, which is longer than the specified 256\n",
      "Created a chunk of size 492, which is longer than the specified 256\n",
      "Created a chunk of size 759, which is longer than the specified 256\n",
      "Created a chunk of size 658, which is longer than the specified 256\n",
      "Created a chunk of size 737, which is longer than the specified 256\n",
      "Created a chunk of size 299, which is longer than the specified 256\n",
      "Created a chunk of size 879, which is longer than the specified 256\n",
      "Created a chunk of size 862, which is longer than the specified 256\n",
      "Created a chunk of size 626, which is longer than the specified 256\n",
      "Created a chunk of size 929, which is longer than the specified 256\n",
      "Created a chunk of size 414, which is longer than the specified 256\n",
      "Created a chunk of size 830, which is longer than the specified 256\n",
      "Created a chunk of size 1032, which is longer than the specified 256\n",
      "Created a chunk of size 522, which is longer than the specified 256\n",
      "Created a chunk of size 779, which is longer than the specified 256\n",
      "Created a chunk of size 606, which is longer than the specified 256\n",
      "Created a chunk of size 449, which is longer than the specified 256\n",
      "Created a chunk of size 338, which is longer than the specified 256\n",
      "Created a chunk of size 394, which is longer than the specified 256\n",
      "Created a chunk of size 594, which is longer than the specified 256\n",
      "Created a chunk of size 447, which is longer than the specified 256\n",
      "Created a chunk of size 268, which is longer than the specified 256\n",
      "Created a chunk of size 393, which is longer than the specified 256\n",
      "Created a chunk of size 297, which is longer than the specified 256\n",
      "Created a chunk of size 1010, which is longer than the specified 256\n",
      "Created a chunk of size 580, which is longer than the specified 256\n",
      "Created a chunk of size 493, which is longer than the specified 256\n",
      "Created a chunk of size 553, which is longer than the specified 256\n",
      "Created a chunk of size 823, which is longer than the specified 256\n",
      "Created a chunk of size 334, which is longer than the specified 256\n",
      "Created a chunk of size 364, which is longer than the specified 256\n",
      "Created a chunk of size 412, which is longer than the specified 256\n",
      "Created a chunk of size 640, which is longer than the specified 256\n",
      "Created a chunk of size 706, which is longer than the specified 256\n",
      "Created a chunk of size 300, which is longer than the specified 256\n",
      "Created a chunk of size 876, which is longer than the specified 256\n",
      "Created a chunk of size 598, which is longer than the specified 256\n",
      "Created a chunk of size 651, which is longer than the specified 256\n",
      "Created a chunk of size 409, which is longer than the specified 256\n",
      "Created a chunk of size 411, which is longer than the specified 256\n",
      "Created a chunk of size 396, which is longer than the specified 256\n",
      "Created a chunk of size 608, which is longer than the specified 256\n",
      "Created a chunk of size 491, which is longer than the specified 256\n",
      "Created a chunk of size 591, which is longer than the specified 256\n",
      "Created a chunk of size 756, which is longer than the specified 256\n",
      "Created a chunk of size 604, which is longer than the specified 256\n",
      "Created a chunk of size 442, which is longer than the specified 256\n",
      "Created a chunk of size 364, which is longer than the specified 256\n",
      "Created a chunk of size 912, which is longer than the specified 256\n",
      "Created a chunk of size 645, which is longer than the specified 256\n",
      "Created a chunk of size 1466, which is longer than the specified 256\n",
      "Created a chunk of size 740, which is longer than the specified 256\n",
      "Created a chunk of size 618, which is longer than the specified 256\n",
      "Created a chunk of size 1096, which is longer than the specified 256\n",
      "Created a chunk of size 1173, which is longer than the specified 256\n",
      "Created a chunk of size 1044, which is longer than the specified 256\n",
      "Created a chunk of size 727, which is longer than the specified 256\n",
      "Created a chunk of size 910, which is longer than the specified 256\n",
      "Created a chunk of size 275, which is longer than the specified 256\n",
      "Created a chunk of size 276, which is longer than the specified 256\n",
      "Created a chunk of size 279, which is longer than the specified 256\n",
      "Created a chunk of size 261, which is longer than the specified 256\n",
      "Created a chunk of size 261, which is longer than the specified 256\n",
      "Created a chunk of size 261, which is longer than the specified 256\n",
      "Created a chunk of size 258, which is longer than the specified 256\n",
      "Created a chunk of size 264, which is longer than the specified 256\n",
      "Created a chunk of size 294, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 265, which is longer than the specified 256\n",
      "Created a chunk of size 274, which is longer than the specified 256\n",
      "Created a chunk of size 324, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 278, which is longer than the specified 256\n",
      "Created a chunk of size 261, which is longer than the specified 256\n",
      "Created a chunk of size 648, which is longer than the specified 256\n",
      "Created a chunk of size 275, which is longer than the specified 256\n",
      "Created a chunk of size 1221, which is longer than the specified 256\n",
      "Created a chunk of size 492, which is longer than the specified 256\n",
      "Created a chunk of size 378, which is longer than the specified 256\n",
      "Created a chunk of size 354, which is longer than the specified 256\n",
      "Created a chunk of size 377, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 292, which is longer than the specified 256\n",
      "Created a chunk of size 284, which is longer than the specified 256\n",
      "Created a chunk of size 983, which is longer than the specified 256\n",
      "Created a chunk of size 393, which is longer than the specified 256\n",
      "Created a chunk of size 600, which is longer than the specified 256\n",
      "Created a chunk of size 504, which is longer than the specified 256\n",
      "Created a chunk of size 545, which is longer than the specified 256\n",
      "Created a chunk of size 416, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 422, which is longer than the specified 256\n",
      "Created a chunk of size 1241, which is longer than the specified 256\n",
      "Created a chunk of size 1105, which is longer than the specified 256\n",
      "Created a chunk of size 283, which is longer than the specified 256\n",
      "Created a chunk of size 566, which is longer than the specified 256\n",
      "Created a chunk of size 289, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 567, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 567, which is longer than the specified 256\n",
      "Created a chunk of size 559, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 709, which is longer than the specified 256\n",
      "Created a chunk of size 277, which is longer than the specified 256\n",
      "Created a chunk of size 277, which is longer than the specified 256\n",
      "Created a chunk of size 435, which is longer than the specified 256\n",
      "Created a chunk of size 283, which is longer than the specified 256\n",
      "Created a chunk of size 427, which is longer than the specified 256\n",
      "Created a chunk of size 422, which is longer than the specified 256\n",
      "Created a chunk of size 307, which is longer than the specified 256\n",
      "Created a chunk of size 860, which is longer than the specified 256\n",
      "Created a chunk of size 711, which is longer than the specified 256\n",
      "Created a chunk of size 666, which is longer than the specified 256\n",
      "Created a chunk of size 556, which is longer than the specified 256\n",
      "Created a chunk of size 1098, which is longer than the specified 256\n",
      "Created a chunk of size 606, which is longer than the specified 256\n",
      "Created a chunk of size 415, which is longer than the specified 256\n",
      "Created a chunk of size 311, which is longer than the specified 256\n",
      "Created a chunk of size 1227, which is longer than the specified 256\n",
      "Created a chunk of size 698, which is longer than the specified 256\n",
      "Created a chunk of size 425, which is longer than the specified 256\n",
      "Created a chunk of size 1091, which is longer than the specified 256\n",
      "Created a chunk of size 1332, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 298, which is longer than the specified 256\n",
      "Created a chunk of size 537, which is longer than the specified 256\n",
      "Created a chunk of size 462, which is longer than the specified 256\n",
      "Created a chunk of size 599, which is longer than the specified 256\n",
      "Created a chunk of size 535, which is longer than the specified 256\n",
      "Created a chunk of size 664, which is longer than the specified 256\n",
      "Created a chunk of size 277, which is longer than the specified 256\n",
      "Created a chunk of size 1020, which is longer than the specified 256\n",
      "Created a chunk of size 421, which is longer than the specified 256\n",
      "Created a chunk of size 266, which is longer than the specified 256\n",
      "Created a chunk of size 828, which is longer than the specified 256\n",
      "Created a chunk of size 825, which is longer than the specified 256\n",
      "Created a chunk of size 262, which is longer than the specified 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing chunks\n",
      "\n",
      "['’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.', '“Beware the Jabberwock, my son!\\n      The jaws that bite, the claws that catch!\\nBeware the Jubjub bird, and shun\\n      The frumious Bandersnatch!”', 'He took his vorpal sword in hand;\\n      Long time the manxome foe he sought—\\nSo rested he by the Tumtum tree\\n      And stood awhile in thought.', 'And, as in uffish thought he stood,\\n      The Jabberwock, with eyes of flame,\\nCame whiffling through the tulgey wood,\\n      And burbled as it came!', 'One, two! One, two! And through and through\\n      The vorpal blade went snicker-snack!\\nHe left it dead, and with its head\\n      He went galumphing back.', '“And hast thou slain the Jabberwock?\\n      Come to my arms, my beamish boy!\\nO frabjous day! Callooh! Callay!”\\n      He chortled in his joy.', '’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.', 'It was the best of times, it was the worst of times, \\nit was the age of wisdom, it was the age of foolishness, \\nit was the epoch of belief, it was the epoch of incredulity, \\nit was the season of light, it was the season of darkness, \\nit was the spring of hope, it was the winter of despair, \\nwe had everything before us, we had nothing before us, \\nwe were all going direct to heaven, \\nwe were all going direct the other way–in short, \\nthe period was so far like the present period, \\nthat some of its noisiest authorities insisted on its being received, \\nfor good or for evil, in the superlative degree of comparison only.', 'I wish you to know that you have been the last dream of my soul. \\nIn my degradation I have not been so degraded but that the sight \\nof you with your father, and of this home made such a home by you, \\nhas stirred old shadows that I thought had died out of me. \\nSince I knew you, I have been troubled by a remorse that I \\nthought would never reproach me again, and have heard whispers \\nfrom old voices impelling me upward, that I thought were silent \\nfor ever. I have had unformed ideas of striving afresh, beginning anew, \\nshaking off sloth and sensuality, and fighting out the abandoned fight. \\nA dream, all a dream, that ends in nothing, and leaves the sleeper \\nwhere he lay down, but I wish you to know that you inspired it.', 'A wonderful fact to reflect upon, that every human creature is \\nconstituted to be that profound secret and mystery to every other.', \"The dog is a gentleman; I hope to go to his heaven not man's.\", 'If a man aspires towards a righteous life, his first act of abstinence is from injury to animals.', 'Tweedledum and Tweedledee: She then meets the fat twin brothers \\nTweedledum and Tweedledee, whom she knows from the nursery rhyme. \\nAfter reciting the long poem \"The Walrus and the Carpenter\", \\nthey draw Alice\\'s attention to the Red King—loudly snoring away \\nunder a nearby tree—and maliciously provoke her with idle philosophical \\nbanter that she exists only as an imaginary figure in the Red King\\'s dreams. \\nFinally, the brothers begin suiting up for battle, only to be frightened \\naway by an enormous crow, as the nursery rhyme about them predicts.', 'Golden retrievers are not bred to be guard dogs, and considering the size of their hearts and their irrepressible joy and life, they are less likely to bite than to bark, less likely to bark than to lick a hand in greeting. In spite of their size, they think they are lap dogs, and in spite of being dogs, they think they’re also human, and nearly every human they meet is judged to have the potential to be a boon companion who might at any moment, cry, “Let’s go!” and lead them on a great adventure.', 'If you’re lucky, a golden retriever will come into your life, steal your heart, and change everything', 'My friend Phil has a theory that the Lord, having made teenagers, felt constrained to make amends and so created the golden retriever.', 'If you don’t believe that dogs have souls, you haven’t looked into their eyes long enough.', \"A thing of beauty is a joy for ever:\\nIts loveliness increases; it will never\\nPass into nothingness; but still will keep\\nA bower quiet for us, and a sleep\\nFull of sweet dreams, and health, and quiet breathing.\\nTherefore, on every morrow, are we wreathing\\nA flowery band to bind us to the earth,\\nSpite of despondence, of the inhuman dearth\\nOf noble natures, of the gloomy days,\\nOf all the unhealthy and o'er-darkn'd ways\\nMade for our searching: yes, in spite of all,\\nSome shape of beauty moves away the pall\\nFrom our dark spirits. Such the sun, the moon,\\nTrees old and young, sprouting a shady boon\\nFor simple sheep; and such are daffodils\\nWith the green world they live in; and clear rills\\nThat for themselves a cooling covert make\\n'Gainst the hot season; the mid-forest brake,\\nRich with a sprinkling of fair musk-rose blooms:\\nAnd such too is the grandeur of the dooms\\nWe have imagined for the mighty dead;\\nAn endless fountain of immortal drink,\\nPouring unto us from the heaven's brink\", 'The dominant sequence transduction models are based on \\ncomplex recurrent or convolutional neural networks in an encoder-decoder configuration. \\nThe best performing models also connect the encoder and decoder through \\nan attention mechanism. We propose a new simple network architecture, \\nthe Transformer, based solely on attention mechanisms, \\ndispensing with recurrence and convolutions entirely. \\nExperiments on two machine translation tasks show these models \\nto be superior in quality while being more parallelizable \\nand requiring significantly less time to train. \\nOur model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, \\nimproving over the existing best results, including ensembles by over 2 BLEU. \\nOn the WMT 2014 English-to-French translation task, our model establishes \\na new single-model state-of-the-art BLEU score of 41.8 after training for \\n3.5 days on eight GPUs, a small fraction of the training costs of the \\nbest models from the literature. We show that the Transformer \\ngeneralizes well to other tasks by applying it successfully to \\nEnglish constituency parsing both with large and limited training data.', 'In machine learning, backpropagation\\n(backprop,[1] BP) is a widely used\\nalgorithm for training feedforward\\nartificial neural networks.\\nGeneralizations of backpropagation\\nexist for other artificial neural\\nnetworks (ANNs), and for functions\\ngenerally. These classes of algorithms\\nare all referred to generically as\\n\"backpropagation\".[2] In fitting a\\nneural network, backpropagation\\ncomputes the gradient of the loss\\nfunction with respect to the weights of\\nthe network for a single input–output\\nexample, and does so efficiently,\\nunlike a naive direct computation of\\nthe gradient with respect to each\\nweight individually. This efficiency\\nmakes it feasible to use gradient\\nmethods for training multilayer\\nnetworks, updating weights to minimize\\nloss; gradient descent, or variants\\nsuch as stochastic gradient descent,\\nare commonly used. The backpropagation\\nalgorithm works by computing the\\ngradient of the loss function with\\nrespect to each weight by the chain\\nrule, computing the gradient one layer\\nat a time, iterating backward from the\\nlast layer to avoid redundant\\ncalculations of intermediate terms in\\nthe chain rule; this is an example of\\ndynamic programming.[3]', 'She dwelt among the untrodden ways\\nBeside the springs of Dove,\\nA Maid whom there were none to praise\\nAnd very few to love:\\n\\nA violet by a mossy stone\\nHalf hidden from the eye!\\n—Fair as a star, when only one\\nIs shining in the sky.', 'She lived unknown, and few could know\\nWhen Lucy ceased to be;\\nBut she is in her grave, and, oh,\\nThe difference to me!', 'What is this life if, full of care,\\nWe have no time to stand and stare.\\n\\nNo time to stand beneath the boughs\\nAnd stare as long as sheep or cows.\\n\\nNo time to see, when woods we pass,\\nWhere squirrels hide their nuts in grass.', \"No time to see, in broad daylight,\\nStreams full of stars, like skies at night.\\n\\nNo time to turn at Beauty's glance,\\nAnd watch her feet, how they can dance.\\n\\nNo time to wait till her mouth can\\nEnrich that smile her eyes began.\", 'A poor life this if, full of care,\\nWe have no time to stand and stare.', 'Kapil Dev Ramlal Nikhanj (Pronunciation: [kəpiːl deːʋ] born 6 January 1959) is an Indian former cricketer. One of the greatest all-rounders in the history of cricket, he was a fast-medium bowler and a hard-hitting middle-order batsman. Dev is the only player in the history of cricket to have taken more than 400 wickets (434 wickets) and scored more than 5,000 runs in Test.[4]', 'Dev captained the Indian cricket team that won the 1983 Cricket World Cup,[5] becoming the first Indian captain to win the Cricket World Cup. He is still the youngest captain (at the age of 24) to win the World Cup for any team.[6] He retired in 1994, as the first player to take 200 ODI wickets,[7] and holding the world record for the highest number of wickets taken in Test cricket, a record subsequently broken by Courtney Walsh in 2000.[8] Kapil Dev still holds the record for the highest individual score i.e. 175* scored by a batsman batting at number 5 or lower in ODIs.[9]', 'After retiring, he coached the Indian national team between September 1999 and September 2000.[10][11]', 'In 1982, Dev was awarded the Padma Shri, and in 1991 the Padma Bhushan. In 2002, he was named by Wisden as the Indian Cricketer of the Century. On 11 March 2010, Dev was inducted into the ICC Cricket Hall of Fame.[12] In 2013, he received the C. K. Nayudu Lifetime Achievement Award, the highest honour conferred by BCCI on a former player.[13]', 'Early life\\nKapildev Ramlal Nikhanj[14] was born in a[15] family from Punjab to his father Ram Lal Nikhanj, a teak merchant and his wife, Rajkumari in Chandigarh on 6 January 1959. His family moved to Fazilka after the Partition before eventually moving to Chandigarh. His paternal family is from Montgomery (now known as Sahiwal) and his mother was born in Pakpattan, Okara.[16][17][18] Dev was a student at D.A.V.College.[19][20]', 'Sachin Ramesh Tendulkar, (/ˌsʌtʃɪn tɛnˈduːlkər/ i; pronounced [sətɕin teːɳɖulkəɾ]; born 24 April 1973) is an Indian former international cricketer who captained the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket.[4] He is the all-time highest run-scorer in both ODI and Test cricket with more than 18,000 runs and 15,000 runs, respectively.[5] He also holds the record for receiving the most man-of-the-match awards in international cricket.[6] Tendulkar was a Member of Parliament, Rajya Sabha by nomination from 2012 to 2018.[7][8]', 'Tendulkar took up cricket at the age of eleven, made his Test match debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for over 24 years.[9] In 2002, halfway through his career, Wisden ranked him the second-greatest Test batsman of all time, behind Don Bradman, and the second-greatest ODI batsman of all time, behind Viv Richards.[10] The same year, Tendulkar was a part of the team that was one of the joint-winners of the 2002 ICC Champions Trophy. Later in his career, Tendulkar was part of the Indian team that won the 2011 Cricket World Cup, his first win in six World Cup appearances for India.[11] He had previously been named \"Player of the Tournament\" at the 2003 World Cup.', \"Tendulkar has received several awards from the government of India: the Arjuna Award (1994), the Khel Ratna Award (1997), the Padma Shri (1998), and the Padma Vibhushan (2008).[12][13] After Tendulkar played his last match in November 2013, the Prime Minister's Office announced the decision to award him the Bharat Ratna, India's highest civilian award.[14][15] He was the first sportsperson to receive the reward and, as of 2023, is the youngest recipient.[16][17][18] In 2010, Time included Tendulkar in its annual list of the most influential people in the world.[19] Tendulkar was awarded the Sir Garfield Sobers Trophy for cricketer of the year at the 2010 International Cricket Council (ICC) Awards.[20]\", \"Having retired from ODI cricket in 2012,[21][22] he retired from all forms of cricket in November 2013 after playing his 200th Test match.[23] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[24] In 2013, Tendulkar was included in an all-time Test World XI to mark the 150th anniversary of Wisden Cricketers' Almanack, and he was the only specialist batsman of the post–World War II era, along with Viv Richards, to get featured in the team.[25] In 2019, he was inducted into the ICC Cricket Hall of Fame.[26] On 24 April 2023, the Sydney Cricket Ground unveiled a set of gates named after Tendulkar and Brian Lara on the occasion of Tendulkar's 50th birthday and the 30th anniversary of Lara's inning of 277 at the ground.[27][28][29]\", 'Shubman Gill (born 8 September 1999) is an Indian cricketer. Representing Indian cricket team at the international level, he also plays for Gujarat Titans in the Indian Premier League and Punjab in domestic cricket. Gill served as the vice-captain of the Indian Under-19 cricket team in the 2018 Under-19 Cricket World Cup and won Player of the Tournament award. A right-handed opening batsman, he is considered one of the best young cricketers in the world. He is nicknamed \"The Prince\" for the success he achieved in his career still now.[1] Gill holds the record for youngest cricketer to score a double century in One Day International cricket.[2] and for the highest T20 score by an individual for the Indian team.', 'He made his List-A debut against Vidharbha[3] in 2017 and first-class debut for Punjab against Bengal in the 2017–18 Ranji Trophy, in late 2017, with a half-century in the game,[4] and 129 runs in the LAST match against Services.[5] He made his international debut for the Indian cricket team in January 2019.[6]', \"He was drafted into India's Under-19 side as the vice-captain for the 2018 Under-19 Cricket World Cup. Shubman scored 372 runs at an average of 124.00 at the tournament, where he batted at number three to play a crucial role in India's record fourth world title and was adjudged the edition's Player of the Tournament.[7] His match-winning 102 not out in the semi-final against arch-rivals Pakistan U-19 drew praises from batting greats such as Rahul Dravid, Sachin Tendulkar, VVS Laxman, and Sourav Ganguly.[8][9]\", 'In 2022, Gill was part of the IPL championship winning Gujarat Titans team. Gill would go on to win the 2023 IPL Orange Cap, scoring 890 runs, the second highest total in IPL history, including three centuries. Gill scored 129 in the IPL Qualifies 2, the highest individual score in IPL Playoffs history. In 2023, Gill made his acting debut as the voice of Pavitr \"Pav\" Prabhakar / Spider-Man India in the Hindi and Punjabi dubs of the Sony Pictures Animation film Spider-Man: Across the Spider-Verse.', 'Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] i; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays for Royal Challengers Bangalore in the IPL and Delhi in domestic cricket. Considered to be one of the best cricketers in the world, he is widely regarded as one of the greatest batsmen in the history of the sport.[4] Nicknamed \"The King\", due to his dominant style of play and popularity, Kohli holds numerous records in his career across all formats. In 2020, the International Cricket Council named him the male cricketer of the decade. Kohli has also contributed to India\\'s successes, captaining the team from 2014 to 2022, and winning the 2011 World Cup and the 2013 Champions trophy. He is among the only four Indian cricketers who have played over 500 matches for India.[5]', \"Born and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team. He made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011. In 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time. During 2014 T20 World Cup, he set a record for the most runs scored in the tournament. In 2018, he achieved yet another milestone, becoming the world's top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game. His form continued in 2019, when he became the first player to score 20,000 international runs in a single decade. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\", \"He has received many accolades for his performances on the cricket field. He was recognized as the ICC ODI Player of the Year in 2012 and has won the Sir Garfield Sobers Trophy, given to the ICC Cricketer of the Year, on two occasions, in 2017 and 2018 respectively. Subsequently, Kohli also won ICC Test Player of the Year and ICC ODI Player of the Year awards in 2018, becoming the first player to win both awards in the same year. Additionally, he was named the Wisden Leading Cricketer in the World for three consecutive years, from 2016 to 2018. At the national level, Kohli was honoured with the Arjuna Award in 2013, the Padma Shri under the sports category in 2017 and the Khel Ratna award, India's highest sporting honour, in 2018.\", \"In 2016, he was ranked as one of the world's most famous athletes by ESPN, and one of the most valuable athlete brands by Forbes. In 2018, Time magazine included him on its list of the 100 most influential people in the world. In 2020, he was ranked 66th in Forbes list of the top 100 highest-paid athletes in the world for the year 2020 with estimated earnings of over $26 million. Kohli has been deemed one of the most commercially viable cricketers, with estimated earnings of ₹165 crore (US$21 million) in the year 2022.\", 'Learning Transferable Visual Models From Natural Language Supervision\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision\\n\\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1', 'Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1', 'Abstract\\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw\\ntext about images is a promising alternative which\\nleverages a much broader source of supervision.\\nWe demonstrate that the simple pre-training task\\nof predicting which caption goes with which im-\\nage is an efficient and scalable way to learn SOTA\\nimage representations from scratch on a dataset\\nof 400 million (image, text) pairs collected from\\nthe internet. After pre-training, natural language\\nis used to reference learned visual concepts (or\\ndescribe new ones) enabling zero-shot transfer\\nof the model to downstream tasks. We study\\nthe performance of this approach by benchmark-\\ning on over 30 different existing computer vi-\\nsion datasets, spanning tasks such as OCR, ac-\\ntion recognition in videos, geo-localization, and\\nmany types of fine-grained object classification.\\nThe model transfers non-trivially to most tasks\\nand is often competitive with a fully supervised\\nbaseline without the need for any dataset spe-\\ncific training. For instance, we match the ac-\\ncuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28\\nmillion training examples it was trained on. We\\nrelease our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.', '1. Introduction and Motivating Work\\nPre-training methods which learn directly from raw text\\nhave revolutionized NLP over the last few years (Dai &\\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).', '*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\\nCorrespondence to: <{alec, jongwook}@openai.com>.', 'Task-agnostic objectives such as autoregressive and masked\\nlanguage modeling have scaled across many orders of mag-\\nnitude in compute, model capacity, and data, steadily im-\\nproving capabilities. The development of “text-to-text” as\\na standardized input-output interface (McCann et al., 2018;\\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\\nagnostic architectures to zero-shot transfer to downstream\\ndatasets removing the need for specialized output heads or\\ndataset specific customization. Flagship systems like GPT-3\\n(Brown et al., 2020) are now competitive across many tasks\\nwith bespoke models while requiring little to no dataset\\nspecific training data.', 'These results suggest that the aggregate supervision acces-\\nsible to modern pre-training methods within web-scale col-\\nlections of text surpasses that of high-quality crowd-labeled\\nNLP datasets. However, in other fields such as computer\\nvision it is still standard practice to pre-train models on\\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\\nCould scalable pre-training methods which learn directly\\nfrom web text result in a similar breakthrough in computer\\nvision? Prior work is encouraging.', 'Over 20 years ago Mori et al. (1999) explored improving\\ncontent based image retrieval by training a model to pre-\\ndict the nouns and adjectives in text documents paired with\\nimages. Quattoni et al. (2007) demonstrated it was possi-\\nble to learn more data efficient image representations via\\nmanifold learning in the weight space of classifiers trained\\nto predict words in captions associated with images. Sri-\\nvastava & Salakhutdinov (2012) explored deep represen-\\ntation learning by training multimodal Deep Boltzmann\\nMachines on top of low-level image and text tag features.\\nJoulin et al. (2016) modernized this line of work and demon-\\nstrated that CNNs trained to predict words in image cap-\\ntions learn useful image representations. They converted\\nthe title, description, and hashtag metadata of images in the\\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\\nwords multi-label classification task and showed that pre-\\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\\nlabels learned representations which preformed similarly\\nto ImageNet-based pre-training on transfer tasks. Li et al.\\n(2017) then extended this approach to predicting phrase n-\\ngrams in addition to individual words and demonstrated the\\nability of their system to zero-shot transfer to other image', 'ar\\nX\\n\\niv\\n:2\\n\\n10\\n3.\\n\\n00\\n02\\n\\n0v\\n1 \\n\\n [\\ncs\\n\\n.C\\nV\\n\\n] \\n 2\\n\\n6 \\nFe\\n\\nb \\n20\\n\\n21\\n\\nhttps://github.com/OpenAI/CLIP\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 2\\n\\nI1·T2 I1·T3 …\\n\\nI2·T1 I2·T3 …\\n\\nI3·T1 I3·T2 …\\n\\n⋮ ⋮ ⋮\\n\\nI1·T1\\n\\nI2·T2\\n\\nI3·T3', 'I1·T1\\n\\nI2·T2\\n\\nI3·T3\\n\\n(1) Contrastive pre-training\\n\\nImage\\nEncoder\\n\\nText\\nEncoderPepper\\tthe\\n\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nT1 T2 T3 …\\n\\nI1\\n\\nI2\\n\\nI3\\n\\n⋮\\n\\n(2) Create dataset classifier from label text\\n\\nplane\\n\\ncar', 'plane\\n\\ncar\\n\\ndog\\n\\n⋮\\n\\nbird\\n\\nA\\tphoto\\tof\\na\\t{object}.\\n\\n⋮\\n\\nText\\nEncoder\\n\\nT1 T2 T3 TN\\n\\n…\\n\\n(3) Use for zero-shot prediction\\n\\nImage\\nEncoder\\n\\nI1 I1·T2 I1·TNI1·T1\\n\\n…\\n\\n…\\n\\nA\\tphoto\\tof\\n\\ta\\tdog.\\n\\nTN\\n\\nIN·T1 IN·T2 IN·T3\\n\\nI1·TN\\n\\nI2·TN\\n\\nI3·TN\\n\\n⋮\\n\\n…IN\\n\\n…\\n\\n⋮ ⋱\\n\\nIN·TN\\n\\nI1·T3', 'Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\\ntarget dataset’s classes.', 'classification datasets by scoring target classes based on\\ntheir dictionary of learned visual n-grams and predicting the\\none with the highest score. Adopting more recent architec-\\ntures and pre-training approaches, VirTex (Desai & Johnson,\\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\\nVIRT (Zhang et al., 2020) have recently demonstrated the\\npotential of transformer-based language modeling, masked\\nlanguage modeling, and contrastive objectives to learn im-\\nage representations from text.', 'While exciting as proofs of concept, using natural language\\nsupervision for image representation learning is still rare.\\nThis is likely because demonstrated performance on com-\\nmon benchmarks is much lower than alternative approaches.\\nFor example, Li et al. (2017) reach only 11.5% accuracy\\non ImageNet in a zero-shot setting. This is well below the\\n88.4% accuracy of the current state of the art (Xie et al.,\\n2020). It is even below the 50% accuracy of classic com-\\nputer vision approaches (Deng et al., 2012). Instead, more\\nnarrowly scoped but well-targeted uses of weak supervision\\nhave improved performance. Mahajan et al. (2018) showed\\nthat predicting ImageNet-related hashtags on Instagram im-\\nages is an effective pre-training task. When fine-tuned to\\nImageNet these pre-trained models increased accuracy by\\nover 5% and improved the overall state of the art at the time.\\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\\nalso demonstrated large gains on a broader set of transfer\\nbenchmarks by pre-training models to predict the classes of\\nthe noisily labeled JFT-300M dataset.', 'This line of work represents the current pragmatic middle\\nground between learning from a limited amount of super-\\nvised “gold-labels” and learning from practically unlimited\\namounts of raw text. However, it is not without compro-', 'mises. Both works carefully design, and in the process limit,\\ntheir supervision to 1000 and 18291 classes respectively.\\nNatural language is able to express, and therefore supervise,\\na much wider set of visual concepts through its general-\\nity. Both approaches also use static softmax classifiers to\\nperform prediction and lack a mechanism for dynamic out-\\nputs. This severely curtails their flexibility and limits their\\n“zero-shot” capabilities.', 'A crucial difference between these weakly supervised mod-\\nels and recent explorations of learning image representations\\ndirectly from natural language is scale. While Mahajan et al.\\n(2018) and Kolesnikov et al. (2019) trained their models for\\naccelerator years on millions to billions of images, VirTex,\\nICMLM, and ConVIRT trained for accelerator days on one\\nto two hundred thousand images. In this work, we close\\nthis gap and study the behaviors of image classifiers trained\\nwith natural language supervision at large scale. Enabled\\nby the large amounts of publicly available data of this form\\non the internet, we create a new dataset of 400 million (im-\\nage, text) pairs and demonstrate that a simplified version of\\nConVIRT trained from scratch, which we call CLIP, for Con-\\ntrastive Language-Image Pre-training, is an efficient method\\nof learning from natural language supervision. We study\\nthe scalability of CLIP by training a series of eight models\\nspanning almost 2 orders of magnitude of compute and ob-\\nserve that transfer performance is a smoothly predictable\\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\\n2020). We find that CLIP, similar to the GPT family, learns\\nto perform a wide set of tasks during pre-training including\\nOCR, geo-localization, action recognition, and many others.\\nWe measure this by benchmarking the zero-shot transfer\\nperformance of CLIP on over 30 existing datasets and find', 'Learning Transferable Visual Models From Natural Language Supervision 3\\n\\n2M 33M 67M 134M 268M 400M\\n# of images processed\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt I\\nm\\n\\nag\\neN\\n\\net\\n A\\n\\ncc\\nur\\n\\nac\\ny\\n\\n3X efficiency4X efficiency', 'Bag of Words Contrastive (CLIP)\\nBag of Words Prediction\\nTransformer Language Model', 'Figure 2. CLIP is much more efficient at zero-shot transfer\\nthan our image caption baseline. Although highly expressive,\\nwe found that transformer-based language models are relatively\\nweak at zero-shot ImageNet classification. Here, we see that it\\nlearns 3x slower than a baseline which predicts a bag-of-words\\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\\nprediction objective for the contrastive objective of CLIP further\\nimproves efficiency another 4x.', 'it can be competitive with prior task-specific supervised\\nmodels. We also confirm these findings with linear-probe\\nrepresentation learning analysis and show that CLIP out-\\nperforms the best publicly available ImageNet model while\\nalso being more computationally efficient. We additionally\\nfind that zero-shot CLIP models are much more robust than\\nequivalent accuracy supervised ImageNet models which\\nsuggests that zero-shot evaluation of task-agnostic models is\\nmuch more representative of a model’s capability. These re-\\nsults have significant policy and ethical implications, which\\nwe consider in Section 7.', '2. Approach\\n2.1. Natural Language Supervision', 'At the core of our approach is the idea of learning percep-\\ntion from supervision contained in natural language. As\\ndiscussed in the introduction, this is not at all a new idea,\\nhowever terminology used to describe work in this space\\nis varied, even seemingly contradictory, and stated motiva-\\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\\nduce methods which learn visual representations from text\\npaired with images but describe their approaches as unsuper-\\nvised, self-supervised, weakly supervised, and supervised\\nrespectively.', 'We emphasize that what is common across this line of work\\nis not any of the details of the particular methods used but\\nthe appreciation of natural language as a training signal. All\\nthese approaches are learning from natural language super-', 'vision. Although early work wrestled with the complexity\\nof natural language when using topic model and n-gram\\nrepresentations, improvements in deep contextual represen-\\ntation learning suggest we now have the tools to effectively\\nleverage this abundant source of supervision (McCann et al.,\\n2017).', 'Learning from natural language has several potential\\nstrengths over other training methods. It’s much easier\\nto scale natural language supervision compared to standard\\ncrowd-sourced labeling for image classification since it does\\nnot require annotations to be in a classic “machine learning\\ncompatible format” such as the canonical 1-of-N majority\\nvote “gold label”. Instead, methods which work on natural\\nlanguage can learn passively from the supervision contained\\nin the vast amount of text on the internet. Learning from\\nnatural language also has an important advantage over most\\nunsupervised or self-supervised learning approaches in that\\nit doesn’t “just” learn a representation but also connects that\\nrepresentation to language which enables flexible zero-shot\\ntransfer. In the following subsections, we detail the specific\\napproach we settled on.', '2.2. Creating a Sufficiently Large Dataset', 'Existing work has mainly used three datasets, MS-COCO\\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\\nYFCC100M (Thomee et al., 2016). While MS-COCO and\\nVisual Genome are high quality crowd-labeled datasets, they\\nare small by modern standards with approximately 100,000\\ntraining photos each. By comparison, other computer vision\\nsystems are trained on up to 3.5 billion Instagram photos\\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\\nis a possible alternative, but the metadata for each image is\\nsparse and of varying quality. Many images use automati-\\ncally generated filenames like 20160716 113957.JPG\\nas “titles” or contain “descriptions” of camera exposure\\nsettings. After filtering to keep only images with natural\\nlanguage titles and/or descriptions in English, the dataset\\nshrunk by a factor of 6 to only 15 million photos. This is\\napproximately the same size as ImageNet.', 'A major motivation for natural language supervision is the\\nlarge quantities of data of this form available publicly on the\\ninternet. Since existing datasets do not adequately reflect\\nthis possibility, considering results only on them would un-\\nderestimate the potential of this line of research. To address\\nthis, we constructed a new dataset of 400 million (image,\\ntext) pairs collected form a variety of publicly available\\nsources on the Internet. To attempt to cover as broad a set\\nof visual concepts as possible, we search for (image, text)\\npairs as part of the construction process whose text includes\\none of a set of 500,000 queries.1 We approximately class', '1The base query list is all words occurring at least 100 times in\\nthe English version of Wikipedia. This is augmented with bi-grams\\n\\nLearning Transferable Visual Models From Natural Language Supervision 4', 'balance the results by including up to 20,000 (image, text)\\npairs per query. The resulting dataset has a similar total\\nword count as the WebText dataset used to train GPT-2. We\\nrefer to this dataset as WIT for WebImageText.', '2.3. Selecting an Efficient Pre-Training Method', 'State-of-the-art computer vision systems use very large\\namounts of compute. Mahajan et al. (2018) required 19\\nGPU years to train their ResNeXt101-32x48d and Xie et al.\\n(2020) required 33 TPUv3 core-years to train their Noisy\\nStudent EfficientNet-L2. When considering that both these\\nsystems were trained to predict only 1000 ImageNet classes,\\nthe task of learning an open set of visual concepts from\\nnatural language seems daunting. In the course of our ef-\\nforts, we found training efficiency was key to successfully\\nscaling natural language supervision and we selected our\\nfinal pre-training method based on this metric.', 'Our initial approach, similar to VirTex, jointly trained an\\nimage CNN and text transformer from scratch to predict the\\ncaption of an image. However, we encountered difficulties\\nefficiently scaling this method. In Figure 2 we show that a\\n63 million parameter transformer language model, which\\nalready uses twice the compute of its ResNet-50 image\\nencoder, learns to recognize ImageNet classes three times\\nslower than a much simpler baseline that predicts a bag-of-\\nwords encoding of the same text.', 'Both these approaches share a key similarity. They try to pre-\\ndict the exact words of the text accompanying each image.\\nThis is a difficult task due to the wide variety of descriptions,\\ncomments, and related text that co-occur with images. Re-\\ncent work in contrastive representation learning for images\\nhas found that contrastive objectives can learn better repre-\\nsentations than their equivalent predictive objective (Tian\\net al., 2019). Other work has found that although generative\\nmodels of images can learn high quality image representa-\\ntions, they require over an order of magnitude more compute\\nthan contrastive models with the same performance (Chen\\net al., 2020a). Noting these findings, we explored training\\na system to solve the potentially easier proxy task of pre-\\ndicting only which text as a whole is paired with which\\nimage and not the exact words of that text. Starting with\\nthe same bag-of-words encoding baseline, we swapped the\\npredictive objective for a contrastive objective in Figure 2\\nand observed a further 4x efficiency improvement in the rate\\nof zero-shot transfer to ImageNet.', 'Given a batch of N (image, text) pairs, CLIP is trained to\\npredict which of the N ×N possible (image, text) pairings\\nacross a batch actually occurred. To do this, CLIP learns a', 'with high pointwise mutual information as well as the names of\\nall Wikipedia articles above a certain search volume. Finally all\\nWordNet synsets not already in the query list are added.', 'multi-modal embedding space by jointly training an image\\nencoder and text encoder to maximize the cosine similar-\\nity of the image and text embeddings of the N real pairs\\nin the batch while minimizing the cosine similarity of the\\nembeddings of the N2 − N incorrect pairings. We opti-\\nmize a symmetric cross entropy loss over these similarity\\nscores. In Figure 3 we include pseudocode of the core of an\\nimplementation of CLIP. To our knowledge this batch con-\\nstruction technique and objective was first introduced in the\\narea of deep metric learning as the multi-class N-pair loss\\nSohn (2016), was popularized for contrastive representation\\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\\nrecently adapted for contrastive (text, image) representation\\nlearning in the domain of medical imaging by Zhang et al.\\n(2020).', 'Due to the large size of our pre-training dataset, over-fitting\\nis not a major concern and the details of training CLIP are\\nsimplified compared to the implementation of Zhang et al.\\n(2020). We train CLIP from scratch without initializing the\\nimage encoder with ImageNet weights or the text encoder\\nwith pre-trained weights. We do not use the non-linear\\nprojection between the representation and the contrastive\\nembedding space, a change which was introduced by Bach-\\nman et al. (2019) and popularized by Chen et al. (2020b).\\nWe instead use only a linear projection to map from each en-\\ncoder’s representation to the multi-modal embedding space.\\nWe did not notice a difference in training efficiency between\\nthe two versions and speculate that non-linear projections\\nmay be co-adapted with details of current image only in\\nself-supervised representation learning methods. We also\\nremove the text transformation function tu from Zhang et al.\\n(2020) which samples a single sentence at uniform from\\nthe text since many of the (image, text) pairs in CLIP’s pre-\\ntraining dataset are only a single sentence. We also simplify\\nthe image transformation function tv. A random square\\ncrop from resized images is the only data augmentation\\nused during training. Finally, the temperature parameter\\nwhich controls the range of the logits in the softmax, τ , is\\ndirectly optimized during training as a log-parameterized\\nmultiplicative scalar to avoid turning as a hyper-parameter.', '2.4. Choosing and Scaling a Model', 'We consider two different architectures for the image en-\\ncoder. For the first, we use ResNet-50 (He et al., 2016a)\\nas the base architecture for the image encoder due to its\\nwidespread adoption and proven performance. We make sev-\\neral modifications to the original version using the ResNet-\\nD improvements from He et al. (2019) and the antialiased\\nrect-2 blur pooling from Zhang (2019). We also replace\\nthe global average pooling layer with an attention pooling\\nmechanism. The attention pooling is implemented as a sin-\\ngle layer of “transformer-style” multi-head QKV attention\\nwhere the query is conditioned on the global average-pooled', 'Learning Transferable Visual Models From Natural Language Supervision 5', '# image_encoder - ResNet or Vision Transformer\\n# text_encoder  - CBOW or Text Transformer\\n# I[n, h, w, c] - minibatch of aligned images\\n# T[n, l]       - minibatch of aligned texts\\n# W_i[d_i, d_e] - learned proj of image to embed\\n# W_t[d_t, d_e] - learned proj of text to embed\\n# t             - learned temperature parameter', '# extract feature representations of each modality\\nI_f = image_encoder(I) #[n, d_i]\\nT_f = text_encoder(T)  #[n, d_t]\\n\\n# joint multimodal embedding [n, d_e]\\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)', '# scaled pairwise cosine similarities [n, n]\\nlogits = np.dot(I_e, T_e.T) * np.exp(t)', '# symmetric loss function\\nlabels = np.arange(n)\\nloss_i = cross_entropy_loss(logits, labels, axis=0)\\nloss_t = cross_entropy_loss(logits, labels, axis=1)\\nloss   = (loss_i + loss_t)/2', 'Figure 3. Numpy-like pseudocode for the core of an implementa-\\ntion of CLIP.', 'representation of the image. For the second architecture, we\\nexperiment with the recently introduced Vision Transformer\\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\\nimplementation with only the minor modification of adding\\nan additional layer normalization to the combined patch\\nand position embeddings before the transformer and use a\\nslightly different initialization scheme.', 'The text encoder is a Transformer (Vaswani et al., 2017)\\nwith the architecture modifications described in Radford\\net al. (2019). As a base size we use a 63M-parameter 12-\\nlayer 512-wide model with 8 attention heads. The trans-\\nformer operates on a lower-cased byte pair encoding (BPE)\\nrepresentation of the text with a 49,152 vocab size (Sen-\\nnrich et al., 2015). For computational efficiency, the max\\nsequence length was capped at 76. The text sequence is\\nbracketed with [SOS] and [EOS] tokens and the activa-\\ntions of the highest layer of the transformer at the [EOS]\\ntoken are treated as the feature representation of the text\\nwhich is layer normalized and then linearly projected into\\nthe multi-modal embedding space. Masked self-attention\\nwas used in the text encoder to preserve the ability to ini-\\ntialize with a pre-trained language model or add language\\nmodeling as an auxiliary objective, though exploration of\\nthis is left as future work.', 'While previous computer vision research has often scaled\\nmodels by increasing the width (Mahajan et al., 2018) or\\ndepth (He et al., 2016a) in isolation, for the ResNet image\\nencoders we adapt the approach of Tan & Le (2019) which\\nfound that allocating additional compute across all of width,\\ndepth, and resolution outperforms only allocating it to only', 'one dimension of the model. While Tan & Le (2019) tune\\nthe ratio of compute allocated to each dimension for their\\nEfficientNet architecture, we use a simple baseline of allo-\\ncating additional compute equally to increasing the width,\\ndepth, and resolution of the model. For the text encoder, we\\nonly scale the width of the model to be proportional to the\\ncalculated increase in width of the ResNet and do not scale\\nthe depth at all, as we found CLIP’s performance to be less\\nsensitive to the capacity of the text encoder.', '2.5. Training', 'We train a series of 5 ResNets and 3 Vision Transformers.\\nFor the ResNets we train a ResNet-50, a ResNet-101, and\\nthen 3 more which follow EfficientNet-style model scaling\\nand use approximately 4x, 16x, and 64x the compute of a\\nResNet-50. They are denoted as RN50x4, RN50x16, and\\nRN50x64 respectively. For the Vision Transformers we\\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\\nmodels for 32 epochs. We use the Adam optimizer (Kingma\\n& Ba, 2014) with decoupled weight decay regularization\\n(Loshchilov & Hutter, 2017) applied to all weights that are\\nnot gains or biases, and decay the learning rate using a\\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\\nparameters were set using a combination of grid searches,\\nrandom search, and manual tuning on the baseline ResNet-\\n50 model when trained for 1 epoch. Hyper-parameters were\\nthen adapted heuristically for larger models due to compu-\\ntational constraints. The learnable temperature parameter\\nτ was initialized to the equivalent of 0.07 from (Wu et al.,\\n2018) and clipped to prevent scaling the logits by more\\nthan 100 which we found necessary to prevent training in-\\nstability. We use a very large minibatch size of 32,768.\\nMixed-precision (Micikevicius et al., 2017) was used to ac-\\ncelerate training and save memory. To save additional mem-\\nory, gradient checkpointing (Griewank & Walther, 2000;\\nChen et al., 2016), half-precision Adam statistics (Dhariwal\\net al., 2020), and half-precision stochastically rounded text\\nencoder weights were used. The calculation of embedding\\nsimilarities was also sharded with individual GPUs comput-\\ning only the subset of the pairwise similarities necessary for\\ntheir local batch of embeddings. The largest ResNet model,\\nRN50x64, took 18 days to train on 592 V100 GPUs while\\nthe largest Vision Transformer took 12 days on 256 V100\\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\\npixel resolution for one additional epoch to boost perfor-\\nmance similar to FixRes (Touvron et al., 2019). We denote\\nthis model as ViT-L/14@336px. Unless otherwise specified,\\nall results reported in this paper as “CLIP” use this model\\nwhich we found to perform best.', 'Learning Transferable Visual Models From Natural Language Supervision 6\\n\\n3. Experiments\\n3.1. Zero-Shot Transfer\\n\\n3.1.1. MOTIVATION', 'In computer vision, zero-shot learning usually refers to the\\nstudy of generalizing to unseen object categories in image\\nclassification (Lampert et al., 2009). We instead use the\\nterm in a broader sense and study generalization to unseen\\ndatasets. We motivate this as a proxy for performing un-\\nseen tasks, as aspired to in the zero-data learning paper of\\nLarochelle et al. (2008). While much research in the field of\\nunsupervised learning focuses on the representation learn-\\ning capabilities of machine learning systems, we motivate\\nstudying zero-shot transfer as a way of measuring the task-\\nlearning capabilities of machine learning systems. In this\\nview, a dataset evaluates performance on a task on a spe-\\ncific distribution. However, many popular computer vision\\ndatasets were created by the research community primarily\\nas benchmarks to guide the development of generic image\\nclassification methods rather than measuring performance\\non a specific task. While it is reasonable to say that the\\nSVHN dataset measures the task of street number transcrip-\\ntion on the distribution of Google Street View photos, it is\\nunclear what “real” task the CIFAR-10 dataset measures.\\nIt is clear, however, what distribution CIFAR-10 is drawn\\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\\nrobustness to distribution shift and domain generalization\\nrather than task generalization. Please see Section 3.3 for\\nanalysis focused on this.', 'To our knowledge, Visual N-Grams (Li et al., 2017) first\\nstudied zero-shot transfer to existing image classification\\ndatasets in the manner described above. It is also the only\\nother work we are aware of that has studied zero-shot trans-\\nfer to standard image classification datasets using a gener-\\nically pre-trained model and serves as the best reference\\npoint for contextualizing CLIP. Their approach learns the\\nparameters of a dictionary of 142,806 visual n-grams (span-\\nning 1- to 5- grams) and optimizes these n-grams using a\\ndifferential version of Jelinek-Mercer smoothing to maxi-\\nmize the probability of all text n-grams for a given image.\\nIn order to perform zero-shot transfer, they first convert the\\ntext of each of the dataset’s class names into its n-gram\\nrepresentation and then compute its probability according\\nto their model, predicting the one with the highest score.', 'Our focus on studying zero-shot transfer as an evaluation of\\ntask learning is inspired by work demonstrating task learn-\\ning in the field of NLP. To our knowledge Liu et al. (2018)\\nfirst identified task learning as an “unexpected side-effect”\\nwhen a language model trained to generate Wikipedia ar-\\nticles learned to reliably transliterate names between lan-\\nguages. While GPT-1 (Radford et al., 2018) focused on pre-', 'training as a transfer learning method to improve supervised\\nfine-tuning, it also included an ablation study demonstrat-\\ning that the performance of four heuristic zero-shot transfer\\nmethods improved steadily over the course of pre-training,\\nwithout any supervised adaption. This analysis served as the\\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\\nsively on studying the task-learning capabilities of language\\nmodels via zero-shot transfer.', '3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER', 'CLIP is pre-trained to predict if an image and a text snippet\\nare paired together in its dataset. To perform zero-shot clas-\\nsification, we reuse this capability. For each dataset, we use\\nthe names of all the classes in the dataset as the set of poten-\\ntial text pairings and predict the most probable (image, text)\\npair according to CLIP. In a bit more detail, we first compute\\nthe feature embedding of the image and the feature embed-\\nding of the set of possible texts by their respective encoders.\\nThe cosine similarity of these embeddings is then calculated,\\nscaled by a temperature parameter τ , and normalized into a\\nprobability distribution via a softmax. Note that this predic-\\ntion layer is a multinomial logistic regression classifier with\\nL2-normalized inputs, L2-normalized weights, no bias, and\\ntemperature scaling. When interpreted this way, the image\\nencoder is the computer vision backbone which computes a\\nfeature representation for the image and the text encoder is a\\nhypernetwork (Ha et al., 2016) which generates the weights\\nof a linear classifier based on the text specifying the visual\\nconcepts that the classes represent. Lei Ba et al. (2015) first\\nintroduced a zero-shot image classifier of this form while\\nthe idea of generating a classifier from natural language\\ndates back to at least Elhoseiny et al. (2013). Continuing\\nwith this interpretation, every step of CLIP pre-training can\\nbe viewed as optimizing the performance of a randomly\\ncreated proxy to a computer vision dataset which contains 1\\nexample per class and has 32,768 total classes defined via\\nnatural language descriptions. For zero-shot evaluation, we\\ncache the zero-shot classifier once it has been computed by\\nthe text encoder and reuse it for all subsequent predictions.\\nThis allows the cost of generating it to be amortized across\\nall the predictions in a dataset.', '3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS', 'In Table 1 we compare Visual N-Grams to CLIP. The best\\nCLIP model improves accuracy on ImageNet from a proof\\nof concept 11.5% to 76.2% and matches the performance\\nof the original ResNet-50 despite using none of the 1.28\\nmillion crowd-labeled training examples available for this\\ndataset. Additionally, the top-5 accuracy of CLIP models\\nare noticeably higher than their top-1, and this model has a\\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\\n2016). The ability to match the performance of a strong,\\nfully supervised baselines in a zero-shot setting suggests', 'Learning Transferable Visual Models From Natural Language Supervision 7\\n\\naYahoo ImageNet SUN\\n\\nVisual N-Grams 72.4 11.5 23.0\\nCLIP 98.4 76.2 58.5', 'Table 1. Comparing CLIP to prior zero-shot transfer image classi-\\nfication results. CLIP improves performance on all three datasets\\nby a large amount. This improvement reflects many differences\\nin the 4 years since the development of Visual N-Grams (Li et al.,\\n2017).', 'CLIP is a significant step towards flexible and practical\\nzero-shot computer vision classifiers. As mentioned above,\\nthe comparison to Visual N-Grams is meant for contextu-\\nalizing the performance of CLIP and should not be inter-\\npreted as a direct methods comparison between CLIP and\\nVisual N-Grams as many performance relevant differences\\nbetween the two systems were not controlled for. For in-\\nstance, we train on a dataset that is 10x larger, use a vision\\nmodel that requires nearly 100x more compute per predic-\\ntion, likely used over 1000x their training compute, and\\nuse a transformer-based model which did not exist when\\nVisual N-Grams was published. As a closer comparison, we\\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\\nthat Visual N-Grams was trained on and found it matched\\ntheir reported ImageNet performance within a V100 GPU\\nday. This baseline was also trained from scratch instead of\\nbeing initialized from pre-trained ImageNet weights as in\\nVisual N-Grams.', 'CLIP also outperforms Visual N-Grams on the other 2 re-\\nported datasets. On aYahoo, CLIP achieves a 95% reduction\\nin the number of errors, and on SUN, CLIP more than dou-\\nbles the accuracy of Visual N-Grams. To conduct a more\\ncomprehensive analysis and stress test, we implement a\\nmuch larger evaluation suite detailed in Appendix A. In\\ntotal we expand from the 3 datasets reported in Visual N-\\nGrams to include over 30 datasets and compare to over 50\\nexisting computer vision systems to contextualize results.', '3.1.4. PROMPT ENGINEERING AND ENSEMBLING', 'Most standard image classification datasets treat the infor-\\nmation naming or describing classes which enables natural\\nlanguage based zero-shot transfer as an afterthought. The\\nvast majority of datasets annotate images with just a numeric\\nid of the label and contain a file mapping these ids back to\\ntheir names in English. Some datasets, such as Flowers102\\nand GTSRB, don’t appear to include this mapping at all\\nin their released versions preventing zero-shot transfer en-\\ntirely.2 For many datasets, we observed these labels may be', '2Alec learned much more about flower species and German\\ntraffic signs over the course of this project than he originally antic-\\nipated.\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\n4X efficiency gain', '4X efficiency gain\\n\\n5 point\\nimprovement\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64\\n\\nPrompt engineering and ensembling\\nContextless class names (Li et al. 2017)', 'Figure 4. Prompt engineering and ensembling improve zero-\\nshot performance. Compared to the baseline of using contextless\\nclass names, prompt engineering and ensembling boost zero-shot\\nclassification performance by almost 5 points on average across\\n36 datasets. This improvement is similar to the gain from using\\n4 times more compute with the baseline zero-shot method but is\\n“free” when amortized over many predictions.', 'chosen somewhat haphazardly and do not anticipate issues\\nrelated to zero-shot transfer which relies on task description\\nin order to transfer successfully.', 'A common issue is polysemy. When the name of a class\\nis the only information provided to CLIP’s text encoder it\\nis unable to differentiate which word sense is meant due to\\nthe lack of context. In some cases multiple meanings of the\\nsame word might be included as different classes in the same\\ndataset! This happens in ImageNet which contains both\\nconstruction cranes and cranes that fly. Another example is\\nfound in classes of the Oxford-IIIT Pet dataset where the\\nword boxer is, from context, clearly referring to a breed of\\ndog, but to a text encoder lacking context could just as likely\\nrefer to a type of athlete.', 'Another issue we encountered is that it’s relatively rare in\\nour pre-training dataset for the text paired with the image\\nto be just a single word. Usually the text is a full sentence\\ndescribing the image in some way. To help bridge this\\ndistribution gap, we found that using the prompt template\\n“A photo of a {label}.” to be a good default that\\nhelps specify the text is about the content of the image. This\\noften improves performance over the baseline of using only\\nthe label text. For instance, just using this prompt improves\\naccuracy on ImageNet by 1.3%.', 'Learning Transferable Visual Models From Natural Language Supervision 8', 'Similar to the “prompt engineering” discussion around GPT-\\n3 (Brown et al., 2020; Gao et al., 2020), we have also\\nobserved that zero-shot performance can be significantly\\nimproved by customizing the prompt text to each task. A\\nfew, non exhaustive, examples follow. We found on several\\nfine-grained image classification datasets that it helped to\\nspecify the category. For example on Oxford-IIIT Pets, us-\\ning “A photo of a {label}, a type of pet.”\\nto help provide context worked well. Likewise, on Food101\\nspecifying a type of food and on FGVC Aircraft a type of\\naircraft helped too. For OCR datasets, we found that putting\\nquotes around the text or number to be recognized improved\\nperformance. Finally, we found that on satellite image classi-\\nfication datasets it helped to specify that the images were of\\nthis form and we use variants of “a satellite photo\\nof a {label}.”.', 'We also experimented with ensembling over multiple zero-\\nshot classifiers as another way of improving performance.\\nThese classifiers are computed by using different context\\nprompts such as ‘A photo of a big {label}” and\\n“A photo of a small {label}”. We construct the\\nensemble over the embedding space instead of probability\\nspace. This allows us to cache a single set of averaged text\\nembeddings so that the compute cost of the ensemble is the\\nsame as using a single classifier when amortized over many\\npredictions. We’ve observed ensembling across many gen-\\nerated zero-shot classifiers to reliably improve performance\\nand use it for the majority of datasets. On ImageNet, we\\nensemble 80 different context prompts and this improves\\nperformance by an additional 3.5% over the single default\\nprompt discussed above. When considered together, prompt\\nengineering and ensembling improve ImageNet accuracy\\nby almost 5%. In Figure 4 we visualize how prompt engi-\\nneering and ensembling change the performance of a set of\\nCLIP models compared to the contextless baseline approach\\nof directly embedding the class name as done in Li et al.\\n(2017).', '3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE', 'Since task-agnostic zero-shot classifiers for computer vision\\nhave been understudied, CLIP provides a promising oppor-\\ntunity to gain a better understanding of this type of model.\\nIn this section, we conduct a study of various properties of\\nCLIP’s zero-shot classifiers. As a first question, we look\\nsimply at how well zero-shot classifiers perform. To con-\\ntextualize this, we compare to the performance of a simple\\noff-the-shelf baseline: fitting a fully supervised, regularized,\\nlogistic regression classifier on the features of the canonical\\nResNet-50. In Figure 5 we show this comparison across 27\\ndatasets. Please see Appendix A for details of datasets and\\nsetup.', 'Zero-shot CLIP outperforms this baseline slightly more of-\\n\\n40 30 20 10 0 10 20 30 40\\n Score (%)\\n\\nZero-Shot CLIP vs. Linear Probe on ResNet50', 'EuroSAT-37.1\\nKITTI Distance-34.0\\nPatchCamelyon-19.5\\nGTSRB-18.4\\nCLEVRCounts-18.2\\nDTD-16.6\\nFlowers102-12.5\\nRESISC45-11.9\\nFGVCAircraft-11.3\\nMNIST-10.0\\nBirdsnap-3.2\\n+0.5PascalVOC2007\\n+1.1OxfordPets\\n+1.9ImageNet\\n+2.0Caltech101\\n+2.8FER2013\\n+3.0STL10\\n+3.0CIFAR100\\n+3.9CIFAR10', '+6.7HatefulMemes\\n+7.7UCF101\\n+7.8SUN397\\n\\n+12.4SST2\\n+14.5Kinetics700\\n\\n+22.5Food101\\n+23.2Country211\\n\\n+28.9StanfordCars', 'Figure 5. Zero-shot CLIP is competitive with a fully super-\\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\\nclassifier outperforms a fully supervised linear classifier fitted on\\nResNet-50 features on 16 datasets, including ImageNet.', 'ten than not and wins on 16 of the 27 datasets. Looking at\\nindividual datasets reveals some interesting behavior. On\\nfine-grained classification tasks, we observe a wide spread\\nin performance. On two of these datasets, Stanford Cars and\\nFood101, zero-shot CLIP outperforms logistic regression\\non ResNet-50 features by over 20% while on two others,\\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\\nforms by over 10%. On OxfordPets and Birdsnap, per-\\nformance is much closer. We suspect these difference are\\nprimarily due to varying amounts of per-task supervision\\nbetween WIT and ImageNet. On “general” object classifica-\\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\\nPascalVOC2007 performance is relatively similar with a\\nslight advantage for zero-shot CLIP in all cases. On STL10,\\nCLIP achieves 99.3% overall which appears to be a new\\nstate of the art despite not using any training examples. Zero-\\nshot CLIP significantly outperforms a ResNet-50 on two\\ndatasets measuring action recognition in videos. On Kinet-\\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\\non UCF101. We speculate this is due to natural language\\nproviding wider supervision for visual concepts involving\\nverbs, compared to the noun-centric object supervision in\\nImageNet.', 'Looking at where zero-shot CLIP notably underperforms,\\n\\nLearning Transferable Visual Models From Natural Language Supervision 9\\n\\n0 1 2 4 8 16\\n# of labeled training examples per class\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\nAv\\n\\ner\\nag\\n\\ne \\nSc\\n\\nor\\ne \\n\\n(%\\n)', 'e \\nSc\\n\\nor\\ne \\n\\n(%\\n)\\n\\nZero-Shot\\nCLIP BiT-M (ImageNet-21K)\\n\\nLinear Probe CLIP\\n\\nSimCLRv2\\n\\nResNet50', 'Figure 6. Zero-shot CLIP outperforms few-shot linear probes.\\nZero-shot CLIP matches the average performance of a 4-shot linear\\nclassifier trained on the same feature space and nearly matches the\\nbest results of a 16-shot linear classifier across publicly available\\nmodels. For both BiT-M and SimCLRv2, the best performing\\nmodel is highlighted. Light gray lines are other models in the eval\\nsuite. The 20 datasets with at least 16 examples per class were\\nused in this analysis.', 'we see that zero-shot CLIP is quite weak on several spe-\\ncialized, complex, or abstract tasks such as satellite image\\nclassification (EuroSAT and RESISC45), lymph node tumor\\ndetection (PatchCamelyon), counting objects in synthetic\\nscenes (CLEVRCounts), self-driving related tasks such as\\nGerman traffic sign recognition (GTSRB), recognizing dis-\\ntance to the nearest car (KITTI Distance). These results\\nhighlight the poor capability of zero-shot CLIP on more\\ncomplex tasks. By contrast, non-expert humans can robustly\\nperform several of these tasks, such as counting, satellite\\nimage classification, and traffic sign recognition, suggesting\\nsignificant room for improvement. However, we caution\\nthat it is unclear whether measuring zero-shot transfer, as\\nopposed to few-shot transfer, is a meaningful evaluation for\\ndifficult tasks that a learner has no prior experience with,\\nsuch as lymph node tumor classification for almost all hu-\\nmans (and possibly CLIP).', 'While comparing zero-shot performance to fully supervised\\nmodels contextualizes the task-learning capabilities of CLIP,\\ncomparing to few-shot methods is a more direct compari-\\nson, since zero-shot is its limit. In Figure 6, we visualize\\nhow zero-shot CLIP compares to few-shot logistic regres-\\nsion on the features of many image models including the\\nbest publicly available ImageNet models, self-supervised\\nlearning methods, and CLIP itself. While it is intuitive to', 'expect zero-shot to underperform one-shot, we instead find\\nthat zero-shot CLIP matches the performance of 4-shot lo-\\ngistic regression on the same feature space. This is likely\\ndue to an important difference between the zero-shot and\\nfew-shot approach. First, CLIP’s zero-shot classifier is gen-\\nerated via natural language which allows for visual concepts\\nto be directly specified (“communicated”). By contrast,\\n“normal” supervised learning must infer concepts indirectly\\nfrom training examples. Context-less example-based learn-\\ning has the drawback that many different hypotheses can\\nbe consistent with the data, especially in the one-shot case.\\nA single image often contains many different visual con-\\ncepts. Although a capable learner is able to exploit visual\\ncues and heuristics, such as assuming that the concept being\\ndemonstrated is the primary object in an image, there is no\\nguarantee.', 'A potential resolution of this discrepancy between zero-\\nshot and few-shot performance is to use CLIP’s zero-shot\\nclassifier as a prior for the weights of the few-shot classifier.\\nWhile adding an L2 penalty towards the generated weights\\nis a straightforward implementation of this idea, we found\\nthat hyperparameter optimization would often select for\\nsuch a large value of this regularizer that the resulting few-\\nshot classifier was “just” the zero-shot classifier. Research\\ninto better methods of combining the strength of zero-shot\\ntransfer with flexibility of few-shot learning is a promising\\ndirection for future work.', 'When comparing zero-shot CLIP to few-shot logistic re-\\ngression on the features of other models, zero-shot CLIP\\nroughly matches the performance of the best performing\\n16-shot classifier in our evaluation suite, which uses the fea-\\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\\nWe are certain that a BiT-L model trained on JFT-300M\\nwould perform even better but these models have not been\\npublicly released. That a BiT-M ResNet-152x2 performs\\nbest in a 16-shot setting is somewhat surprising since, as\\nanalyzed in Section 3.2, the Noisy Student EfficientNet-L2\\noutperforms it in a fully supervised setting by almost 5% on\\naverage across 27 datasets.', 'In addition to studying the average performance of zero-shot\\nCLIP and few-shot logistic regression, we also examine\\nperformance on individual datasets. In Figure 7, we show\\nestimates for the number of labeled examples per class that\\na logistic regression classifier on the same feature space\\nrequires to match the performance of zero-shot CLIP. Since\\nzero-shot CLIP is also a linear classifier, this estimates the\\neffective data efficiency of zero-shot transfer in this setting.\\nIn order to avoid training thousands of linear classifiers,\\nwe estimate the effective data efficiency based on a log-\\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\\nshot (when possible), and a fully supervised linear classifier\\ntrained on each dataset. We find that zero-shot transfer can', 'Learning Transferable Visual Models From Natural Language Supervision 10\\n\\n0 25 50 75 100 125 150 175 200\\n# of labeled examples per class\\n\\nrequired to match zero-shot\\n\\nFlowers102\\nEuroSAT\\n\\nRESISC45\\nCLEVRCounts\\n\\nGTSRB\\nFGVCAircraft\\n\\nDTD\\nBirdsnap\\nUCF101', 'DTD\\nBirdsnap\\nUCF101\\n\\nKITTI Distance\\nCaltech101\\n\\nSUN397\\nMNIST\\n\\nStanfordCars\\nHatefulMemes\\n\\nCIFAR100\\nSTL10\\n\\nKinetics700\\nSST2\\n\\nPCam\\nImageNet\\n\\nCountry211\\nOxfordPets\\n\\nFood101\\nCIFAR10\\nFER2013', '0.9\\n0.9\\n1.5\\n1.5\\n1.6\\n2.0\\n2.6\\n2.7\\n2.9\\n2.9\\n3.5\\n3.9\\n4.8\\n6.0\\n9.8\\n12.0\\n12.7\\n13.6\\n14.4\\n14.7\\n16.0\\n\\n32\\n48\\n\\n64\\n81\\n\\n184\\n\\nMedian: 5.4\\nMean:  20.8', 'Figure 7. The data efficiency of zero-shot transfer varies\\nwidely. Calculating the number of labeled examples per class\\na linear classifier on the same CLIP feature space requires to match\\nthe performance of the zero-shot classifier contextualizes the ef-\\nfectiveness of zero-shot transfer. Values are estimated based on\\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\\nresults. Performance varies widely from still underperforming a\\none-shot classifier on two datasets to matching an estimated 184\\nlabeled examples per class.', 'have widely varying efficiency per dataset from less than 1\\nlabeled example per class to 184. Two datasets, Flowers102\\nand EuroSAT underperform one-shot models. Half of the\\ndatasets require less than 5 examples per class with a median\\nof 5.4. However, the mean estimated data efficiency is 20.8\\nexamples per class. This is due to the 20% of datasets\\nwhere supervised classifiers require many labeled examples\\nper class in order to match performance. On ImageNet,\\nzero-shot CLIP matches the performance of a 16-shot linear\\nclassifier trained on the same feature space.', 'If we assume that evaluation datasets are large enough that\\nthe parameters of linear classifiers trained on them are well\\nestimated, then, because CLIP’s zero-shot classifier is also\\na linear classifier, the performance of the fully supervised\\nclassifiers roughly sets an upper bound for what zero-shot\\ntransfer can achieve. In Figure 8 we compare CLIP’s zero-\\nshot performance with fully supervised linear classifiers\\nacross datasets. The dashed, y = x line represents an “op-\\ntimal” zero-shot classifier that matches the performance of\\nits fully supervised equivalent. For most datasets, the per-\\nformance of zero-shot classifiers still underperform fully su-\\npervised classifiers by 10% to 25%, suggesting that there is\\nstill plenty of headroom for improving CLIP’s task-learning\\nand zero-shot transfer capabilities.', 'There is a positive correlation of 0.82 (p-value < 10−6)\\nbetween zero-shot performance and fully supervised perfor-\\n\\n20 30 40 50 60 70 80 90 100\\nLinear Probe CLIP Performance\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt C\\nLI\\n\\nP \\nPe\\n\\nrfo\\nrm\\n\\nan\\nce', 'P \\nPe\\n\\nrfo\\nrm\\n\\nan\\nce\\n\\nr = 0.82\\n\\nVOC2007\\n\\nCountry211\\n\\nHatefulMemes\\n\\nMNIST\\n\\nCIFAR10\\n\\nSST2\\n\\nDTD\\n\\nPCAM\\n\\nRESISC45\\n\\nEuroSAT\\n\\nGTSRB\\n\\nCLEVRCounts\\n\\nFER2013\\n\\nUCF101\\n\\nBirdsnap\\n\\nOxfordPets\\n\\nCIFAR100\\n\\nFGVCAircraft\\n\\nFood101\\n\\nFlowers102Stanford Cars\\n\\nCaltech101\\n\\nSUN397', 'Caltech101\\n\\nSUN397\\n\\nImageNet\\n\\nSTL10\\n\\nKITTI Distance\\n\\nKinetics700', 'Figure 8. Zero-shot performance is correlated with linear\\nprobe performance but still mostly sub-optimal. Comparing\\nzero-shot and linear probe performance across datasets shows a\\nstrong correlation with zero-shot performance mostly shifted 10 to\\n25 points lower. On only 5 datasets does zero-shot performance\\napproach linear probe performance (≤3 point difference).', 'mance, suggesting that CLIP is relatively consistent at con-\\nnecting underlying representation and task learning to zero-\\nshot transfer. However, zero-shot CLIP only approaches\\nfully supervised performance on 5 datasets: STL10, CI-\\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\\ndatasets, both zero-shot accuracy and fully supervised accu-\\nracy are over 90%. This suggests that CLIP may be more\\neffective at zero-shot transfer for tasks where its underly-\\ning representations are also high quality. The slope of a\\nlinear regression model predicting zero-shot performance\\nas a function of fully supervised performance estimates that\\nfor every 1% improvement in fully supervised performance,\\nzero-shot performance improves by 1.28%. However, the\\n95th-percentile confidence intervals still include values of\\nless than 1 (0.93-1.79).', 'Over the past few years, empirical studies of deep learning\\nsystems have documented that performance is predictable as\\na function of important quantities such as training compute\\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\\nThe GPT family of models has so far demonstrated consis-\\ntent improvements in zero-shot performance across a 1000x\\nincrease in training compute. In Figure 9, we check whether\\nthe zero-shot performance of CLIP follows a similar scaling\\npattern. We plot the average error rate of the 5 ResNet CLIP\\nmodels across 39 evaluations on 36 different datasets and\\nfind that a similar log-log linear scaling trend holds for CLIP\\nacross a 44x increase in model compute. While the overall\\ntrend is smooth, we found that performance on individual\\nevaluations can be much noisier. We are unsure whether', 'Learning Transferable Visual Models From Natural Language Supervision 11\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nEr\\nro\\n\\nr (\\n%\\n\\n)\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64', 'Figure 9. Zero-shot CLIP performance scales smoothly as a\\nfunction of model compute. Across 39 evals on 36 different\\ndatasets, average zero-shot error is well modeled by a log-log lin-\\near trend across a 44x range of compute spanning 5 different CLIP\\nmodels. Lightly shaded lines are performance on individual evals,\\nshowing that performance is much more varied despite the smooth\\noverall trend.', 'this is caused by high variance between individual training\\nruns on sub-tasks (as documented in D’Amour et al. (2020))\\nmasking a steadily improving trend or whether performance\\nis actually non-monotonic as a function of compute on some\\ntasks.', '3.2. Representation Learning', 'While we have extensively analyzed the task-learning ca-\\npabilities of CLIP through zero-shot transfer in the previ-\\nous section, it is more common to study the representation\\nlearning capabilities of a model. There exist many ways to\\nevaluate the quality of representations as well as disagree-\\nments over what properties an “ideal” representation should\\nhave (Locatello et al., 2020). Fitting a linear classifier on\\na representation extracted from the model and measuring\\nits performance on various datasets is a common approach.\\nAn alternative is measuring the performance of end-to-end\\nfine-tuning of the model. This increases flexibility, and\\nprior work has convincingly demonstrated that fine-tuning\\noutperforms linear classification on most image classifi-\\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\\nWhile the high performance of fine-tuning motivates its\\nstudy for practical reasons, we still opt for linear classifier\\nbased evaluation for several reasons. Our work is focused\\non developing a high-performing task and dataset-agnostic\\npre-training approach. Fine-tuning, because it adapts rep-\\nresentations to each dataset during the fine-tuning phase,\\ncan compensate for and potentially mask failures to learn\\ngeneral and robust representations during the pre-training\\nphase. Linear classifiers, because of their limited flexibility,\\ninstead highlight these failures and provide clear feedback\\nduring development. For CLIP, training supervised linear', 'classifiers has the added benefit of being very similar to the\\napproach used for its zero-shot classifiers which enables\\nextensive comparisons and analysis in Section 3.1. Finally,\\nwe aim to compare CLIP to a comprehensive set of existing\\nmodels across many tasks. Studying 66 different models on\\n27 different datasets requires tuning 1782 different evalua-\\ntions. Fine-tuning opens up a much larger design and hyper-\\nparameter space, which makes it difficult to fairly evaluate\\nand computationally expensive to compare a diverse set of\\ntechniques as discussed in other large scale empirical studies\\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\\nclassifiers require minimal hyper-parameter tuning and have\\nstandardized implementations and evaluation procedures.\\nPlease see Appendix A for further details on evaluation.', 'Figure 10 summarizes our findings. To minimize selection\\neffects that could raise concerns of confirmation or reporting\\nbias, we first study performance on the 12 dataset evaluation\\nsuite from Kornblith et al. (2019). While small CLIP mod-\\nels such as a ResNet-50 and ResNet-101 outperform other\\nResNets trained on ImageNet-1K (BiT-S and the originals),\\nthey underperform ResNets trained on ImageNet-21K (BiT-\\nM). These small CLIP models also underperform models\\nin the EfficientNet family with similar compute require-\\nments. However, models trained with CLIP scale very well\\nand the largest model we trained (ResNet-50x64) slightly\\noutperforms the best performing existing model (a Noisy\\nStudent EfficientNet-L2) on both overall score and compute\\nefficiency. We also find that CLIP vision transformers are\\nabout 3x more compute efficient than CLIP ResNets, which\\nallows us to reach higher overall performance within our\\ncompute budget. These results qualitatively replicate the\\nfindings of Dosovitskiy et al. (2020) which reported that\\nvision transformers are more compute efficient than con-\\nvnets when trained on sufficiently large datasets. Our best\\noverall model is a ViT-L/14 that is fine-tuned at a higher res-\\nolution of 336 pixels on our dataset for 1 additional epoch.\\nThis model outperforms the best existing model across this\\nevaluation suite by an average of 2.6%.', 'As Figure 21 qualitatively shows, CLIP models learn a wider\\nset of tasks than has previously been demonstrated in a sin-\\ngle computer vision model trained end-to-end from random\\ninitialization. These tasks include geo-localization, optical\\ncharacter recognition, facial emotion recognition, and action\\nrecognition. None of these tasks are measured in the evalua-\\ntion suite of Kornblith et al. (2019). This could be argued\\nto be a form of selection bias in Kornblith et al. (2019)’s\\nstudy towards tasks that overlap with ImageNet. To address\\nthis, we also measure performance on a broader 27 dataset\\nevaluation suite. This evaluation suite, detailed in Appendix\\nA includes datasets representing the aforementioned tasks,\\nGerman Traffic Signs Recognition Benchmark (Stallkamp\\net al., 2011), as well as several other datasets adapted from\\nVTAB (Zhai et al., 2019).', \"Learning Transferable Visual Models From Natural Language Supervision 12\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\nLinear probe average over Kornblith et al.'s 12 datasets\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\", '70\\n\\n75\\n\\n80\\n\\n85\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\nLinear probe average over all 27 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet', 'Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\\nEfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or\\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.', 'On this broader evaluation suite, the benefits of CLIP are\\nmore clear. All CLIP models, regardless of scale, outper-\\nform all evaluated systems in terms of compute efficiency.\\nThe improvement in average score of the best model over\\nprevious systems increases from 2.6% to 5%. We also find\\nthat self-supervised systems do noticeably better on our\\nbroader evaluation suite. For instance, while SimCLRv2\\nstill underperforms BiT-M on average on the 12 datasets\\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\\non our 27 dataset evaluation suite. These findings suggest\\ncontinuing to expand task diversity and coverage in order\\nto better understand the “general” performance of systems.\\nWe suspect additional evaluation efforts along the lines of\\nVTAB to be valuable.', 'In addition to the aggregate analysis above, we visualize\\nper-dataset differences in the performance of the best CLIP\\nmodel and the best model in our evaluation suite across\\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\\nStudent EfficientNet-L2 on 21 of the 27 datasets. CLIP\\nimproves the most on tasks which require OCR (SST2', 'and HatefulMemes), geo-localization and scene recognition\\n(Country211, SUN397), and activity recognition in videos\\n(Kinetics700 and UCF101). In addition CLIP also does\\nmuch better on fine-grained car and traffic sign recognition\\n(Stanford Cars and GTSRB). This may reflect a problem\\nwith overly narrow supervision in ImageNet. A result such\\nas the 14.7% improvement on GTSRB could be indicative\\nof an issue with ImageNet-1K, which has only a single la-\\nbel for all traffic and street signs. This could encourage\\na supervised representation to collapse intra-class details\\nand hurt accuracy on a fine-grained downstream task. As\\nmentioned, CLIP still underperforms the EfficientNet on\\nseveral datasets. Unsurprisingly, the dataset that the Effi-\\ncientNet does best relative to CLIP on is the one it was\\ntrained on: ImageNet. The EffcientNet also slightly outper-\\nforms CLIP on low-resolution datasets such as CIFAR10\\nand CIFAR100. We suspect this is at least partly due to the\\nlack of scale-based data augmentation in CLIP. The Effi-\\ncientNet also does slightly better on PatchCamelyon and\\nCLEVRCounts, datasets where overall performance is still', 'Learning Transferable Visual Models From Natural Language Supervision 13\\n\\n10 5 0 5 10 15 20 25\\n Score (%)\\n\\nLogistic Regression on CLIP vs. EfficientNet L2 NS', 'ImageNet-3.0\\nCLEVRCounts-2.4\\nCIFAR100-1.7\\nPatchCamelyon-1.2\\nCIFAR10-0.8\\nOxfordPets-0.5\\n+0.0STL10\\n+0.5VOC2007\\n+0.5DTD\\n+0.6MNIST\\n+0.9EuroSAT\\n+1.3Caltech101\\n+1.4Flowers102\\n+1.4Birdsnap\\n+2.3KITTI Distance\\n+3.1UCF101\\n+3.2FGVCAircraft\\n+3.9Food101\\n+4.5FER2013\\n+5.1RESISC45', '+6.2Kinetics700\\n+6.5SUN397\\n\\n+14.7GTSRB\\n+15.9StanfordCars\\n\\n+18.8HatefulMemes\\n+22.7Country211\\n\\n+23.6SST2', '+23.6SST2\\n\\nFigure 11. CLIP’s features outperform the features of the best\\nImageNet model on a wide variety of datasets. Fitting a linear\\nclassifier on CLIP’s features outperforms using the Noisy Student\\nEfficientNet-L2 on 21 out of 27 datasets.', 'low for both approaches.\\n\\n3.3. Robustness to Natural Distribution Shift', 'In 2015, it was announced that a deep learning model ex-\\nceeded human performance on the ImageNet test set (He\\net al., 2015). However, research in the subsequent years\\nhas repeatedly found that these models still make many sim-\\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\\nAlcorn et al., 2019), and new benchmarks testing these sys-\\ntems has often found their performance to be much lower\\nthan both their ImageNet accuracy and human accuracy\\n(Recht et al., 2019; Barbu et al., 2019). What explains this\\ndiscrepancy? Various ideas have been suggested and stud-\\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\\ntheme of proposed explanations is that deep learning models\\nare exceedingly adept at finding correlations and patterns\\nwhich hold across their training dataset and thus improve\\nin-distribution performance. However many of these corre-\\nlations and patterns are actually spurious and do not hold for\\nother distributions and result in large drops in performance\\non other datasets.', 'We caution that, to date, most of these studies limit their\\nevaluation to models trained on ImageNet. Recalling the\\ntopic of discussion, it may be a mistake to generalize too\\nfar from these initial findings. To what degree are these\\nfailures attributable to deep learning, ImageNet, or some', 'combination of the two? CLIP models, which are trained via\\nnatural language supervision on a very large dataset and are\\ncapable of high zero-shot performance, are an opportunity\\nto investigate this question from a different angle.', 'Taori et al. (2020) is a recent comprehensive study mov-\\ning towards quantifying and understanding these behaviors\\nfor ImageNet models. Taori et al. (2020) study how the\\nperformance of ImageNet models change when evaluated\\non natural distribution shifts. They measure performance\\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\\nand ImageNet Rendition (Hendrycks et al., 2020a). They\\ndistinguish these datasets, which all consist of novel images\\ncollected from a variety of sources, from synthetic distri-\\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\\nsarial attacks (Goodfellow et al., 2014) which are created by\\nperturbing existing images in various ways. They propose\\nthis distinction because in part because they find that while\\nseveral techniques have been demonstrated to improve per-\\nformance on synthetic distribution shifts, they often fail to\\nyield consistent improvements on natural distributions.3', 'Across these collected datasets, the accuracy of ImageNet\\nmodels drop well below the expectation set by the Ima-\\ngeNet validation set. For the following summary discussion\\nwe report average accuracy across all 7 natural distribution\\nshift datasets and average accuracy across the correspond-\\ning class subsets of ImageNet unless otherwise specified.\\nAdditionally, for Youtube-BB and ImageNet-Vid, which\\nhave two different evaluation settings, we use the average\\nof pm-0 and pm-10 accuracy.', 'A ResNet-101 makes 5 times as many mistakes when eval-\\nuated on these natural distribution shifts compared to the\\nImageNet validation set. Encouragingly however, Taori et al.\\n(2020) find that accuracy under distribution shift increases\\npredictably with ImageNet accuracy and is well modeled\\nas a linear function of logit-transformed accuracy. Taori\\net al. (2020) use this finding to propose that robustness\\nanalysis should distinguish between effective and relative\\nrobustness. Effective robustness measures improvements\\nin accuracy under distribution shift above what is predicted\\nby the documented relationship between in-distribution and\\nout-of-distribution accuracy. Relative robustness captures\\nany improvement in out-of-distribution accuracy. Taori et al.\\n(2020) argue that robustness techniques should aim to im-\\nprove both effective robustness and relative robustness.', 'Almost all models studied in Taori et al. (2020) are trained\\n3We refer readers to Hendrycks et al. (2020a) for additional\\n\\nexperiments and discussion on this claim.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 14', \"65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\\nLinear probe average over Kornblith et al.'s 12 datasets\\n\\n65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\", 'S\\nco\\n\\nre\\n (%\\n\\n)\\n\\nLinear probe average over 26 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet', 'Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.', 'or fine-tuned on the ImageNet dataset. Returning to the\\ndiscussion in the introduction to this section - is training\\nor adapting to the ImageNet dataset distribution the cause\\nof the observed robustness gap? Intuitively, a zero-shot\\nmodel should not be able to exploit spurious correlations\\nor patterns that hold only on a specific distribution, since it\\nis not trained on that distribution. 4 Thus it is reasonable\\nto expect zero-shot models to have much higher effective\\nrobustness. In Figure 13, we compare the performance of\\nzero-shot CLIP with existing ImageNet models on natural\\ndistribution shifts. All zero-shot CLIP models improve\\neffective robustness by a large amount and reduce the size\\nof the gap between ImageNet accuracy and accuracy under\\ndistribution shift by up to 75%.', 'While these results show that zero-shot models can be much\\nmore robust, they do not necessarily mean that supervised\\nlearning on ImageNet causes a robustness gap. Other details\\nof CLIP, such as its large and diverse pre-training dataset\\nor use of natural language supervision could also result', '4We caution that a zero-shot model can still exploit spurious\\ncorrelations that are shared between the pre-training and evaluation\\ndistributions.', 'in much more robust models regardless of whether they\\nare zero-shot or fine-tuned. As an initial experiment to\\npotentially begin narrowing this down, we also measure\\nhow the performance of CLIP models change after adapting\\nto the ImageNet distribution via a L2 regularized logistic\\nregression classifier fit to CLIP features on the ImageNet\\ntraining set. We visualize how performance changes from\\nthe zero-shot classifier in Figure 14. Although adapting\\nCLIP to the ImageNet distribution increases its ImageNet\\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\\nof the 2018 SOTA from Mahajan et al. (2018), average\\naccuracy under distribution shift slightly decreases.', 'It is surprising to see a 9.2% increase in accuracy, which cor-\\nresponds to roughly 3 years of improvement in SOTA, fail\\nto translate into any improvement in average performance\\nunder distribution shift. We also break down the differences\\nbetween zero-shot accuracy and linear classifier accuracy\\nper dataset in Figure 14 and find performance still increases\\nsignificantly on one dataset, ImageNetV2. ImageNetV2\\nclosely followed the creation process of the original Ima-\\ngeNet dataset which suggests that gains in accuracy from\\nsupervised adaptation are closely concentrated around the\\nImageNet distribution. Performance decreases by 4.7% on', 'Learning Transferable Visual Models From Natural Language Supervision 15\\n\\n65 70 75 80 85 90 95 100\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n\\n100\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut', 'd\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\nIdeal robust model (y = x)\\nZero-Shot CLIP\\nStandard ImageNet training\\nExisiting robustness techniques ImageNet\\n\\nImageNetV2\\n\\nImageNet-A\\n\\nImageNet-R\\n\\nObjectNet\\n\\nImageNet \\nSketch\\n\\n76.2 76.2', '76.2 76.2\\n\\n64.3 70.1\\n\\n2.7 77.1\\n\\n37.7 88.9\\n\\n32.6 72.3\\n\\n25.2 60.2\\n\\nImageNet\\nResNet101\\n\\nZero-Shot\\nCLIP\\n\\n0%\\n\\n+5.8%\\n\\n+74.4%\\n\\n+51.2%\\n\\n+39.7%\\n\\n+35.0%\\n\\nΔ ScoreDataset Examples', 'Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\\nthis “robustness gap” by up to 75%. Linear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals.\\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\\nset, ResNet-101.', 'ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\\nand 1.9% on ImageNet-A. The change in accuracy on the\\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\\nsignificant.', 'How is it possible to improve accuracy by 9.2% on the Im-\\nageNet dataset with little to no increase in accuracy under\\ndistribution shift? Is the gain primarily from “exploiting\\nspurious correlations”? Is this behavior unique to some com-\\nbination of CLIP, the ImageNet datatset, and the distribution\\nshifts studied, or a more general phenomena? Does it hold\\nfor end-to-end finetuning as well as linear classifiers? We\\ndo not have confident answers to these questions at this time.\\nPrior work has also pre-trained models on distributions other\\nthan ImageNet, but it is common to study and release mod-\\nels only after they have been fine-tuned to ImageNet. As a\\nstep towards understanding whether pre-trained zero-shot\\nmodels consistently have higher effective robustness than\\nfine-tuned models, we encourage the authors of Mahajan\\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\\n(2020) to, if possible, study these questions on their models\\nas well.', 'We also investigate another robustness intervention enabled\\nby flexible zero-shot natural-language-based image classi-\\nfiers. The target classes across the 7 transfer datasets are\\nnot always perfectly aligned with those of ImageNet. Two\\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\\nclasses of ImageNet. This presents a problem when trying\\nto use the fixed 1000-way classifier of an ImageNet model\\nto make predictions. Taori et al. (2020) handle this by max-', 'pooling predictions across all sub-classes according to the\\nImageNet class hierarchy. Sometimes this mapping is much\\nless than perfect. For the person class in Youtube-BB, pre-\\ndictions are made by pooling over the ImageNet classes for\\na baseball player, a bridegroom, and a scuba diver. With\\nCLIP we can instead generate a custom zero-shot classi-\\nfier for each dataset directly based on its class names. In\\nFigure 14 we see that this improves average effective ro-\\nbustness by 5% but is concentrated in large improvements\\non only a few datasets. Curiously, accuracy on ObjectNet\\nalso increases by 2.3%. Although the dataset was designed\\nto closely overlap with ImageNet classes, using the names\\nprovided for each class by ObjectNet’s creators still helps a\\nsmall amount compared to using ImageNet class names and\\npooling predictions when necessary.', 'While zero-shot CLIP improves effective robustness, Figure\\n14 shows that the benefit is almost entirely gone in a fully\\nsupervised setting. To better understand this difference, we\\ninvestigate how effective robustness changes on the contin-\\nuum from zero-shot to fully supervised. In Figure 15 we\\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\\n..., 128-shot, and fully supervised logistic regression classi-\\nfiers on the best CLIP model’s features. We see that while\\nfew-shot models also show higher effective robustness than\\nexisting models, this benefit fades as in-distribution per-\\nformance increases with more training data and is mostly,\\nthough not entirely, gone for the fully supervised model.\\nAdditionally, zero-shot CLIP is notably more robust than\\na few-shot model with equivalent ImageNet performance.', 'Learning Transferable Visual Models From Natural Language Supervision 16\\n\\n70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift', 'io\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\nAdapt to class shift\\n\\nAdapt to ImageNet', 'Adapt to ImageNet\\n\\nIdeal robust model (y = x)\\nAdaptive Zero-Shot CLIP\\nImageNet Zero-Shot CLIP\\nLogistic Regression CLIP\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data', '10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\nImageNet-R-4.7\\nObjectNet-3.8\\nImageNet Sketch-2.8\\nImageNet-A-1.9\\nImageNet Vid-0.5\\n+0.6Youtube-BB\\n\\n+5.8ImageNetV2\\n+9.2ImageNet\\n\\nAdapt to ImageNet', 'Adapt to ImageNet\\n\\n10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\n0ImageNet\\n0ImageNetV2\\n0ImageNet-A\\n0ImageNet-R\\n0ImageNet Sketch\\n\\n+2.3ObjectNet\\n+8.3ImageNet Vid\\n\\n+26.9Youtube-BB\\nAdapt to class shift', 'Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier\\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot\\nclassifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\\nwith ImageNet categories.', 'Across our experiments, high effective robustness seems to\\nresult from minimizing the amount of distribution specific\\ntraining data a model has access to, but this comes at a cost\\nof reducing dataset-specific performance.', 'Taken together, these results suggest that the recent shift\\ntowards large-scale task and dataset agnostic pre-training\\ncombined with a reorientation towards zero-shot and few-\\nshot benchmarking on broad evaluation suites (as advocated\\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\\ndevelopment of more robust systems and provides a more\\naccurate assessment of performance. We are curious to see\\nif the same results hold for zero-shot models in the field\\nof NLP such as the GPT family. While Hendrycks et al.\\n(2020b) has reported that pre-training improves relative ro-\\nbustness on sentiment analysis, Miller et al. (2020)’s study\\nof the robustness of question answering models under nat-\\nural distribution shift finds, similar to Taori et al. (2020),\\nlittle evidence of effective robustness improvements to date.', '4. Comparison to Human Performance\\nHow does CLIP compare to human performance and human\\nlearning? To get a better understanding of how well humans\\nperform in similar evaluation settings to CLIP, we evaluated', 'humans on one of our tasks. We wanted to get a sense of\\nhow strong human zero-shot performance is at these tasks,\\nand how much human performance is improved if they are\\nshown one or two image samples. This can help us to\\ncompare task difficulty for humans and CLIP, and identify\\ncorrelations and differences between them.', 'We had five different humans look at each of 3669 images\\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\\n2012) and select which of the 37 cat or dog breeds best\\nmatched the image (or ‘I don’t know’ if they were com-\\npletely uncertain). In the zero-shot case the humans were\\ngiven no examples of the breeds and asked to label them\\nto the best of their ability without an internet search. In\\nthe one-shot experiment the humans were given one sample\\nimage of each breed and in the two-shot experiment they\\nwere given two sample images of each breed.5', 'One possible concern was that the human workers were not\\nsufficiently motivated in the zero-shot task. High human\\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)', '5There is not a perfect correspondence between the human\\nfew-shot tasks and the model’s few-shot performance since the\\nmodel cannot refer to sample images in the way that the humans\\ncan.', 'Learning Transferable Visual Models From Natural Language Supervision 17\\n\\n65 70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift', 'io\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\n1 shot\\n\\n2 shot\\n\\n4 shot\\n\\n8 shot\\n\\n16 shot\\n\\n32\\n64\\n\\n128\\nall0 shot', '32\\n64\\n\\n128\\nall0 shot\\n\\nIdeal robust model (y = x)\\nFew-Shot CLIP (best model)\\nZero-Shot CLIP (best model)\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data', 'Figure 15. Few-shot CLIP also increases effective robustness\\ncompared to existing ImageNet models but is less robust than\\nzero-shot CLIP. Minimizing the amount of ImageNet training\\ndata used for adaption increases effective robustness at the cost of\\ndecreasing relative robustness. 16-shot logistic regression CLIP\\nmatches zero-shot CLIP on ImageNet, as previously reported in\\nFigure 7, but is less robust.', 'and 97-100% accuracy on the subset of attention check\\nimages increased our trust in the human workers.', 'Interestingly, humans went from a performance average of\\n54% to 76% with just one training example per class, and\\nthe marginal gain from an additional training example is\\nminimal. The gain in accuracy going from zero to one shot\\nis almost entirely on images that humans were uncertain\\nabout. This suggests that humans “know what they don’t\\nknow” and are able to update their priors on the images they\\nare most uncertain in based on a single example. Given this,\\nit seems that while CLIP is a promising training strategy\\nfor zero-shot performance (Figure 5) and does well on tests\\nof natural distribution shift (Figure 13), there is a large\\ndifference between how humans learn from a few examples\\nand the few-shot methods in this paper.', 'This suggests that there are still algorithmic improvements\\nwaiting to be made to decrease the gap between machine\\nand human sample efficiency, as noted by Lake et al. (2016)\\nand others. Because these few-shot evaluations of CLIP\\ndon’t make effective use of prior knowledge and the humans\\ndo, we speculate that finding a method to properly integrate\\nprior knowledge into few-shot learning is an important step\\nin algorithmic improvements to CLIP. To our knowledge,\\nusing a linear classifier on top of the features of a high-', 'Accuracy Majority Vote\\non Full Dataset\\n\\nAccuracy\\non Guesses\\n\\nMajority Vote\\nAccuracy\\n\\non Guesses\\n\\nZero-shot human 53.7 57.0 69.7 63.9\\nZero-shot CLIP 93.5 93.5 93.5 93.5\\nOne-shot human 75.7 80.3 78.5 81.2\\nTwo-shot human 75.7 85.0 79.2 86.1', 'Table 2. Comparison of human performance on Oxford IIT Pets.\\nAs in Parkhi et al. (2012), the metric is average per-class classifica-\\ntion accuracy. Most of the gain in performance when going from\\nthe human zero shot case to the human one shot case is on images\\nthat participants were highly uncertain on. “Guesses” refers to\\nrestricting the dataset to where participants selected an answer\\nother than “I don’t know”, the “majority vote” is taking the most\\nfrequent (exclusive of ties) answer per image.', 'quality pre-trained model is near state-of-the-art for few\\nshot learning (Tian et al., 2020), which suggests that there is\\na gap between the best few-shot machine learning methods\\nand human few-shot learning.', 'If we plot human accuracy vs CLIP’s zero shot accuracy\\n(Figure 16), we see that the hardest problems for CLIP are\\nalso hard for humans. To the extent that errors are consistent,\\nour hypothesis is that this is due to at least a two factors:\\nnoise in the dataset (including mislabeled images) and out of\\ndistribution images being hard for both humans and models.', '5. Data Overlap Analysis\\nA concern with pre-training on a very large internet dataset\\nis unintentional overlap with downstream evals. This is\\nimportant to investigate since, in a worst-case scenario, a\\ncomplete copy of an evaluation dataset could leak into the\\npre-training dataset and invalidate the evaluation as a mean-\\ningful test of generalization. One option to prevent this is to\\nidentify and remove all duplicates before training a model.\\nWhile this guarantees reporting true hold-out performance,\\nit requires knowing all possible data which a model might\\nbe evaluated on ahead of time. This has the downside of\\nlimiting the scope of benchmarking and analysis. Adding a\\nnew evaluation would require an expensive re-train or risk\\nreporting an un-quantified benefit due to overlap.', 'Instead, we document how much overlap occurs and how\\nperformance changes due to these overlaps. To do this, we\\nuse the following procedure:', '1) For each evaluation dataset, we run a duplicate detector\\n(see Appendix C) on its examples. We then manually inspect\\nthe found nearest neighbors and set a per dataset threshold\\nto keep high precision while maximizing recall. Using\\nthis threshold, we then create two new subsets, Overlap,\\nwhich contains all examples which have a similarity to a\\ntraining example above the threshold, and Clean, which', 'Learning Transferable Visual Models From Natural Language Supervision 18\\n\\npu\\ng\\n\\nsp\\nhy\\n\\nnx\\nge\\n\\nrm\\nan\\n\\n_s\\nho\\n\\nrth\\nai\\n\\nre\\nd\\n\\nsh\\nib\\n\\na_\\nin\\n\\nu\\nbe\\n\\nag\\nle\\n\\ngr\\nea\\n\\nt_\\npy\\n\\nre\\nne\\n\\nes\\nen\\n\\ngl\\nish\\n\\n_s\\net\\n\\nte\\nr\\n\\nsa\\nm\\n\\noy\\ned\\n\\nsa\\nin\\n\\nt_\\nbe\\n\\nrn\\nar\\n\\nd\\npo\\n\\nm\\ner\\n\\nan\\nia\\n\\nn\\nne', 'm\\ner\\n\\nan\\nia\\n\\nn\\nne\\n\\nwf\\nou\\n\\nnd\\nla\\n\\nnd\\nwh\\n\\nea\\nte\\n\\nn_\\nte\\n\\nrri\\ner\\n\\nsc\\not\\n\\ntis\\nh_\\n\\nte\\nrri\\n\\ner\\nyo\\n\\nrk\\nsh\\n\\nire\\n_t\\n\\ner\\nrie\\n\\nr\\nsia\\n\\nm\\nes\\n\\ne\\nm\\n\\nin\\nia\\n\\ntu\\nre\\n\\n_p\\nin\\n\\nsc\\nhe\\n\\nr\\nha\\n\\nva\\nne\\n\\nse\\nke\\n\\nes\\nho\\n\\nnd\\nbo\\n\\nm\\nba\\n\\ny\\nm\\n\\nai\\nne\\n\\n_c\\noo\\n\\nn\\nch\\n\\nih\\nua\\n\\nhu\\na\\n\\nba\\nss\\n\\net\\n_h', 'hu\\na\\n\\nba\\nss\\n\\net\\n_h\\n\\nou\\nnd\\n\\nja\\npa\\n\\nne\\nse\\n\\n_c\\nhi\\n\\nn\\nru\\n\\nss\\nia\\n\\nn_\\nbl\\n\\nue\\nam\\n\\ner\\nica\\n\\nn_\\nbu\\n\\nlld\\nog\\n\\npe\\nrs\\n\\nia\\nn\\n\\nbe\\nng\\n\\nal\\nle\\n\\non\\nbe\\n\\nrg\\ner\\n\\nab\\nys\\n\\nsin\\nia\\n\\nn\\nbo\\n\\nxe\\nr\\n\\nbr\\niti\\n\\nsh\\n_s\\n\\nho\\nrth\\n\\nai\\nr\\n\\nst\\naf\\n\\nfo\\nrd\\n\\nsh\\nire\\n\\n_b\\nul\\n\\nl_t\\ner\\n\\nrie\\nr\\n\\nam\\ner\\n\\nica\\nn_', 'rie\\nr\\n\\nam\\ner\\n\\nica\\nn_\\n\\npi\\nt_\\n\\nbu\\nll_\\n\\nte\\nrri\\n\\ner\\neg\\n\\nyp\\ntia\\n\\nn_\\nm\\n\\nau\\nbi\\n\\nrm\\nan\\n\\nen\\ngl\\n\\nish\\n_c\\n\\noc\\nke\\n\\nr_\\nsp\\n\\nan\\nie\\n\\nl\\nra\\n\\ngd\\nol\\n\\nl\\n\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n (%\\n)\\n\\nZero-Shot CLIP\\nOne-Shot Human\\nZero-Shot Human', 'Figure 16. The hardest problems for CLIP also tend to be the hard-\\nest problems for humans. Here we rank image categories by diffi-\\nculty for CLIP as measured as probability of the correct label.', 'contains all examples that are below this threshold. We\\ndenote the unaltered full dataset All for reference. From\\nthis we first record the degree of data contamination as the\\nratio of the number of examples in Overlap to the size of\\nAll.', '2) We then compute the zero-shot accuracy of CLIP\\nRN50x64 on the three splits and report All - Clean\\nas our main metric. This is the difference in accuracy due\\nto contamination. When positive it is our estimate of how\\nmuch the overall reported accuracy on the dataset was in-\\nflated by over-fitting to overlapping data.', '3) The amount of overlap is often small so we also run a\\nbinomial significance test where we use the accuracy on\\nClean as the null hypothesis and compute the one-tailed\\n(greater) p-value for the Overlap subset. We also calculate\\n99.5% Clopper-Pearson confidence intervals on Dirty as\\nanother check.', 'A summary of this analysis is presented in Figure 17. Out\\nof 35 datasets studied, 9 datasets have no detected overlap\\nat all. Most of these datasets are synthetic or specialized\\nmaking them unlikely to be posted as normal images on\\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\\nare guaranteed to have no overlap due to containing novel\\ndata from after the date our dataset was created (ObjectNet\\nand Hateful Memes). This demonstrates our detector has\\na low-false positive rate which is important as false posi-\\ntives would under-estimate the effect of contamination in', 'our analysis. There is a median overlap of 2.2% and an av-\\nerage overlap of 3.2%. Due to this small amount of overlap,\\noverall accuracy is rarely shifted by more than 0.1% with\\nonly 7 datasets above this threshold. Of these, only 2 are\\nstatistically significant after Bonferroni correction. The max\\ndetected improvement is only 0.6% on Birdsnap which has\\nthe second largest overlap at 12.1%. The largest overlap is\\nfor Country211 at 21.5%. This is due to it being constructed\\nout of YFCC100M, which our pre-training dataset contains\\na filtered subset of. Despite this large overlap there is only\\na 0.2% increase in accuracy on Country211. This may be\\nbecause the training text accompanying an example is often\\nnot related to the specific task a downstream eval measures.\\nCountry211 measures geo-localization ability, but inspect-\\ning the training text for these duplicates showed they often\\ndo not mention the location of the image.', 'We are aware of two potential concerns with our analysis.\\nFirst our detector is not perfect. While it achieves near\\n100% accuracy on its proxy training task and manual in-\\nspection + threshold tuning results in very high precision\\nwith good recall among the found nearest-neighbors, we can\\nnot tractably check its recall across 400 million examples.\\nAnother potential confounder of our analysis is that the un-\\nderlying data distribution may shift between the Overlap\\nand Clean subsets. For example, on Kinetics-700 many\\n“overlaps” are in fact all black transition frames. This ex-\\nplains why Kinetics-700 has an apparent 20% accuracy drop\\non Overlap. We suspect more subtle distribution shifts\\nlikely exist. One possibility we noticed on CIFAR-100 is\\nthat, due to the very low resolution of its images, many\\nduplicates were false positives of small objects such as birds\\nor planes. Changes in accuracy could instead be due to\\nchanges in the class distribution or difficulty of the dupli-\\ncates. Unfortunately, these distribution and difficulty shifts\\ncould also mask the effects of over-fitting.', 'However, these results closely follow the findings of simi-\\nlar duplicate analysis in previous work on large scale pre-\\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\\ndetected similar overlap rates and found minimal changes in\\noverall performance. Importantly, Kolesnikov et al. (2019)\\nalso compared the alternative de-duplication strategy dis-\\ncussed in the introduction to this section with the approach\\nwe settled on and observed little difference between the two\\napproaches.', '6. Limitations\\nThere are still many limitations to CLIP. While several of\\nthese are discussed as part of analysis in various sections,\\nwe summarize and collect them here.', 'On datasets with training splits, the performance of zero-\\nshot CLIP is on average competitive with the simple su-\\n\\nLearning Transferable Visual Models From Natural Language Supervision 19', '0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-20\\n\\n-10\\n\\n0\\n\\n10\\n\\n20\\n\\nDi\\nffe\\n\\nre\\nnc\\n\\ne \\nin\\n\\n A\\ncc\\n\\nur\\nac\\n\\ny \\non\\n\\n O\\nve\\n\\nrla\\npp\\n\\nin\\ng \\n\\nvs\\n. C\\n\\nle\\nan\\n\\n D\\nat\\n\\na \\n(%\\n\\n)\\n\\nSUN397\\n\\nCIFAR-100\\n\\nImageNet Sketch\\n\\nSUN\\n\\nKinetics-700', 'SUN\\n\\nKinetics-700\\n\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-0.75\\n\\n-0.5\\n\\n-0.25\\n\\n0\\n\\n0.25\\n\\n0.5\\n\\n0.75\\n\\nOv\\ner\\n\\nal\\nl A\\n\\ncc\\nur\\n\\nac\\ny \\n\\nCh\\nan\\n\\nge\\n D\\n\\nue\\n T\\n\\no \\nOv\\n\\ner\\nla\\n\\np \\n(%\\n\\n)\\n\\nStanford CarsSUN397\\n\\nBirdsnap\\nCIFAR-100\\n\\nFER2013', 'FER2013\\n\\nCountry211\\nSUN\\n\\np < 1e-3\\np < 0.05\\np > 0.05', 'Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have\\nup to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\\n99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\\nimprovements statistically significant when calculated using a one-sided binomial test.', 'pervised baseline of a linear classifier on top of ResNet-50\\nfeatures. On most of these datasets, the performance of\\nthis baseline is now well below the overall state of the art.\\nSignificant work is still needed to improve the task learning\\nand transfer capabilities of CLIP. While scaling has so far\\nsteadily improved performance and suggests a route for con-\\ntinued improvement, we estimate around a 1000x increase\\nin compute is required for zero-shot CLIP to reach overall\\nstate-of-the-art performance. This is infeasible to train with\\ncurrent hardware. Further research into improving upon the\\ncomputational and data efficiency of CLIP will be necessary.', 'Analysis in Section 3.1 found that CLIP’s zero-shot perfor-\\nmance is still quite weak on several kinds of tasks. When\\ncompared to task-specific models, the performance of CLIP\\nis poor on several types of fine-grained classification such\\nas differentiating models of cars, species of flowers, and\\nvariants of aircraft. CLIP also struggles with more abstract\\nand systematic tasks such as counting the number of objects\\nin an image. Finally for novel tasks which are unlikely to be\\nincluded in CLIP’s pre-training dataset, such as classifying\\nthe distance to the nearest car in a photo, CLIP’s perfor-\\nmance can be near random. We are confident that there are\\nstill many, many, tasks where CLIP’s zero-shot performance\\nis near chance level.', 'While zero-shot CLIP generalizes well to many natural im-\\nage distributions as investigated in Section 3.3, we’ve ob-\\nserved that zero-shot CLIP still generalizes poorly to data\\nthat is truly out-of-distribution for it. An illustrative exam-\\nple occurs for the task of OCR as reported in Appendix E.', 'CLIP learns a high quality semantic OCR representation that\\nperforms well on digitally rendered text, which is common\\nin its pre-training dataset, as evidenced by performance on\\nRendered SST2. However, CLIP only achieves 88% accu-\\nracy on the handwritten digits of MNIST. An embarrassingly\\nsimple baseline of logistic regression on raw pixels outper-\\nforms zero-shot CLIP. Both semantic and near-duplicate\\nnearest-neighbor retrieval verify that there are almost no im-\\nages that resemble MNIST digits in our pre-training dataset.\\nThis suggests CLIP does little to address the underlying\\nproblem of brittle generalization of deep learning models.\\nInstead CLIP tries to circumvent the problem and hopes that\\nby training on such a large and varied dataset that all data\\nwill be effectively in-distribution. This is a naive assumption\\nthat, as MNIST demonstrates, is easy to violate.', 'Although CLIP can flexibly generate zero-shot classifiers\\nfor a wide variety of tasks and datasets, CLIP is still limited\\nto choosing from only those concepts in a given zero-shot\\nclassifier. This is a significant restriction compared to a\\ntruly flexible approach like image captioning which could\\ngenerate novel outputs. Unfortunately, as described in Sec-\\ntion 2.3 we found the computational efficiency of the image\\ncaption baseline we tried to be much lower than CLIP. A\\nsimple idea worth trying is joint training of a contrastive\\nand generative objective with the hope of combining the\\nefficiency of CLIP with the flexibility of a caption model.\\nAs another alternative, search could be performed at infer-\\nence time over many natural language explanations of a\\ngiven image, similar to approach proposed in Learning with\\nLatent Language Andreas et al. (2017).', 'Learning Transferable Visual Models From Natural Language Supervision 20', 'CLIP also does not address the poor data efficiency of deep\\nlearning. Instead CLIP compensates by using a source of\\nsupervision that can be scaled to hundreds of millions of\\ntraining examples. If every image seen during training of\\na CLIP model was presented at a rate of one per second,\\nit would take 405 years to iterate through the 12.8 billion\\nimages seen over 32 training epochs. Combining CLIP\\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\\nself-training (Lee; Xie et al., 2020) methods is a promising\\ndirection given their demonstrated ability to improve data\\nefficiency over standard supervised learning.', 'Our methodology has several significant limitations. De-\\nspite our focus on zero-shot transfer, we repeatedly queried\\nperformance on full validation sets to guide the develop-\\nment of CLIP. These validation sets often have thousands\\nof examples, which is unrealistic for true zero-shot sce-\\nnarios. Similar concerns have been raised in the field of\\nsemi-supervised learning (Oliver et al., 2018). Another po-\\ntential issue is our selection of evaluation datasets. While\\nwe have reported results on Kornblith et al. (2019)’s 12\\ndataset evaluation suite as a standardized collection, our\\nmain results use a somewhat haphazardly assembled col-\\nlection of 27 datasets that is undeniably co-adapted with\\nthe development and capabilities of CLIP. Creating a new\\nbenchmark of tasks designed explicitly to evaluate broad\\nzero-shot transfer capabilities, rather than re-using existing\\nsupervised datasets, would help address these issues.', 'CLIP is trained on text paired with images on the internet.\\nThese image-text pairs are unfiltered and uncurated and\\nresult in CLIP models learning many social biases. This\\nhas been previously demonstrated for image caption models\\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\\nfor detailed analysis and quantification of these behaviors for\\nCLIP as well as discussion of potential mitigation strategies.', 'While we have emphasized throughout this work that speci-\\nfying image classifiers through natural language is a flexible\\nand general interface, it has its own limitations. Many com-\\nplex tasks and visual concepts can be difficult to specify\\njust through text. Actual training examples are undeniably\\nuseful but CLIP does not optimize for few-shot performance\\ndirectly. In our work, we fall back to fitting linear classifiers\\non top of CLIP’s features. This results in a counter-intuitive\\ndrop in performance when transitioning from a zero-shot\\nto a few-shot setting. As discussed in Section 4, this is\\nnotably different from human performance which shows a\\nlarge increase from a zero to a one shot setting. Future work\\nis needed to develop methods that combine CLIP’s strong\\nzero-shot performance with efficient few-shot learning.', '7. Broader Impacts\\nCLIP has a wide range of capabilities due to its ability to\\ncarry out arbitrary image classification tasks. One can give\\nit images of cats and dogs and ask it to classify cats, or give\\nit images taken in a department store and ask it to classify\\nshoplifters–a task with significant social implications and\\nfor which AI may be unfit. Like any image classification\\nsystem, CLIP’s performance and fitness for purpose need to\\nbe evaluated, and its broader impacts analyzed in context.\\nCLIP also introduces a capability that will magnify and alter\\nsuch issues: CLIP makes it possible to easily create your\\nown classes for categorization (to ‘roll your own classifier’)\\nwithout a need for re-training. This capability introduces\\nchallenges similar to those found in characterizing other,\\nlarge-scale generative models like GPT-3 (Brown et al.,\\n2020); models that exhibit non-trivial zero-shot (or few-\\nshot) generalization can have a vast range of capabilities,\\nmany of which are made clear only after testing for them.', 'Our studies of CLIP in a zero-shot setting show that the\\nmodel displays significant promise for widely-applicable\\ntasks like image retrieval or search. For example, it can find\\nrelevant images in a database given text, or relevant text\\ngiven an image. Further, the relative ease of steering CLIP\\ntoward bespoke applications with little or no additional data\\nor training could unlock a variety of novel applications that\\nare hard for us to envision today, as has occurred with large\\nlanguage models over the past few years.', 'In addition to the more than 30 datasets studied in earlier\\nsections of this paper, we evaluate CLIP’s performance on\\nthe FairFace benchmark and undertake exploratory bias\\nprobes. We then characterize the model’s performance in\\na downstream task, surveillance, and discuss its usefulness\\nas compared with other available systems. Many of CLIP’s\\ncapabilities are omni-use in nature (e.g. OCR can be used\\nto make scanned documents searchable, to power screen\\nreading technologies, or to read license plates). Several\\nof the capabilities measured, from action recognition, ob-\\nject classification, and geo-localization, to facial emotion\\nrecognition, can be used in surveillance. Given its social\\nimplications, we address this domain of use specifically in\\nthe Surveillance section.', 'We have also sought to characterize the social biases inher-\\nent to the model. Our bias tests represent our initial efforts\\nto probe aspects of how the model responds in different sce-\\nnarios, and are by nature limited in scope. CLIP and models\\nlike it will need to be analyzed in relation to their specific\\ndeployments to understand how bias manifests and iden-\\ntify potential interventions. Further community exploration\\nwill be required to develop broader, more contextual, and\\nmore robust testing schemes so that AI developers can bet-\\nter characterize biases in general purpose computer vision\\nmodels.', 'Learning Transferable Visual Models From Natural Language Supervision 21\\n\\nModel Race Gender Age\\n\\nFairFace Model 93.7 94.2 59.7\\nLinear Probe CLIP 93.4 96.5 63.8\\nZero-Shot CLIP 58.3 95.9 57.1\\nLinear Probe Instagram 90.8 93.2 54.2', 'Table 3. Percent accuracy on Race, Gender, and Age classification\\nof images in FairFace category ‘White’\\n\\nModel Race Gender Age', 'FairFace Model 75.4 94.4 60.7\\nLinear Probe CLIP 92.8 97.7 63.1\\nZero-Shot CLIP 91.3 97.2 54.3\\nLinear Probe Instagram 87.2 93.9 54.1', 'Table 4. Percent accuracy on Race, Gender, and Age classification\\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\\ngether as FairFace category ‘Non-White’)', 'Middle Southeast East\\nModel Gender Black White Indian Latino Eastern Asian Asian Average\\n\\nMale 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\\nLinear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\\n\\n97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5', 'Male 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\\nZero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\\n\\n96.7 95.9 98.0 97.5 98.0 96.3 96.6', 'Male 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\\nLinear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\\n\\n91.3 93.2 95.6 94.0 95.6 93.4 93.9\\n\\nTable 5. Percent accuracy on gender classification of images by FairFace race category\\n\\n7.1. Bias', 'Algorithmic decisions, training data, and choices about how\\nclasses are defined and taxonomized (which we refer to in-\\nformally as “class design”) can all contribute to and amplify\\nsocial biases and inequalities resulting from the use of AI\\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\\n& Star, 2000). Class design is particularly relevant to mod-\\nels like CLIP, since any developer can define a class and the\\nmodel will provide some result.', 'In this section, we provide preliminary analysis of some\\nof the biases in CLIP, using bias probes inspired by those\\noutlined in Buolamwini & Gebru (2018) and Kärkkäinen\\n& Joo (2019). We also conduct exploratory bias research\\nintended to find specific examples of biases in the model,\\nsimilar to that conducted by Solaiman et al. (2019).', 'We start by analyzing the performance of Zero-Shot CLIP on\\nthe face image dataset FairFace (Kärkkäinen & Joo, 2019)6', '6FairFace is a face image dataset designed to balance age, gen-\\nder, and race, in order to reduce asymmetries common in previous\\nface datasets. It categorizes gender into 2 groups: female and male\\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\\nAsian, Middle Eastern, and Latino. There are inherent problems\\nwith race and gender classifications, as e.g. Bowker & Star (2000)', 'as an initial bias probe, then probe the model further to\\nsurface additional biases and sources of biases, including\\nclass design.', 'We evaluated two versions of CLIP on the FairFace dataset:\\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\\nsion classifier fitted to FairFace’s dataset on top of CLIP’s\\nfeatures (“LR CLIP”). We find that LR CLIP gets higher\\naccuracy on the FairFace dataset than both the ResNext-101\\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\\nhajan et al., 2018) and FairFace’s own model on most of the\\nclassification tests we ran7. ZS CLIP’s performance varies\\nby category and is worse than that of FairFace’s model for a\\nfew categories, and better for others. (See Table 3 and Table\\n4).', 'and Keyes (2018) have shown. While FairFace’s dataset reduces\\nthe proportion of White faces, it still lacks representation of entire\\nlarge demographic groups, effectively erasing such categories. We\\nuse the 2 gender categories and 7 race categories defined in the\\nFairFace dataset in a number of our experiments not in order to\\nreinforce or endorse the use of such reductive categories, but in\\norder to enable us to make comparisons to prior work.', '7One challenge with this comparison is that the FairFace model\\nuses binary classes for race (“White” and “Non-White”), instead\\nof breaking down races into finer-grained sub-groups.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 22', 'Middle Southeast East\\nCategory Black White Indian Latino Eastern Asian Asian\\n\\nCrime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\\nNon-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0', 'Table 6. Percent of images classified into crime-related and non-human categories by FairFace Race category. The label set included 7\\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.', 'Category Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\\n\\nDefault Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\\nDefault Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4', 'Table 7. Percent of images classified into crime-related and non-human categories by FairFace Age category, showing comparison between\\nresults obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.', 'Additionally, we test the performance of the LR CLIP and\\nZS CLIP models across intersectional race and gender cate-\\ngories as they are defined in the FairFace dataset. We find\\nthat model performance on gender classification is above\\n95% for all race categories. Table 5 summarizes these re-\\nsults.', 'While LR CLIP achieves higher accuracy than the Linear\\nProbe Instagram model on the FairFace benchmark dataset\\nfor gender, race and age classification of images by intersec-\\ntional categories, accuracy on benchmarks offers only one\\napproximation of algorithmic fairness, as Raji et al. (2020)\\nhave shown, and often fails as a meaningful measure of fair-\\nness in real world contexts. Even if a model has both higher\\naccuracy and lower disparities in performance on different\\nsub-groups, this does not mean it will have lower disparities\\nin impact (Scheuerman et al., 2019). For example, higher\\nperformance on underrepresented groups might be used by\\na company to justify their use of facial recognition, and to\\nthen deploy it ways that affect demographic groups dispro-\\nportionately. Our use of facial classification benchmarks to\\nprobe for biases is not intended to imply that facial classi-\\nfication is an unproblematic task, nor to endorse the use of\\nrace, age, or gender classification in deployed contexts.', 'We also probed the model using classification terms with\\nhigh potential to cause representational harm, focusing on\\ndenigration harms in particular (Crawford, 2017). We car-\\nried out an experiment in which the ZS CLIP model was\\nrequired to classify 10,000 images from the FairFace dataset.\\nIn addition to the FairFace classes, we added in the follow-\\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\\n‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\\nexperiment was to check if harms of denigration dispropor-\\ntionately impact certain demographic subgroups.', 'We found that 4.9% (confidence intervals between 4.6%\\nand 5.4%) of the images were misclassified into one of\\nthe non-human classes we used in our probes (‘animal’,\\n‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\\nimages had the highest misclassification rate (approximately\\n14%; confidence intervals between [12.6% and 16.4%])\\nwhile all other races had misclassification rates under 8%.\\nPeople aged 0-20 years had the highest proportion being\\nclassified into this category at 14% .', 'We also found that 16.5% of male images were misclassified\\ninto classes related to crime (‘thief’, ‘suspicious person’ and\\n‘criminal’) as compared to 9.8% of female images. Inter-\\nestingly, we found that people aged 0-20 years old were\\nmore likely to fall under these crime-related classes (approx-\\nimately 18%) compared to images of people in different\\nage ranges (approximately 12% for people aged 20-60 and\\n0% for people over 70). We found significant disparities in\\nclassifications across races for crime related terms, which is\\ncaptured in Table 6.', 'Given that we observed that people under 20 were the most\\nlikely to be classified in both the crime-related and non-\\nhuman animal categories, we carried out classification for\\nthe images with the same classes but with an additional\\ncategory ‘child’ added to the categories. Our goal here\\nwas to see if this category would significantly change the\\nbehaviour of the model and shift how the denigration harms\\nare distributed by age. We found that this drastically reduced\\nthe number of images of people under 20 classified in either\\ncrime-related categories or non-human animal categories\\n(Table 7). This points to how class design has the potential\\nto be a key factor determining both the model performance\\nand the unwanted biases or behaviour the model may exhibit\\nwhile also asks overarching questions about the use of face', 'Learning Transferable Visual Models From Natural Language Supervision 23\\n\\nimages to automatically classify people along such lines\\n(y Arcas et al., 2017).', 'The results of these probes can change based on the class\\ncategories one chooses to include as well as the specific\\nlanguage one uses to describe each class. Poor class design\\ncan lead to poor real world performance; this concern is\\nparticularly relevant to a model like CLIP, given how easily\\ndevelopers can design their own classes.', 'We also carried out experiments similar to those outlined by\\nSchwemmer et al. (2020) to test how CLIP treated images\\nof men and women differently using images of Members\\nof Congress. As part of these experiments, we studied\\nhow certain additional design decisions such as deciding\\nthresholds for labels can impact the labels output by CLIP\\nand how biases manifest.', 'We carried out three experiments - we tested for accuracy\\non gender classification and we tested for how labels were\\ndifferentially distributed across two different label sets. For\\nour first label set, we used a label set of 300 occupations and\\nfor our second label set we used a combined set of labels that\\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\\nAzure Computer Vision returned for all the images.', 'We first simply looked into gender prediction performance\\nof the model on the images of Members of Congress, in\\norder to check to see if the model correctly recognized\\nmen as men and women as women given the image of a\\nperson who appeared to be in an official setting/position of\\npower. We found that the model got 100% accuracy on the\\nimages. This is slightly better performance than the model’s\\nperformance on the FairFace dataset. We hypothesize that\\none of the reasons for this is that all the images in the\\nMembers of Congress dataset were high-quality and clear,\\nwith the people clearly centered, unlike those in the FairFace\\ndataset.', 'In order to study how the biases in returned labels depend on\\nthe thresholds set for label probability, we did an experiment\\nin which we set threshold values at 0.5% and 4.0%. We\\nfound that the lower threshold led to lower quality of labels.\\nHowever, even the differing distributions of labels under\\nthis threshold can hold signals for bias. For example, we\\nfind that under the 0.5% threshold labels such as ‘nanny’\\nand ‘housekeeper’ start appearing for women whereas labels\\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\\nThis points to gendered associations similar to those that\\nhave previously been found for occupations (Schwemmer\\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).', 'At the higher 4% threshold, the labels with the highest prob-\\nability across both genders include “lawmaker”, “legislator”\\nand “congressman”. However, the presence of these biases\\namongst lower probability labels nonetheless point to larger\\nquestions about what ‘sufficiently’ safe behaviour may look', 'like for deploying such systems.', 'When given the combined set of labels that Google Cloud\\nVision (GCV), Amazon Rekognition and Microsoft returned\\nfor all the images, similar to the biases Schwemmer et al.\\n(2020) found in GCV systems, we found our system also\\ndisproportionately attached labels to do with hair and ap-\\npearance in general to women more than men. For ex-\\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\\nappeared significantly more often for women. Additionally,\\nCLIP attached some labels that described high status occu-\\npations disproportionately more often to men such as ‘ex-\\necutive’ and ‘doctor’. Out of the only four occupations that\\nit attached more often to women, three were ‘newscaster’,\\n‘television presenter’ and ‘newsreader’ and the fourth was\\n‘Judge’. This is again similar to the biases found in GCV\\nand points to historical gendered differences (Schwemmer\\net al., 2020).', 'Interestingly, when we lowered the threshold to 0.5% for\\nthis set of labels, we found that the labels disproportionately\\ndescribing men also shifted to appearance oriented words\\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\\ntion oriented words such as ‘military person’ and ‘executive’\\n- which were not used to describe images of women at the\\nhigher 4% threshold - were used for both men and women\\nat the lower 0.5% threshold, which could have caused the\\nchange in labels for men. The reverse was not true. Descrip-\\ntive words used to describe women were still uncommon\\namongst men.', 'Design decisions at every stage of building a model impact\\nhow biases manifest and this is especially true for CLIP\\ngiven the flexibility it offers. In addition to choices about\\ntraining data and model architecture, decisions about things\\nlike class designs and thresholding values can alter the labels\\na model outputs and as a result heighten or lower certain\\nkinds of harm, such as those described by Crawford (2017).\\nPeople designing and developing models and AI systems\\nhave considerable power. Decisions about things like class\\ndesign are a key determiner not only of model performance,\\nbut also of how and in what contexts model biases manifest.', 'These experiments are not comprehensive. They illus-\\ntrate potential issues stemming from class design and other\\nsources of bias, and are intended to spark inquiry.\\n\\n7.2. Surveillance', 'We next sought to characterize model performance in re-\\nlation to a downstream task for which there is significant\\nsocietal sensitivity: surveillance. Our analysis aims to better\\nembody the characterization approach described above and\\nto help orient the research community towards the potential\\nfuture impacts of increasingly general purpose computer\\nvision models and aid the development of norms and checks', 'Learning Transferable Visual Models From Natural Language Supervision 24\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nblouse\\npurple\\n\\nnewsreader\\nbangs\\n\\npink\\npixie cut\\n\\nblack hair\\nbob cut\\n\\nmagenta\\nhot\\n\\nlaughing\\nblazer\\n\\nspokesperson\\nblonde', 'spokesperson\\nblonde\\n\\npublic speaking\\nsenior citizen\\n\\nlooking\\nfemale\\n\\nlady\\nwoman\\n\\nTop labels,\\nimages of women\\n\\nWomen\\nMen\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nyellow\\nnecktie\\n\\nkid\\nfrown\\n\\nshoulder\\ntie\\n\\ndisplay\\nelder\\n\\nphotograph\\nwalking\\n\\nmilitary officer\\nphoto', 'suit\\nfacial expression\\n\\nhead\\nblack\\n\\nplayer\\nface\\n\\nmale\\nman\\n\\nTop labels,\\nimages of men\\n\\nWomen\\nMen', 'Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\\nidentified with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\\ncertain label by gender.', 'around such systems. Our inclusion of surveillance is not\\nintended to indicate enthusiasm for this domain - rather, we\\nthink surveillance is an important domain to try to make\\npredictions about given its societal implications (Zuboff,\\n2015; Browne, 2015).', 'We measure the model’s performance on classification of\\nimages from CCTV cameras and zero-shot celebrity identifi-\\ncation. We first tested model performance on low-resolution\\nimages captured from surveillance cameras (e.g. CCTV\\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\\ndata captured by Varadarajan & Odobez (2009), which both\\nconsist of real world outdoor scenes with non-actors.', 'Given CLIP’s flexible class construction, we tested 515\\nsurveillance images captured from 12 different video se-\\nquences on self-constructed general classes for coarse and\\nfine grained classification. Coarse classification required the\\nmodel to correctly identify the main subject of the image (i.e.\\ndetermine if the image was a picture of an empty parking\\nlot, school campus, etc.). For fine-grained classification, the\\nmodel had to choose between two options constructed to\\ndetermine if the model could identify the presence/absence\\nof smaller features in the image such as a person standing\\nin the corner.', 'For coarse classification, we constructed the classes by hand-\\ncaptioning the images ourselves to describe the contents\\nof the image and there were always at least 6 options for', 'the model to choose from. Additionally, we carried out a\\n‘stress test’ where the class set included at least one more\\ncaption for something that was ‘close’ to the image (for\\nexample, ‘parking lot with white car’ vs. ‘parking lot with\\nred car’). We found that the model had a top-1 accuracy\\nof 91.8% on the CCTV images for the initial evaluation.\\nThe accuracy dropped significantly to 51.1% for the second\\nevaluation, with the model incorrectly choosing the ‘close’\\nanswer 40.7% of the time.', 'For fine-grained detection, the zero-shot model performed\\npoorly, with results near random. Note that this experiment\\nwas targeted only towards detecting the presence or absence\\nof small objects in image sequences.', 'We also tested CLIP’s zero-shot performance for ‘in the\\nwild’ identity detection using the CelebA dataset8. We did\\nthis to evaluate the model’s performance for identity detec-\\ntion using just the publicly available data it was pre-trained\\non. While we tested this on a dataset of celebrities who have\\na larger number of images on the internet, we hypothesize\\nthat the number of images in the pre-training data needed\\nfor the model to associate faces with names will keep de-\\ncreasing as models get more powerful (see Table 8), which\\nhas significant societal implications (Garvie, 2019). This', '8Note: The CelebA dataset is more representative of faces with\\nlighter skin tones. Due to the nature of the dataset, we were not\\nable to control for race, gender, age, etc.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 25', 'Model 100 Classes 1k Classes 2k Classes\\n\\nCLIP L/14 59.2 43.3 42.2\\nCLIP RN50x64 56.4 39.5 38.4\\nCLIP RN50x16 52.7 37.4 36.3\\nCLIP RN50x4 52.8 38.1 37.3\\n\\nTable 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy', 'mirrors recent developments in natural language processing,\\nin which recent large language models trained on Internet\\ndata often exhibit a surprising ability to provide informa-\\ntion related to relatively minor public figures (Brown et al.,\\n2020).', 'We found that the model had 59.2% top-1 accuracy out\\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\\nages. However, this performance dropped to 43.3% when\\nwe increased our class sizes to 1k celebrity names. This\\nperformance is not competitive when compared to produc-\\ntion level models such as Google’s Celebrity Recognition\\n(Google). However, what makes these results noteworthy is\\nthat this analysis was done using only zero-shot identifica-\\ntion capabilities based on names inferred from pre-training\\ndata - we didn’t use any additional task-specific dataset, and\\nso the (relatively) strong results further indicate that before\\ndeploying multimodal models, people will need to carefully\\nstudy them for behaviors in a given context and domain.', 'CLIP offers significant benefit for tasks that have relatively\\nlittle data given its zero-shot capabilities. However, large\\ndatasets and high performing supervised models exist for\\nmany in-demand surveillance tasks such as facial recogni-\\ntion. As a result, CLIP’s comparative appeal for such uses\\nis low. Additionally, CLIP is not designed for common\\nsurveillance-relevant tasks like object detection and seman-\\ntic segmentation. This means it has limited use for certain\\nsurveillance tasks when models that are designed with these\\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\\navailable.', 'However, CLIP does unlock a certain aspect of usability\\ngiven how it removes the need for training data. Thus, CLIP\\nand similar models could enable bespoke, niche surveillance\\nuse cases for which no well-tailored models or datasets exist,\\nand could lower the skill requirements to build such appli-\\ncations. As our experiments show, ZS CLIP displays non-\\ntrivial, but not exceptional, performance on a few surveil-\\nlance relevant tasks today.', '7.3. Future Work\\n\\nThis preliminary analysis is intended to illustrate some of\\nthe challenges that general purpose computer vision models\\npose and to give a glimpse into their biases and impacts.', 'We hope that this work motivates future research on the\\ncharacterization of the capabilities, shortcomings, and biases\\nof such models, and we are excited to engage with the\\nresearch community on such questions.', 'We believe one good step forward is community exploration\\nto further characterize the capabilities of models like CLIP\\nand - crucially - identify application areas where they have\\npromising performance and areas where they may have\\nreduced performance9. This process of characterization can\\nhelp researchers increase the likelihood models are used\\nbeneficially by:', '• Identifying potentially beneficial downstream uses of\\nmodels early in the research process, enabling other\\nresearchers to think about applications.', '• Surfacing tasks with significant sensitivity and a large\\nset of societal stakeholders, which may call for inter-\\nvention by policymakers.', '• Better characterizing biases in models, alerting other\\nresearchers to areas of concern and areas for interven-\\ntions.', '• Creating suites of tests to evaluate systems like CLIP\\non, so we can better characterize model capabilities\\nearlier in the development cycle.\\n\\n• Identifying potential failure modes and areas for further\\nwork.', 'We plan to contribute to this work, and hope this analysis\\nprovides some motivating examples for subsequent research.', '8. Related Work\\nAny model that leverages written, spoken, signed or any\\nother form of human language as part of its training signal\\nis arguably using natural language as a source of supervi-\\nsion. This is an admittedly extremely broad area and covers\\nmost work in the field of distributional semantics including\\ntopic models (Blei et al., 2003), word, sentence, and para-\\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\\nMikolov, 2014), and language models (Bengio et al., 2003).\\nIt also includes much of the broader field of NLP that deals\\nwith predicting or modeling sequences of natural language\\nin some way. Work in NLP intentionally leveraging natural\\nlanguage supervision in the form of explanations, feedback,\\ninstructions, and advice for tasks such as classification (as\\nopposed to the commonly used representation of supervision\\nas a set of arbitrarily encoded discrete category labels) has', '9A model could be unfit for use due to inadequate performance\\nor due to the inappropriateness of AI use in the application area\\nitself.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 26', 'been explored in many creative and advanced ways. Dialog\\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\\n2019) develops techniques to learn from interactive natural\\nlanguage feedback in dialog. Several papers have leveraged\\nsemantic parsing to convert natural language explanations\\ninto features (Srivastava et al., 2017) or additional training\\nlabels (Hancock et al., 2018). More recently, ExpBERT\\n(Murty et al., 2020) uses feature representations produced\\nby conditioning a deep contextual language model on nat-\\nural language explanations and descriptions of relations to\\nimprove performance on the task of relation extraction.', 'CLIP is an example of using natural language as a training\\nsignal for learning about a domain other than language. In\\nthis context, the earliest use of the term natural language\\nsupervision that we are aware of is the work of Ramanathan\\net al. (2013) which showed that natural language descrip-\\ntions could be used along side other sources of supervision\\nto improve performance on the task of video event under-\\nstanding. However, as mentioned in the introduction and\\napproach section, methods of leveraging natural language\\ndescriptions in computer vision well predate the use of this\\nspecific term, especially for image retrieval (Mori et al.,\\n1999) and object classification (Wang et al., 2009). Other\\nearly work leveraged tags (but not natural language) asso-\\nciated with images for the task of semantic segmentation\\n(Barnard et al., 2003). More recently, He & Peng (2017)\\nand Liang et al. (2020) demonstrated using natural language\\ndescriptions and explanations to improve fine-grained vi-\\nsual classification of birds. Others have investigated how\\ngrounded language can be used to improve visual represen-\\ntations and classifiers on the ShapeWorld dataset (Kuhnle\\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\\nFinally, techniques which combine natural language with\\nreinforcement learning environments (Narasimhan et al.,\\n2015) have demonstrated exciting emergent behaviors such\\nas systematically accomplishing zero-shot tasks (Hill et al.,\\n2019).', 'CLIP’s pre-training task optimizes for text-image retrieval.\\nThis areas of research dates back to the mid-90s with the\\npreviously mentioned Mori et al. (1999) as representative of\\nearly work. While initial efforts focused primarily on predic-\\ntive objectives over time research shifted towards learning\\njoint multi-modal embedding spaces with techniques like\\nkernel Canonical Correlation Analysis and various ranking\\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\\nHodosh et al., 2013). Over time work explored many combi-\\nnations of training objective, transfer, and more expressive\\nmodels and steadily improved performance (Frome et al.,\\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\\n2014; Faghri et al., 2017).', 'Other work has leveraged natural language supervision for\\ndomains other than images. Stroud et al. (2020) explores', 'large scale representation learning by training a system to\\npair descriptive text with videos instead of images. Several\\nworks have explored using dense spoken natural language\\nsupervision for videos (Miech et al., 2019; 2020b). When\\nconsidered together with CLIP, these works suggest that\\nlarge scale natural language supervision is a promising way\\nto learn high quality perceptual systems for many domains.\\nAlayrac et al. (2020) extended this line of work to an addi-\\ntional modality by adding raw audio as an additional super-\\nvision source and demonstrated benefits from combining all\\nthree sources of supervision.', 'As part of our work on CLIP we also construct a new dataset\\nof image-text pairs. Modern work on image-text retrieval\\nhas relied on a set of crowd-sourced sentence level im-\\nage caption evaluation datasets like Pascal1K (Rashtchian\\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\\n(Young et al., 2014). However, these datasets are still rel-\\natively small and limit achievable performance. Several\\nmethods have been proposed to create larger datasets au-\\ntomatically with Ordonez et al. (2011) as a notable early\\nexample. In the deep learning era, Mithun et al. (2018)\\ndemonstrated an additional set of (image, text) pairs col-\\nlected from the internet could improve retrieval performance\\nand several new automatically constructed datasets such as\\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\\n2020), and OCR-CC (Yang et al., 2020) have been created.\\nHowever, these datasets still use significantly more aggres-\\nsive filtering or are designed for a specific task such as OCR\\nand as a result are still much smaller than WIT with between\\n1 and 10 million training examples.', 'A related idea to CLIP is webly supervised learning. This\\nline of work queries image search engines to build image\\ndatasets by querying for terms and uses the queries as the\\nlabels for the returned images (Fergus et al., 2005). Classi-\\nfiers trained on these large but noisily labeled datasets can\\nbe competitive with those trained on smaller carefully la-\\nbeled datasets. These image-query pairs are also often used\\nto improve performance on standard datasets as additional\\ntraining data (Chen & Gupta, 2015). CLIP also uses search\\nqueries as part of its dataset creation process. However\\nCLIP only uses full text sequences co-occuring with images\\nas supervision rather than just the queries, which are often\\nonly a single word or short n-gram. We also restrict this step\\nin CLIP to text only querying for sub-string matches while\\nmost webly supervised work uses standard image search\\nengines which have their own complex retrieval and filter-\\ning pipelines that often involve computer vision systems.\\nOf this line of work, Learning Everything about Anything:\\nWebly-Supervised Visual Concept Learning (Divvala et al.,\\n2014) has a notably similar ambition and goal as CLIP.', 'Finally, CLIP is related to a recent burst of activity on learn-\\ning joint models of vision and language (Lu et al., 2019; Tan\\n\\nLearning Transferable Visual Models From Natural Language Supervision 27', '& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\\n2020). This line of work focuses on richly connecting vision\\nand language in order to solve complex downstream tasks\\nsuch as visual question answering, visual commonsense\\nreasoning, or multimodal entailment. These approaches\\nleverage impressively engineered models which combine 3\\n(or more) pre-trained subsystems, typically an image feature\\nmodel, a region proposal / object detection model, and a\\npre-trained masked language model such as BERT. These\\nsystems are then jointly fine-tuned via various training objec-\\ntives on image-text pairs and applied to the aforementioned\\ntasks and achieve impressive results. CLIP is instead fo-\\ncused on learning visual models from scratch via natural\\nlanguage supervision and does not densely connect the two\\ndomains with a joint attention model. The only interaction\\nin a CLIP model between the image and text domain is a\\nsingle dot product in a learned joint embedding space. We\\nare excited to see CLIP hybridized with this line of work.', '9. Conclusion\\nWe have investigated whether it is possible to transfer the\\nsuccess of task-agnostic web-scale pre-training in NLP to\\nanother domain. We find that adopting this formula re-\\nsults in similar behaviors emerging in the field of computer\\nvision and discuss the social implications of this line of\\nresearch. In order to optimize their training objective, CLIP\\nmodels learn to perform a wide variety of tasks during pre-\\ntraining. This task learning can then be leveraged via natural\\nlanguage prompting to enable zero-shot transfer to many\\nexisting datasets. At sufficient scale, the performance of this\\napproach can be competitive with task-specific supervised\\nmodels although there is still room for much improvement.', 'ACKNOWLEDGMENTS', 'We’d like to thank the millions of people involved in creating\\nthe data CLIP is trained on. We’d also like to thank Susan\\nZhang for her work on image conditional language models\\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\\nGillian Hadfield for their thoughtful feedback on the broader\\nimpacts section of the paper. We are also grateful to the\\nAcceleration and Supercomputing teams at OpenAI for their\\ncritical work on software and hardware infrastructure this\\nproject used. Finally, we’d also like to thank the developers\\nof the many software packages used throughout this project\\nincluding, but not limited, to Numpy (Harris et al., 2020),\\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\\npandas (pandas development team, 2020), and scikit-learn\\n(Pedregosa et al., 2011).', 'References\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,', 'J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\\nTensorflow: A system for large-scale machine learning. In\\n12th {USENIX} symposium on operating systems design\\nand implementation ({OSDI} 16), pp. 265–283, 2016.', 'Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelović,\\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\\nand Zisserman, A. Self-supervised multimodal versatile\\nnetworks. arXiv preprint arXiv:2006.16228, 2020.', 'Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\\nS., and Nguyen, A. Strike (with) a pose: Neural networks\\nare easily fooled by strange poses of familiar objects. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 4845–4854, 2019.', 'Andreas, J., Klein, D., and Levine, S. Learning with latent\\nlanguage. arXiv preprint arXiv:1711.00482, 2017.\\n\\nAssiri, Y. Stochastic optimization of plain convolutional\\nneural networks with simple methods. arXiv preprint\\narXiv:2001.08856, 2020.', 'Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning\\nrepresentations by maximizing mutual information across\\nviews. In Advances in Neural Information Processing\\nSystems, pp. 15535–15545, 2019.', 'Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\\nlarge-scale bias-controlled dataset for pushing the lim-\\nits of object recognition models. In Advances in Neural\\nInformation Processing Systems, pp. 9453–9463, 2019.', 'Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\\nD. M., and Jordan, M. I. Matching words and pictures.\\nJournal of machine learning research, 3(Feb):1107–1135,\\n2003.', 'Bechmann, A. and Bowker, G. C. Unsupervised by any\\nother name: Hidden layers of knowledge production in\\nartificial intelligence on social media. Big Data & Society,\\n6(1):205395171881956, January 2019. doi: 10.1177/\\n2053951718819569. URL https://doi.org/10.\\n1177/2053951718819569.', 'Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\\nneural probabilistic language model. Journal of machine\\nlearning research, 3(Feb):1137–1155, 2003.', 'Bhargava, S. and Forsyth, D. Exposing and correcting the\\ngender bias in image captioning datasets and models.\\narXiv preprint arXiv:1912.00578, 2019.\\n\\nhttps://doi.org/10.1177/2053951718819569\\nhttps://doi.org/10.1177/2053951718819569', 'Learning Transferable Visual Models From Natural Language Supervision 28\\n\\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\\nallocation. Journal of machine Learning research, 3(Jan):\\n993–1022, 2003.', 'Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\\nKalai, A. T. Man is to computer programmer as woman\\nis to homemaker? debiasing word embeddings. Advances\\nin neural information processing systems, 29:4349–4357,\\n2016.', 'Bowker, G. C. and Star, S. L. Sorting things out: Classifica-\\ntion and its consequences. MIT press, 2000.', 'Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\narXiv preprint arXiv:2005.14165, 2020.', 'Browne, S. Dark Matters: Surveillance of Blackness. Duke\\nUniversity Press, 2015.\\n\\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\\nvisual representations with caption annotations. arXiv\\ne-prints, pp. arXiv–2008, 2020.', 'Buolamwini, J. and Gebru, T. Gender shades: Intersec-\\ntional accuracy disparities in commercial gender classi-\\nfication. In Conference on fairness, accountability and\\ntransparency, pp. 77–91, 2018.', 'Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A\\nshort note on the kinetics-700 human action dataset. arXiv\\npreprint arXiv:1907.06987, 2019.', 'Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\\nD., and Sutskever, I. Generative pretraining from pixels.\\nIn International Conference on Machine Learning, pp.\\n1691–1703. PMLR, 2020a.', 'Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training\\ndeep nets with sublinear memory cost. arXiv preprint\\narXiv:1604.06174, 2016.', 'Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\\nsimple framework for contrastive learning of visual rep-\\nresentations. arXiv preprint arXiv:2002.05709, 2020b.', 'Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and\\nHinton, G. Big self-supervised models are strong semi-\\nsupervised learners. arXiv preprint arXiv:2006.10029,\\n2020c.', 'Chen, X. and Gupta, A. Webly supervised learning of\\nconvolutional networks. In Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 1431–\\n1439, 2015.', 'Chen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning. arXiv\\npreprint arXiv:2003.04297, 2020d.', 'Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\\nCheng, Y., and Liu, J. Uniter: Learning universal image-\\ntext representations. arXiv preprint arXiv:1909.11740,\\n2019.', 'Cheng, G., Han, J., and Lu, X. Remote sensing image scene\\nclassification: Benchmark and state of the art. Proceed-\\nings of the IEEE, 105(10):1865–1883, 2017.', 'Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\\nand Dahl, G. E. On empirical comparisons of optimiz-\\ners for deep learning. arXiv preprint arXiv:1910.05446,\\n2019.', 'Coates, A., Ng, A., and Lee, H. An analysis of single-\\nlayer networks in unsupervised feature learning. In Pro-\\nceedings of the fourteenth international conference on\\nartificial intelligence and statistics, pp. 215–223, 2011.', 'Crawford, K. The trouble with bias. NIPS 2017\\nKeynote, 2017. URL https://www.youtube.com/\\nwatch?v=fMym_BKWQzk.\\n\\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning.\\nIn Advances in neural information processing systems,\\npp. 3079–3087, 2015.', 'D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al. Underspecification presents\\nchallenges for credibility in modern machine learning.\\narXiv preprint arXiv:2011.03395, 2020.', 'Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\\nFei, L. ImageNet: A Large-Scale Hierarchical Image\\nDatabase. In CVPR09, 2009.', 'Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\\nimage-net.org/challenges/LSVRC/2012/.', 'Desai, K. and Johnson, J. Virtex: Learning visual rep-\\nresentations from textual annotations. arXiv preprint\\narXiv:2006.06666, 2020.', 'Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.', 'Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\\nand Sutskever, I. Jukebox: A generative model for music.\\narXiv preprint arXiv:2005.00341, 2020.', 'https://www.youtube.com/watch?v=fMym_BKWQzk\\nhttps://www.youtube.com/watch?v=fMym_BKWQzk\\nhttp://www.image-net.org/challenges/LSVRC/2012/\\nhttp://www.image-net.org/challenges/LSVRC/2012/', 'Learning Transferable Visual Models From Natural Language Supervision 29', 'Divvala, S. K., Farhadi, A., and Guestrin, C. Learning\\neverything about anything: Webly-supervised visual con-\\ncept learning. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 3270–\\n3277, 2014.', 'Dodge, S. and Karam, L. A study and comparison of human\\nand deep learning recognition performance under visual\\ndistortions. In 2017 26th international conference on\\ncomputer communication and networks (ICCCN), pp. 1–\\n7. IEEE, 2017.', 'Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929, 2020.', 'Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\\nfier: Zero-shot learning using purely textual descriptions.\\nIn Proceedings of the IEEE International Conference on\\nComputer Vision, pp. 2584–2591, 2013.', 'Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\\nproving visual-semantic embeddings with hard negatives.\\narXiv preprint arXiv:1707.05612, 2017.', 'Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\\ning object categories from google’s image search. In\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\\n2005.', 'Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\\nRanzato, M., and Mikolov, T. Devise: A deep visual-\\nsemantic embedding model. In Advances in neural infor-\\nmation processing systems, pp. 2121–2129, 2013.', 'Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\\nLarge-scale adversarial training for vision-and-language\\nrepresentation learning. arXiv preprint arXiv:2006.06195,\\n2020.', 'Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-\\nguage models better few-shot learners. arXiv preprint\\narXiv:2012.15723, 2020.\\n\\nGarvie, C., May 2019. URL https://www.\\nflawedfacedata.com/.', 'Geiger, A., Lenz, P., and Urtasun, R. Are we ready for\\nautonomous driving? the kitti vision benchmark suite. In\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), 2012.', 'Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\\nmann, F. A., and Brendel, W. Imagenet-trained cnns are\\n\\nbiased towards texture; increasing shape bias improves ac-\\ncuracy and robustness. arXiv preprint arXiv:1811.12231,\\n2018.', 'Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\\ncut learning in deep neural networks. arXiv preprint\\narXiv:2004.07780, 2020.', 'Gomez, L., Patel, Y., Rusiñol, M., Karatzas, D., and Jawahar,\\nC. Self-supervised learning of visual features through\\nembedding images into text topic spaces. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 4230–4239, 2017.', 'Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\\ning and harnessing adversarial examples. arXiv preprint\\narXiv:1412.6572, 2014.', 'Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\\nD., Lee, D.-H., et al. Challenges in representation learn-\\ning: A report on three machine learning contests. Neural\\nNetworks, 64:59–63, 2015.', 'Google. Google cloud api: Celebrity recognition. URL\\nhttps://cloud.google.com/vision/docs/\\ncelebrity-recognition.', 'Griewank, A. and Walther, A. Algorithm 799: revolve: an\\nimplementation of checkpointing for the reverse or ad-\\njoint mode of computational differentiation. ACM Trans-\\nactions on Mathematical Software (TOMS), 26(1):19–45,\\n2000.', 'Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,\\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020.', 'Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv\\npreprint arXiv:1609.09106, 2016.', 'Hancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\\nS., and Ré, C. Training classifiers with natural language\\nexplanations. In Proceedings of the conference. Associ-\\nation for Computational Linguistics. Meeting, volume\\n2018, pp. 1884. NIH Public Access, 2018.', 'Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\\nLearning from dialogue after deployment: Feed yourself,\\nchatbot! arXiv preprint arXiv:1901.05415, 2019.', 'Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\\nKerkwijk, M. H., Brett, M., Haldane, A., Fernández del', 'https://www.flawedfacedata.com/\\nhttps://www.flawedfacedata.com/\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 30', 'Rı́o, J., Wiebe, M., Peterson, P., Gérard-Marchant, P.,\\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\\nGohlke, C., and Oliphant, T. E. Array programming\\nwith NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\\ns41586-020-2649-2.', 'Hays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image. In 2008 ieee confer-\\nence on computer vision and pattern recognition, pp. 1–8.\\nIEEE, 2008.', 'He, K., Zhang, X., Ren, S., and Sun, J. Delving deep\\ninto rectifiers: Surpassing human-level performance on\\nimagenet classification. In Proceedings of the IEEE inter-\\nnational conference on computer vision, pp. 1026–1034,\\n2015.', 'He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016a.', 'He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016b.', 'He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pp. 9729–\\n9738, 2020.', 'He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 558–\\n567, 2019.', 'He, X. and Peng, Y. Fine-grained image classification via\\ncombining vision and language. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition, pp. 5994–6002, 2017.', 'Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\\nA novel dataset and deep learning benchmark for land\\nuse and land cover classification. IEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote\\nSensing, 12(7):2217–2226, 2019.', 'Henaff, O. Data-efficient image recognition with contrastive\\npredictive coding. In International Conference on Ma-\\nchine Learning, pp. 4182–4192. PMLR, 2020.', 'Hendrycks, D. and Dietterich, T. Benchmarking neural\\nnetwork robustness to common corruptions and perturba-\\ntions. arXiv preprint arXiv:1903.12261, 2019.', 'Hendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\\nSong, D. Natural adversarial examples. arXiv preprint\\narXiv:1907.07174, 2019.', 'Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\\net al. The many faces of robustness: A critical analy-\\nsis of out-of-distribution generalization. arXiv preprint\\narXiv:2006.16241, 2020a.', 'Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\\nR., and Song, D. Pretrained transformers improve out-of-\\ndistribution robustness. arXiv preprint arXiv:2004.06100,\\n2020b.', 'Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\\nY. Deep learning scaling is predictable, empirically. arXiv\\npreprint arXiv:1712.00409, 2017.', 'Hill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\\nM., McClelland, J. L., and Santoro, A. Environmental\\ndrivers of systematicity and generalization in a situated\\nagent. In International Conference on Learning Repre-\\nsentations, 2019.', 'Hodosh, M., Young, P., and Hockenmaier, J. Framing image\\ndescription as a ranking task: Data, models and evaluation\\nmetrics. Journal of Artificial Intelligence Research, 47:\\n853–899, 2013.', 'Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\\nEnhancing image geolocalization by combinatorial parti-\\ntioning of maps. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV), pp. 536–551, 2018.', 'Howard, J. and Ruder, S. Universal language model\\nfine-tuning for text classification. arXiv preprint\\narXiv:1801.06146, 2018.', 'Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\\nB., and Madry, A. Adversarial examples are not bugs,\\nthey are features. In Advances in Neural Information\\nProcessing Systems, pp. 125–136, 2019.', 'Ioffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\narXiv preprint arXiv:1502.03167, 2015.', 'Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\\nA. Deep structured output learning for unconstrained text\\nrecognition. arXiv preprint arXiv:1412.5903, 2014.', 'Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\\ntransformer networks. Advances in neural information\\nprocessing systems, 28:2017–2025, 2015.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 31', 'Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\\nnostic dataset for compositional language and elementary\\nvisual reasoning. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pp.\\n2901–2910, 2017.', 'Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\\nLearning visual features from large weakly supervised\\ndata. In European Conference on Computer Vision, pp.\\n67–84. Springer, 2016.', 'Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\\nmodeling in 3d cnn architectures with bert for action\\nrecognition. arXiv preprint arXiv:2008.01232, 2020.', 'Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361, 2020.', 'Karpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\\nembeddings for bidirectional image sentence mapping.\\nIn Advances in neural information processing systems,\\npp. 1889–1897, 2014.', 'Keyes, O. The misgendering machines: Trans/hci implica-\\ntions of automatic gender recognition. Proceedings of the\\nACM on Human-Computer Interaction, 2(CSCW):1–22,\\n2018.', 'Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\narXiv preprint arXiv:2005.04790, 2020.', 'Kingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.', 'Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\\nvisual-semantic embeddings with multimodal neural lan-\\nguage models. arXiv preprint arXiv:1411.2539, 2014.', 'Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\\nAdvances in neural information processing systems, 28:\\n3294–3302, 2015.', 'Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\\nJ., Gelly, S., and Houlsby, N. Large scale learning of\\ngeneral visual representations for transfer. arXiv preprint\\narXiv:1912.11370, 2019.', 'Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\\nmodels transfer better? In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 2661–2671, 2019.', 'Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\\nD. A., et al. Visual genome: Connecting language and\\nvision using crowdsourced dense image annotations. In-\\nternational journal of computer vision, 123(1):32–73,\\n2017.', 'Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassification with deep convolutional neural networks.\\nIn Advances in neural information processing systems,\\npp. 1097–1105, 2012.', 'Kuhnle, A. and Copestake, A. Shapeworld-a new test\\nmethodology for multimodal language understanding.\\narXiv preprint arXiv:1704.04517, 2017.\\n\\nKärkkäinen, K. and Joo, J. Fairface: Face attribute dataset\\nfor balanced race, gender, and age, 2019.', 'Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.', 'Lampert, C. H., Nickisch, H., and Harmeling, S. Learning\\nto detect unseen object classes by between-class attribute\\ntransfer. In 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 951–958. IEEE, 2009.', 'Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\\nof new tasks. 2008.\\n\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents. In International conference on\\nmachine learning, pp. 1188–1196, 2014.', 'LeCun, Y. The mnist database of handwritten digits.\\nhttp://yann. lecun. com/exdb/mnist/.\\n\\nLee, D.-H. Pseudo-label: The simple and efficient semi-\\nsupervised learning method for deep neural networks.', 'Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\\nzero-shot convolutional neural networks using textual\\ndescriptions. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 4247–4255, 2015.', 'Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\\nvisual n-grams from web data. In Proceedings of the\\nIEEE International Conference on Computer Vision, pp.\\n4183–4192, 2017.', 'Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\\nUnicoder-vl: A universal encoder for vision and language\\nby cross-modal pre-training. 2020a.', 'Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\\nLearning through dialogue interactions by asking ques-\\ntions. arXiv preprint arXiv:1612.04936, 2016.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 32', 'Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\\nsemantics aligned pre-training for vision-language tasks.\\narXiv preprint arXiv:2004.06165, 2020b.', 'Liang, W., Zou, J., and Yu, Z. Alice: Active learning with\\ncontrastive natural language explanations. arXiv preprint\\narXiv:2009.10259, 2020.', 'Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context. In European conference on\\ncomputer vision, pp. 740–755. Springer, 2014.', 'Linzen, T. How can we accelerate progress towards\\nhuman-like linguistic generalization? arXiv preprint\\narXiv:2005.00955, 2020.', 'Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\\ntimodal framework for the detection of hateful memes.\\narXiv preprint arXiv:2012.12871, 2020.', 'Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\\nssi, R., Kaiser, L., and Shazeer, N. Generating\\nwikipedia by summarizing long sequences. arXiv preprint\\narXiv:1801.10198, 2018.', 'Locatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S.,\\nSchölkopf, B., and Bachem, O. A sober look at the\\nunsupervised learning of disentangled representations\\nand their evaluation. arXiv preprint arXiv:2010.14766,\\n2020.', 'Loshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\\ndient descent with warm restarts. arXiv preprint\\narXiv:1608.03983, 2016.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization. arXiv preprint arXiv:1711.05101, 2017.', 'Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\\ntask-agnostic visiolinguistic representations for vision-\\nand-language tasks. In Advances in Neural Information\\nProcessing Systems, pp. 13–23, 2019.', 'Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\\nweakly supervised data and pose representation for action\\nrecognition, 2020. URL https://www.youtube.\\ncom/watch?v=KOQFxbPPLOE&t=1390s.', 'Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\\nquet, O. Are gans created equal? a large-scale study.\\nAdvances in neural information processing systems, 31:\\n700–709, 2018.', 'Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\\nM., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\\nploring the limits of weakly supervised pretraining. In\\n\\nProceedings of the European Conference on Computer\\nVision (ECCV), pp. 181–196, 2018.', 'McCann, B., Bradbury, J., Xiong, C., and Socher, R.\\nLearned in translation: Contextualized word vectors. In\\nAdvances in neural information processing systems, pp.\\n6294–6305, 2017.', 'McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\\nnatural language decathlon: Multitask learning as ques-\\ntion answering. arXiv preprint arXiv:1806.08730, 2018.', 'Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\\nVenkatesh, G., et al. Mixed precision training. arXiv\\npreprint arXiv:1710.03740, 2017.', 'Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\\nI., and Sivic, J. Howto100m: Learning a text-video em-\\nbedding by watching hundred million narrated video clips.\\nIn Proceedings of the IEEE international conference on\\ncomputer vision, pp. 2630–2640, 2019.', 'Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\\nman, A. Rareact: A video dataset of unusual interactions.\\narXiv preprint arXiv:2008.01018, 2020a.', 'Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\\nand Zisserman, A. End-to-end learning of visual represen-\\ntations from uncurated instructional videos. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 9879–9889, 2020b.', 'Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality. Advances in neural informa-\\ntion processing systems, 26:3111–3119, 2013.', 'Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\\nof natural distribution shift on question answering models.\\narXiv preprint arXiv:2004.14444, 2020.', 'Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\\ntion using higher order language priors. 2012.', 'Mithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\\nChowdhury, A. K. Webly supervised joint embedding for\\ncross-modal image-text retrieval. In Proceedings of the\\n26th ACM international conference on Multimedia, pp.\\n1856–1864, 2018.', 'Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\\nformation based on dividing and vector quantizing images\\nwith words. Citeseer, 1999.', 'Mu, J., Liang, P., and Goodman, N. Shaping visual represen-\\ntations with language for few-shot classification. arXiv\\npreprint arXiv:1911.02683, 2019.\\n\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s', 'Learning Transferable Visual Models From Natural Language Supervision 33', 'Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\\ncation estimation of photos using a hierarchical model\\nand scene classification. In Proceedings of the European\\nConference on Computer Vision (ECCV), pp. 563–579,\\n2018.', 'Murty, S., Koh, P. W., and Liang, P. Expbert: Representation\\nengineering with natural language explanations. arXiv\\npreprint arXiv:2005.01932, 2020.', 'Narasimhan, K., Kulkarni, T., and Barzilay, R. Language\\nunderstanding for text-based games using deep reinforce-\\nment learning. arXiv preprint arXiv:1506.08941, 2015.', 'Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y. Reading digits in natural images with\\nunsupervised feature learning. 2011.\\n\\nNoble, S. U. Algorithms of oppression: How search engines\\nreinforce racism. 2018.', 'Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site. Group Dynamics: Theory, Research, and\\nPractice, 6(1):101, 2002.', 'Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\\nA large-scale benchmark dataset for event recognition in\\nsurveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\\n2011.', 'Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\\nfellow, I. Realistic evaluation of deep semi-supervised\\nlearning algorithms. Advances in neural information pro-\\ncessing systems, 31:3235–3246, 2018.', 'Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\\ning with contrastive predictive coding. arXiv preprint\\narXiv:1807.03748, 2018.', 'Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. Advances\\nin neural information processing systems, 24:1143–1151,\\n2011.', 'pandas development team, T. pandas-dev/pandas: Pan-\\ndas, February 2020. URL https://doi.org/10.\\n5281/zenodo.3509134.', 'Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\\nC. V. Cats and dogs. In IEEE Conference on Computer\\nVision and Pattern Recognition, 2012.', 'Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,', 'Bai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library. In Advances\\nin Neural Information Processing Systems 32, pp. 8024–\\n8035, 2019.', 'Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research, 12:2825–2830, 2011.', 'Pennington, J., Socher, R., and Manning, C. D. Glove:\\nGlobal vectors for word representation. In Proceedings\\nof the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pp. 1532–1543, 2014.', 'Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. arXiv preprint arXiv:1802.05365,\\n2018.', 'Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\\nA. Imagebert: Cross-modal pre-training with large-\\nscale weak-supervised image-text data. arXiv preprint\\narXiv:2001.07966, 2020.', 'Quattoni, A., Collins, M., and Darrell, T. Learning visual\\nrepresentations using images with captions. In 2007 IEEE\\nConference on Computer Vision and Pattern Recognition,\\npp. 1–8. IEEE, 2007.', 'Radford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018.', 'Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.', 'Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.', 'Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\\nJ., and Denton, E. Saving face: Investigating the ethical\\nconcerns of facial recognition auditing, 2020.', 'Ramanathan, V., Liang, P., and Fei-Fei, L. Video event\\nunderstanding using natural language descriptions. In\\nProceedings of the IEEE International Conference on\\nComputer Vision, pp. 905–912, 2013.', 'Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\\nCollecting image annotations using amazon’s mechanical\\nturk. In Proceedings of the NAACL HLT 2010 Workshop\\non Creating Speech and Language Data with Amazon’s\\nMechanical Turk, pp. 139–147, 2010.', 'https://doi.org/10.5281/zenodo.3509134\\nhttps://doi.org/10.5281/zenodo.3509134\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 34', 'Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\\nagenet classifiers generalize to imagenet? arXiv preprint\\narXiv:1902.10811, 2019.', 'Salimans, T. and Kingma, D. P. Weight normalization: A\\nsimple reparameterization to accelerate training of deep\\nneural networks. In Advances in neural information pro-\\ncessing systems, pp. 901–909, 2016.', 'Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\\ncomputers see gender: An evaluation of gender classifica-\\ntion in commercial facial analysis services. Proceedings\\nof the ACM on Human-Computer Interaction, 3(CSCW):\\n1–33, 2019.', 'Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\\ngender bias in image recognition systems. Socius, 6:\\n2378023120967171, 2020.', 'Sennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909, 2015.', 'Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\\nand Schmidt, L. Do image classifiers generalize across\\ntime? arXiv preprint arXiv:1906.02168, 2019.', 'Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Proceedings\\nof the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\\n2565, 2018.', 'Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\\nmodels that can read. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, pp.\\n8317–8326, 2019.', 'Socher, R. and Fei-Fei, L. Connecting modalities: Semi-\\nsupervised segmentation and annotation of images using\\nunaligned text corpora. In 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition,\\npp. 966–973. IEEE, 2010.', 'Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\\nsemantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods\\nin natural language processing, pp. 1631–1642, 2013.', 'Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\\nA. Y. Grounded compositional semantics for finding and\\ndescribing images with sentences. Transactions of the\\nAssociation for Computational Linguistics, 2:207–218,\\n2014.', 'Sohn, K. Improved deep metric learning with multi-class\\nn-pair loss objective. In Advances in neural information\\nprocessing systems, pp. 1857–1865, 2016.', 'Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\\nVoss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\\nMcGuffie, K., and Wang, J. Release strategies and the\\nsocial impacts of language models, 2019.', 'Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\\nof 101 human actions classes from videos in the wild.\\narXiv preprint arXiv:1212.0402, 2012.\\n\\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/\\n10.5281/zenodo.2591652. Version 5.5.', 'Srivastava, N. and Salakhutdinov, R. Multimodal learning\\nwith deep boltzmann machines. In NIPS, 2012.', 'Srivastava, S., Labutov, I., and Mitchell, T. Joint concept\\nlearning and semantic parsing from natural language ex-\\nplanations. In Proceedings of the 2017 conference on\\nempirical methods in natural language processing, pp.\\n1527–1536, 2017.', 'Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Traffic Sign Recognition Benchmark: A multi-\\nclass classification competition. In IEEE International\\nJoint Conference on Neural Networks, pp. 1453–1460,\\n2011.', 'Stroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\\nand Schmid, C. Learning video representations from tex-\\ntual web supervision. arXiv preprint arXiv:2007.14937,\\n2020.', 'Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\\nA. Inception-v4, inception-resnet and the impact\\nof residual connections on learning. arXiv preprint\\narXiv:1602.07261, 2016.', 'Tan, H. and Bansal, M. Lxmert: Learning cross-modality\\nencoder representations from transformers. arXiv preprint\\narXiv:1908.07490, 2019.', 'Tan, M. and Le, Q. V. Efficientnet: Rethinking model\\nscaling for convolutional neural networks. arXiv preprint\\narXiv:1905.11946, 2019.', 'Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\\nand Schmidt, L. Measuring robustness to natural dis-\\ntribution shifts in image classification. arXiv preprint\\narXiv:2007.00644, 2020.', 'Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\\nnew data in multimedia research. Communications of the\\nACM, 59(2):64–73, 2016.', 'https://doi.org/10.5281/zenodo.2591652\\nhttps://doi.org/10.5281/zenodo.2591652\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 35', 'Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview\\ncoding. arXiv preprint arXiv:1906.05849, 2019.', 'Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\\nIsola, P. Rethinking few-shot image classification: a\\ngood embedding is all you need? arXiv preprint\\narXiv:2003.11539, 2020.', 'Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\\nimages: A large data set for nonparametric object and\\nscene recognition. IEEE transactions on pattern analysis\\nand machine intelligence, 30(11):1958–1970, 2008.', 'Touvron, H., Vedaldi, A., Douze, M., and Jégou, H. Fix-\\ning the train-test resolution discrepancy. In Advances in\\nneural information processing systems, pp. 8252–8262,\\n2019.', 'Varadarajan, J. and Odobez, J.-M. Topic models for scene\\nanalysis and abnormality detection. In 2009 IEEE 12th\\nInternational Conference on Computer Vision Workshops,\\nICCV Workshops, pp. 1338–1345. IEEE, 2009.', 'Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in neural information\\nprocessing systems, pp. 5998–6008, 2017.', 'Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\\nWelling, M. Rotation equivariant CNNs for digital pathol-\\nogy. June 2018.', 'Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, İ.,\\nFeng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\\n1.0: Fundamental Algorithms for Scientific Computing\\nin Python. Nature Methods, 17:261–272, 2020. doi:\\n10.1038/s41592-019-0686-2.', 'Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\\ndeep learning era. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pp. 2621–2630,\\n2017.', 'Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461, 2018.', 'Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\\nbust global representations by penalizing local predictive\\npower. In Advances in Neural Information Processing\\nSystems, pp. 10506–10518, 2019.', 'Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\\nM., Wang, Y., and Liu, W. All you need is boundary: To-\\nward arbitrary-shaped text spotting. In Proceedings of the\\nAAAI Conference on Artificial Intelligence, volume 34,\\npp. 12160–12167, 2020.', 'Wang, J., Markert, K., and Everingham, M. Learning mod-\\nels for object recognition from natural language descrip-\\ntions. In BMVC, volume 1, pp. 2, 2009.', 'Weston, J., Bengio, S., and Usunier, N. Large scale im-\\nage annotation: learning to rank with joint word-image\\nembeddings. Machine learning, 81(1):21–35, 2010.', 'Weston, J. E. Dialog-based language learning. In Advances\\nin Neural Information Processing Systems, pp. 829–837,\\n2016.', 'Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\\ncation with convolutional neural networks. In European\\nConference on Computer Vision, pp. 37–55. Springer,\\n2016.', 'Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\\nshick, R. Detectron2. https://github.com/\\nfacebookresearch/detectron2, 2019.', 'Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\\nlearning via non-parametric instance-level discrimination.\\narXiv preprint arXiv:1805.01978, 2018.', 'Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\\nwith noisy student improves imagenet classification. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 10687–10698, 2020.', 'y Arcas, B. A., Mitchell, M., and Todorov,\\nA. Physiognomy’s new clothes. 2017.\\nURL https://medium.com/@blaisea/\\nphysiognomys-new-clothes-f2d4b59fdd6a.', 'Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\\npre-training for text-vqa and text-caption. arXiv preprint\\narXiv:2012.04638, 2020.', 'Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\\nYu, L., Dyer, C., et al. Learning and evaluating general\\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\\n2019.', 'Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\\nimage descriptions to visual denotations: New similarity\\nmetrics for semantic inference over event descriptions.\\nTransactions of the Association for Computational Lin-\\nguistics, 2:67–78, 2014.', 'https://github.com/facebookresearch/detectron2\\nhttps://github.com/facebookresearch/detectron2\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a', 'Learning Transferable Visual Models From Natural Language Supervision 36', 'Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\\nlanguage representations through scene graph. arXiv\\npreprint arXiv:2006.16934, 2020.', 'Zeiler, M. D. and Fergus, R. Visualizing and understand-\\ning convolutional networks. In European conference on\\ncomputer vision, pp. 818–833. Springer, 2014.', 'Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\\nmann, M., Dosovitskiy, A., et al. A large-scale study of\\nrepresentation learning with the visual task adaptation\\nbenchmark. arXiv preprint arXiv:1910.04867, 2019.', 'Zhang, R. Making convolutional networks shift-invariant\\nagain. arXiv preprint arXiv:1904.11486, 2019.', 'Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\\nglotz, C. P. Contrastive learning of medical visual repre-\\nsentations from paired images and text. arXiv preprint\\narXiv:2010.00747, 2020.', 'Zuboff, S. Big other: surveillance capitalism and the\\nprospects of an information civilization. Journal of Infor-\\nmation Technology, 30(1):75–89, 2015.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 37', 'A. Linear-probe evaluation\\nWe provide additional details for linear probe experiments\\npresented in this paper, including the list of the datasets and\\nmodels used for evaluation.\\n\\nA.1. Datasets', 'We use the 12 datasets from the well-studied evaluation\\nsuite introduced by (Kornblith et al., 2019) and add 15\\nadditional datasets in order to assess the performance of\\nmodels on a wider variety of distributions and tasks. These\\ndatasets include MNIST, the Facial Expression Recognition\\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\\nfic Sign Recognition Benchmark (GTSRB) dataset (Stal-\\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\\nreira et al., 2019), 2,500 random samples of the CLEVR\\ndataset (Johnson et al., 2017), the Hateful Memes dataset\\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\\net al., 2012). For the two video datasets (UCF101 and Ki-\\nnetics700), we use the middle frame of each video clip as\\nthe input image. STL-10 and UCF101 have multiple pre-\\ndefined train/validation/test splits, 10 and 3 respectively, and\\nwe report the average over all splits. Details on each dataset\\nand the corresponding evaluation metrics are provided in\\nTable 9.', 'Additionally, we created two datasets that we call Coun-\\ntry211 and Rendered SST2. The Country211 dataset is\\ndesigned to assess the geolocation capability of visual rep-\\nresentations. We filtered the YFCC100m dataset (Thomee\\net al., 2016) to find 211 countries (defined as having an\\nISO-3166 country code) that have at least 300 photos with\\nGPS coordinates, and we built a balanced dataset with 211\\ncategories, by sampling 200 photos for training and 100\\nphotos for testing, for each country.', 'The Rendered SST2 dataset is designed to measure the opti-\\ncal character recognition capability of visual representations.\\nTo do so, we used the sentences from the Stanford Sentiment\\nTreebank dataset (Socher et al., 2013) and rendered them\\ninto images, with black texts on a white background, in a\\n448×448 resolution. Two example images from this dataset\\nare shown in Figure 19.', 'A.2. Models\\n\\nIn combination with the datasets listed above, we evaluate\\nthe following series of models using linear probes.\\n\\nLM RN50 This is a multimodal model that uses an au-\\ntoregressive loss instead of a contrastive loss, while using', 'the ResNet-50 architecture as in the smallest contrastive\\nmodel. To do so, the output from the CNN is projected into\\nfour tokens, which are then fed as a prefix to a language\\nmodel autoregressively predicting the text tokens. Apart\\nfrom the training objective, the model was trained on the\\nsame dataset for the same number of epochs as other CLIP\\nmodels.', 'CLIP-RN Five ResNet-based contrastive CLIP models\\nare included. As discussed in the paper, the first two models\\nfollow ResNet-50 and ResNet-101, and we use EfficientNet-\\nstyle (Tan & Le, 2019) scaling for the next three models\\nwhich simultaneously scale the model width, the number\\nof layers, and the input resolution to obtain models with\\nroughly 4x, 16x, and 64x computation.', 'CLIP-ViT We include four CLIP models that use the Vi-\\nsion Transformer (Dosovitskiy et al., 2020) architecture as\\nthe image encoder. We include three models trained on 224-\\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\\nthe ViT-L/14 model fine-tuned on 336-by-336 pixel input\\nimages.', 'EfficietNet We use the nine models (B0-B8) from the\\noriginal EfficientNet paper (Tan & Le, 2019), as well as\\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\\ntake the input resolutions of 475x475 and 800x800 pixels,\\nrespectively.', 'Instagram-pretrained ResNeXt We use the four models\\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\\net al., 2018), as well as their two FixRes variants which use\\nhigher input resolutions (Touvron et al., 2019).', 'Big Transfer (BiT) We use BiT-S and BiT-M models\\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\\nImageNet-21k datasets. The model weights for BiT-L is not\\npublicly available.', 'Vision Transformer (ViT) We also include four ViT\\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\\nL/16, and ViT-H/14. We note that their best-performing\\nmodels, trained on the JFT-300M dataset, are not available\\npublicly.', 'SimCLRv2 The SimCLRv2 (Chen et al., 2020c) project\\nreleased pre-trained and fine-tuned models in various set-\\ntings. We use the seven pretrain-only checkpoints with\\nselective kernels.', 'BYOL We use the recently released model weights of\\nBYOL (Grill et al., 2020), specifically their 50x1 and 200x2\\n\\nLearning Transferable Visual Models From Natural Language Supervision 38\\n\\nFigure 19. Two example images from the Rendered SST2 dataset', 'checkpoints.\\n\\nMomentum Contrast (MoCo) We include the MoCo-v1\\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\\ncheckpoints.', 'VirTex We use the pretrained model of VirTex (Desai &\\nJohnson, 2020). We note that VirTex has a similar model\\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\\nof high-quality captions from MSCOCO.', 'ResNet We add the original ResNet checkpoints released\\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\\nResNet152.\\n\\nA.3. Evaluation', 'We use image features taken from the penultimate layer of\\neach model, ignoring any classification layer provided. For\\nCLIP-ViT models, we used the features before the linear\\nprojection to the embedding space, which corresponds to\\nI f in Figure 3. We train a logistic regression classifier\\nusing scikit-learn’s L-BFGS implementation, with maxi-\\nmum 1,000 iterations, and report the corresponding met-\\nric for each dataset. We determine the L2 regularization\\nstrength λ using a hyperparameter sweep on the validation\\nsets over the range between 10−6 and 106, with 96 log-\\narithmically spaced steps. To save compute required for\\nthe sweeps, we perform a parametric binary search that\\nstarts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\\neratively halves the interval around the peak until it reaches\\na resolution of 8 steps per decade. The hyperparameter\\nsweeps are performed on a validation split of each dataset.\\nFor the datasets that contain a validation split in addition to', 'a test split, we use the provided validation set to perform\\nthe hyperparameter search, and for the datasets that do not\\nprovide a validation split or have not published labels for\\nthe test data, we split the training dataset to perform the\\nhyperparameter search. For the final result, we combine the\\nvalidation split back with the training split and report the\\nperformance on the unused split.', 'A.4. Results', 'The individual linear probe scores are provided in Table 10\\nand plotted in Figure 20. The best-performing CLIP model,\\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\\nachieved the state of the art in 21 of the 27 datasets, i.e.\\nincluded in the Clopper-Pearson 99.5% confidence interval\\naround each dataset’s top score. For many datasets, CLIP\\nperforms significantly better than other models, demonstrat-\\ning the advantage of natural language supervision over tradi-\\ntional pre-training approaches based on image classification.\\nSee Section 3.2 for more discussions on the linear probe\\nresults.', 'Learning Transferable Visual Models From Natural Language Supervision 39\\n\\nDataset Classes Train size Test size Evaluation metric', 'Food-101 102 75,750 25,250 accuracy\\nCIFAR-10 10 50,000 10,000 accuracy\\nCIFAR-100 100 50,000 10,000 accuracy\\nBirdsnap 500 42,283 2,149 accuracy\\nSUN397 397 19,850 19,850 accuracy\\nStanford Cars 196 8,144 8,041 accuracy\\nFGVC Aircraft 100 6,667 3,333 mean per class\\nPascal VOC 2007 Classification 20 5,011 4,952 11-point mAP\\nDescribable Textures 47 3,760 1,880 accuracy\\nOxford-IIIT Pets 37 3,680 3,669 mean per class\\nCaltech-101 102 3,060 6,085 mean-per-class\\nOxford Flowers 102 102 2,040 6,149 mean per class', 'MNIST 10 60,000 10,000 accuracy\\nFacial Emotion Recognition 2013 8 32,140 3,574 accuracy\\nSTL-10 10 1000 8000 accuracy\\nEuroSAT 10 10,000 5,000 accuracy\\nRESISC45 45 3,150 25,200 accuracy\\nGTSRB 43 26,640 12,630 accuracy\\nKITTI 4 6,770 711 accuracy\\nCountry211 211 43,200 21,100 accuracy\\nPatchCamelyon 2 294,912 32,768 accuracy\\nUCF101 101 9,537 1,794 accuracy\\nKinetics700 700 494,801 31,669 mean(top1, top5)\\nCLEVR Counts 8 2,000 500 accuracy\\nHateful Memes 2 8,500 500 ROC AUC\\nRendered SST2 2 7,792 1,821 accuracy\\nImageNet 1000 1,281,167 50,000 accuracy', 'Table 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\\navailable online at the time of this writing.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 40', 'Fo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nC\\nar\\n\\ns\\n\\nA\\nir\\n\\ncr\\naf\\n\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n?\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou', 'K\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nA\\n\\nM\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nSS\\nT\\n\\nIm\\nag\\n\\neN\\net', 'SS\\nT\\n\\nIm\\nag\\n\\neN\\net\\n\\nLM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nN', '50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\\n101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\\n50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2', '50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\\n50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6', 'C\\nL\\n\\nIP\\n-V', 'iT B/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\\nB/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\\nL/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9', 'L/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4\\n\\nE\\nffi\\n\\nci\\nen\\n\\ntN\\net', 'B0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\\nB1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\\nB2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\\nB3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\\nB4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\\nB5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\\nB6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\\nB7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\\nB8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3', 'E\\nffi\\n\\nci\\nen\\n\\ntN\\net\\n\\nN\\noi\\n\\nsy\\nSt\\n\\nud\\nen', 't B0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\\nB1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\\nB2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\\nB3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\\nB4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\\nB5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\\nB6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\\nB7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0', 'L2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\\nL2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4', 'In\\nst\\n\\nag\\nra\\n\\nm', '32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\\n32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\\n32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\\n32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2', 'FixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\\nFixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0', 'B\\niT\\n\\n-S', 'R50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\\nR50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2', 'R101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\\nR101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\\nR152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\\nR152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5', 'B\\niT\\n\\n-M', 'R50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\\nR50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7', 'R101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\\nR101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\\nR152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\\nR152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5', 'V\\niT', 'B/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\\nB/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\\nL/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\\nH/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9', 'Si\\nm\\n\\nC\\nL\\n\\nR\\nv2', 'R50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\\nR50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3', 'R101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\\nR101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\\nR152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\\nR152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\\nR152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0', 'B\\nY\\n\\nO\\nL 50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3', '200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2\\n\\nM\\noC', 'o v1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\\nv2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1', 'VirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7\\n\\nR\\nes', 'R\\nes\\n\\nN\\net 50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3', '101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\\n152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1', 'Table 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson confidence\\ninterval of each dataset’s top score are shown in bold.', '?We updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 41\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\nac\\n\\ncu\\nra\\n\\ncy\\nFood101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94', '90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n100 101 102\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n100 101 102\\n\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n100 101 102\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n100 101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\nac\\ncu', '70\\n\\n80\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n100 101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n100 101 102\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\n\\n88\\n\\n89\\n\\n90\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes\\n\\nPascalVOC2007\\n\\n100 101 102\\n\\n72\\n\\n74\\n\\n76\\n\\n78\\n\\n80', '72\\n\\n74\\n\\n76\\n\\n78\\n\\n80\\n\\n82\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n100 101 102\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n100 101 102\\n\\n90\\n\\n91\\n\\n92\\n\\n93\\n\\n94\\n\\n95\\n\\n96\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\n100\\n\\nm\\nea', '96\\n\\n98\\n\\n100\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n100 101 102\\n\\n97.25\\n\\n97.50\\n\\n97.75\\n\\n98.00\\n\\n98.25\\n\\n98.50\\n\\n98.75\\n\\n99.00\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n100 101 102\\n\\n55.0\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n100 101 102', '100 101 102\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\n98.5\\n\\n99.0\\n\\n99.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n100 101 102\\n\\n95.5\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n100 101 102\\n82\\n\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n100 101 102\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90', '70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n100 101 102\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n100 101 102\\n\\n81\\n\\n82\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\nac\\n\\ncu\\nra\\n\\ncy\\nPatchCamelyon\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n100 101 102', 'UCF101\\n\\n100 101 102\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n100 101 102\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55', 'GFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\n77.5\\n\\n80.0\\n\\n82.5\\n\\n85.0\\n\\n87.5\\n\\nac\\ncu\\n\\nra\\ncy', '87.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet', 'Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.\\n\\nLearning Transferable Visual Models From Natural Language Supervision 42\\n\\ncorrect label: red and white triangle with exclamation mark warning', '0 20 40 60 80 100\\n\\na zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.', 'a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.', 'a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.\\n\\ncorrect rank: 1/43    correct probability: 45.75%\\nGerman Traffic Sign Recognition Benchmark (GTSRB)\\n\\ncorrect label: positive\\n\\n0 20 40 60 80 100', '0 20 40 60 80 100\\n\\na positive review of a movie.\\n\\na negative review of a movie.\\n\\ncorrect rank: 1/2    correct probability: 78.21%\\nStanford Sentiment Treebank\\n\\ncorrect label: meme\\n\\n0 20 40 60 80 100\\n\\na meme.\\n\\na hatespeech meme.', 'a hatespeech meme.\\n\\ncorrect rank: 1/2    correct probability: 99.20%\\nHateful Memes\\n\\ncorrect label: barn\\n\\n0 20 40 60 80 100\\n\\na photo of a barn.\\n\\na photo of a church.\\n\\na photo of a threshing machine.\\n\\na photo of a sawmill.\\n\\na photo of a prison.', 'a photo of a prison.\\n\\ncorrect rank: 1/1000    correct probability: 79.56%\\nImageNet Sketch\\n\\ncorrect label(s): antelope\\n\\n0 20 40 60 80 100\\n\\na photo of a antelope.\\n\\na photo of a zebra.\\n\\na photo of a car.\\n\\na photo of a cattle.\\n\\na photo of a elephant.', 'correct rank: 1/30    correct probability: 99.77%\\nImageNet Vid\\n\\ncorrect label: 158\\n\\n0 20 40 60 80 100\\n\\na street sign of the number: \"1157\".\\n\\na street sign of the number: \"1165\".\\n\\na street sign of the number: \"1164\".\\n\\na street sign of the number: \"1155\".', 'a street sign of the number: \"1364\".\\n\\ncorrect rank: 83/2000    correct probability: 0.27%\\nStreet View House Numbers (SVHN)\\n\\ncorrect label: 7\\n\\n0 20 40 60 80 100\\n\\na photo of the number: \"7\".\\n\\na photo of the number: \"2\".\\n\\na photo of the number: \"1\".', 'a photo of the number: \"6\".\\n\\na photo of the number: \"4\".\\n\\ncorrect rank: 1/10    correct probability: 85.32%\\nMNIST\\n\\ncorrect label(s): motorcycle\\n\\n0 20 40 60 80 100\\n\\na photo of a motorcycle.\\n\\na photo of a bicycle.\\n\\na photo of a car.\\n\\na photo of a horse.', 'a photo of a horse.\\n\\na photo of a dining table.\\n\\ncorrect rank: 1/20    correct probability: 99.69%\\nPASCAL VOC 2007\\n\\ncorrect label: perforated\\n\\n0 20 40 60 80 100\\n\\na photo of a polka-dotted texture.\\n\\na photo of a perforated texture.', 'a photo of a dotted texture.\\n\\na photo of a studded texture.\\n\\na photo of a freckled texture.\\n\\ncorrect rank: 2/47    correct probability: 20.50%\\nDescribable Textures Dataset (DTD)\\n\\ncorrect label: marimba\\n\\n0 20 40 60 80 100\\n\\na photo of a marimba.', 'a photo of a abacus.\\n\\na photo of a steel drum.\\n\\na photo of a computer keyboard.\\n\\na photo of a pool table.\\n\\ncorrect rank: 1/1000    correct probability: 79.54%\\nImageNet Blurry\\n\\ncorrect label: Pill bottle\\n\\n0 20 40 60 80 100\\n\\na photo of a pill bottle.', 'a photo of a bottle cap.\\n\\na photo of a beer bottle.\\n\\na photo of a pillow.\\n\\na photo of a wine bottle.\\n\\ncorrect rank: 1/113    correct probability: 98.34%\\nObjectNet ImageNet Overlap\\n\\ncorrect label: building\\n\\n0 20 40 60 80 100\\n\\na photo of a building.', 'a photo of a carriage.\\n\\na photo of a statue.\\n\\na photo of a bag.\\n\\na photo of a mug.\\n\\ncorrect rank: 1/12    correct probability: 97.69%\\naYahoo\\n\\ncorrect label: Black chinned Hummingbird\\n\\n0 20 40 60 80 100', '0 20 40 60 80 100\\n\\na photo of a broad tailed hummingbird, a type of bird.\\n\\na photo of a calliope hummingbird, a type of bird.\\n\\na photo of a costas hummingbird, a type of bird.\\n\\na photo of a black chinned hummingbird, a type of bird.', 'a photo of a annas hummingbird, a type of bird.\\n\\ncorrect rank: 4/500    correct probability: 12.00%\\nBirdsnap\\n\\ncorrect label: King Charles Spaniel\\n\\n0 20 40 60 80 100\\n\\na photo of a king charles spaniel.\\n\\na photo of a brittany dog.', 'a photo of a cocker spaniel.\\n\\na photo of a papillon.\\n\\na photo of a sussex spaniel.\\n\\ncorrect rank: 1/1000    correct probability: 91.61%\\nImageNet\\n\\ncorrect label: great masterwort\\n\\n0 20 40 60 80 100\\n\\na photo of a great masterwort, a type of flower.', 'a photo of a bishop of llandaff, a type of flower.\\n\\na photo of a pincushion flower, a type of flower.\\n\\na photo of a globe flower, a type of flower.\\n\\na photo of a prince of wales feathers, a type of flower.', 'correct rank: 1/102    correct probability: 74.25%\\nFlowers-102\\n\\ncorrect label: country line dancing\\n\\n0 20 40 60 80 100\\n\\na photo of country line dancing.\\n\\na photo of square dancing.\\n\\na photo of swing dancing.\\n\\na photo of dancing charleston.', 'a photo of salsa dancing.\\n\\ncorrect rank: 1/700    correct probability: 98.98%\\nKinetics-700\\n\\ncorrect label: kennel indoor\\n\\n0 20 40 60 80 100\\n\\na photo of a kennel indoor.\\n\\na photo of a kennel outdoor.\\n\\na photo of a jail cell.\\n\\na photo of a jail indoor.', 'a photo of a veterinarians office.\\n\\ncorrect rank: 1/723    correct probability: 98.63%\\nSUN\\n\\ncorrect label: 2012 Honda Accord Coupe\\n\\n0 20 40 60 80 100\\n\\na photo of a 2012 honda accord coupe.\\n\\na photo of a 2012 honda accord sedan.', 'a photo of a 2012 acura tl sedan.\\n\\na photo of a 2012 acura tsx sedan.\\n\\na photo of a 2008 acura tl type-s.\\n\\ncorrect rank: 1/196    correct probability: 63.30%\\nStanford Cars\\n\\ncorrect label: roundabout\\n\\n0 20 40 60 80 100\\n\\nsatellite imagery of roundabout.', 'satellite imagery of intersection.\\n\\nsatellite imagery of church.\\n\\nsatellite imagery of medium residential.\\n\\nsatellite imagery of chaparral.\\n\\ncorrect rank: 1/45    correct probability: 96.39%\\nRESISC45\\n\\ncorrect label: Belize\\n\\n0 20 40 60 80 100', '0 20 40 60 80 100\\n\\na photo i took in french guiana.\\n\\na photo i took in gabon.\\n\\na photo i took in cambodia.\\n\\na photo i took in guyana.\\n\\na photo i took in belize.\\n\\ncorrect rank: 5/211    correct probability: 3.92%\\nCountry211\\n\\ncorrect label: Boeing 717', '0 20 40 60 80 100\\n\\na photo of a mcdonnell douglas md-90, a type of aircraft.\\n\\na photo of a boeing 717, a type of aircraft.\\n\\na photo of a fokker 100, a type of aircraft.\\n\\na photo of a mcdonnell douglas dc-9-30, a type of aircraft.', 'a photo of a boeing 727-200, a type of aircraft.\\n\\ncorrect rank: 2/100    correct probability: 9.91%\\nFGVC Aircraft\\n\\ncorrect label: beer bottle\\n\\n0 20 40 60 80 100\\n\\na photo of a beer bottle.\\n\\na photo of a pirate ship.\\n\\na photo of a chocolate syrup.', 'a photo of a product packet / packaging.\\n\\na photo of a wine bottle.\\n\\ncorrect rank: 1/1000    correct probability: 88.27%\\nImageNetV2 Matched Frequency\\n\\ncorrect label: snake\\n\\n0 20 40 60 80 100\\n\\na photo of a snake.\\n\\na photo of a sweet pepper.', 'a photo of a flatfish.\\n\\na photo of a turtle.\\n\\na photo of a lizard.\\n\\ncorrect rank: 1/100    correct probability: 38.02%\\nCIFAR-100\\n\\ncorrect label: Maine Coon\\n\\n0 20 40 60 80 100\\n\\na photo of a maine coon, a type of pet.\\n\\na photo of a persian, a type of pet.', 'a photo of a ragdoll, a type of pet.\\n\\na photo of a birman, a type of pet.\\n\\na photo of a siamese, a type of pet.\\n\\ncorrect rank: 1/37    correct probability: 99.99%\\nOxford-IIIT Pets\\n\\ncorrect label: Siberian Husky\\n\\n0 20 40 60 80 100', '0 20 40 60 80 100\\n\\na photo of a siberian husky.\\n\\na photo of a german shepherd dog.\\n\\na photo of a collie.\\n\\na photo of a border collie.\\n\\na photo of a rottweiler.\\n\\ncorrect rank: 1/200    correct probability: 76.02%\\nImageNet-R (Rendition)', 'correct label: kangaroo\\n\\n0 20 40 60 80 100\\n\\na photo of a kangaroo.\\n\\na photo of a gerenuk.\\n\\na photo of a emu.\\n\\na photo of a wild cat.\\n\\na photo of a scorpion.\\n\\ncorrect rank: 1/102    correct probability: 99.81%\\nCaltech-101\\n\\ncorrect label: Volleyball Spiking', '0 20 40 60 80 100\\n\\na photo of a person volleyball spiking.\\n\\na photo of a person jump rope.\\n\\na photo of a person long jump.\\n\\na photo of a person soccer penalty.\\n\\na photo of a person table tennis shot.', 'correct rank: 1/101    correct probability: 99.30%\\nUCF101\\n\\ncorrect label: angry\\n\\n0 20 40 60 80 100\\n\\na photo of a happy looking face.\\n\\na photo of a neutral looking face.\\n\\na photo of a surprised looking face.\\n\\na photo of a fearful looking face.', 'a photo of a angry looking face.\\n\\ncorrect rank: 5/7    correct probability: 8.16%\\nFacial Emotion Recognition 2013 (FER2013)\\n\\ncorrect label: 4\\n\\n0 20 40 60 80 100\\n\\na photo of 3 objects.\\n\\na photo of 4 objects.\\n\\na photo of 5 objects.\\n\\na photo of 6 objects.', 'a photo of 10 objects.\\n\\ncorrect rank: 2/8    correct probability: 17.11%\\nCLEVR Count\\n\\ncorrect label: bird\\n\\n0 20 40 60 80 100\\n\\na photo of a bird.\\n\\na photo of a cat.\\n\\na photo of a deer.\\n\\na photo of a frog.\\n\\na photo of a dog.', 'a photo of a dog.\\n\\ncorrect rank: 1/10    correct probability: 40.86%\\nCIFAR-10\\n\\ncorrect label: lynx\\n\\n0 20 40 60 80 100\\n\\na photo of a fox squirrel.\\n\\na photo of a mongoose.\\n\\na photo of a skunk.\\n\\na photo of a red fox.\\n\\na photo of a lynx.', 'a photo of a lynx.\\n\\ncorrect rank: 5/200    correct probability: 4.18%\\nImageNet-A (Adversarial)\\n\\ncorrect label: healthy lymph node tissue\\n\\n0 20 40 60 80 100\\n\\nthis is a photo of lymph node tumor tissue\\n\\nthis is a photo of healthy lymph node tissue', 'correct rank: 2/2    correct probability: 22.81%\\nPatchCamelyon (PCam)\\n\\ncorrect label: annual crop land\\n\\n0 20 40 60 80 100\\n\\na centered satellite photo of permanent crop land.\\n\\na centered satellite photo of pasture land.', 'a centered satellite photo of highway or road.\\n\\na centered satellite photo of annual crop land.\\n\\na centered satellite photo of brushland or shrubland.\\n\\ncorrect rank: 4/10    correct probability: 12.90%\\nEuroSAT\\n\\ncorrect label(s): airplane,person', '0 20 40 60 80 100\\n\\na photo of a airplane.\\n\\na photo of a bird.\\n\\na photo of a bear.\\n\\na photo of a giraffe.\\n\\na photo of a car.\\n\\ncorrect rank: 1/23    correct probability: 88.98%\\nYoutube-BB\\n\\ncorrect label: television studio\\n\\n0 20 40 60 80 100', '0 20 40 60 80 100\\n\\na photo of a television studio.\\n\\na photo of a podium indoor.\\n\\na photo of a conference room.\\n\\na photo of a lecture room.\\n\\na photo of a control room.\\n\\ncorrect rank: 1/397    correct probability: 90.22%\\nSUN397\\n\\ncorrect label: guacamole', '0 20 40 60 80 100\\n\\na photo of guacamole, a type of food.\\n\\na photo of ceviche, a type of food.\\n\\na photo of edamame, a type of food.\\n\\na photo of tuna tartare, a type of food.\\n\\na photo of hummus, a type of food.', 'correct rank: 1/101    correct probability: 90.15%\\nFood101', 'Figure 21. Visualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting\\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\\nthe class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect\\nprediction is colored orange.', 'Learning Transferable Visual Models From Natural Language Supervision 43\\n\\nFo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nSt\\nan\\n\\nfo\\nrd\\n\\nC\\nar\\n\\ns\\n\\nFG\\nV\\n\\nC\\nA\\n\\nir\\ncr\\n\\naf\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nO\\nxf\\n\\nor\\nd\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1', 'C\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns1\\n\\n02\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nam\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nR\\nen\\n\\nde\\nre\\n\\nd\\nSS\\n\\nT\\n2\\n\\nIm\\nag\\n\\neN\\net', 'T\\n2\\n\\nIm\\nag\\n\\neN\\net\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nes\\nN', 'et RN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\\nRN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2', 'RN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\\nRN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\\nRN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6', 'C\\nL\\n\\nIP\\n-V', 'iT B/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\\nB/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\\nL/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3', 'L/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\\n\\nTable 11. Zero-shot performance of CLIP models over 27 datasets.\\n\\n101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy', '85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFood101\\n\\n101 102\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n101 102\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n101 102\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n101 102\\n\\n60\\n\\n62\\n\\n64\\n\\n66\\n\\n68\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n101 102\\n50\\n\\n60\\n\\n70\\n\\n80', '60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n101 102\\n\\n20\\n\\n30\\n\\n40\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n101 102\\n82.0\\n\\n82.5\\n\\n83.0\\n\\n83.5\\n\\n84.0\\n\\n84.5\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes PascalVOC2007\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy', '60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n101 102\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n101 102\\n82\\n84\\n86\\n88\\n90\\n92\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n101 102\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n101 102\\n50\\n\\n60\\n\\n70', '101 102\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n101 102\\n\\n95\\n\\n96\\n\\n97\\n\\n98\\n\\n99\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n101 102\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n101 102\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy', '70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n101 102\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n101 102\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nPatchCamelyon\\n\\n101 102\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60', '45\\n\\n50\\n\\n55\\n\\n60\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n101 102\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n101 102\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n101 102\\n\\nGFLOPs/image\\n\\n54\\n\\n56\\n\\n58\\n\\n60\\n\\n62\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n101 102', '101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n101 102\\n\\nGFLOPs/image\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nResNet\\n\\nFigure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance', 'Learning Transferable Visual Models From Natural Language Supervision 44', 'B. Zero-Shot Prediction\\nTo provide a qualitative summary / overview of CLIP’s zero-\\nshot performance we visualize a randomly selected predic-\\ntion for 36 different zero-shot CLIP classifiers in Figure\\n21. In addition, Table 11 and Figure 22 show the individual\\nzero-shot performance scores for each dataset.', 'C. Duplicate Detector\\nOur early attempts at duplicate detection and analysis used\\nnearest neighbors in the model’s learned embedding space.\\nWhile it is intuitive to use a model’s own notion of similar-\\nity, we encountered issues. We found the model’s feature\\nspace is weighted very heavily towards semantic similar-\\nity. Many false positives occurred due to distinct objects\\nthat would be described similarly (soccer balls, flowers of\\nthe same species, etc...) having almost perfect similarity.\\nWe also observed the model was quite poor at assigning\\ncertain kinds of near-duplicates high similarity scores. We\\nnoticed repeatedly that images with high-frequency textures\\n(such as fur or stripe patterns) pre-processed by different\\nresizing algorithms (nearest neighbor vs bi-linear) could\\nhave surprisingly low similarity. This resulted in many false\\nnegatives.', 'We built our own near-duplicate detector to fix this issue.\\nWe created a synthetic data augmentation pipeline that com-\\nbined a variety of common image manipulations. The aug-\\nmentation pipeline combines random cropping and zooming,\\naspect ratio distortion, downsizing and upscaling to different\\nresolutions, minor rotations, jpeg compression, and HSV\\ncolor jitter. The pipeline also randomly selects from differ-\\nent interpolation algorithms for all relevant steps. We then\\ntrained a model to maximize the similarity of an image and\\nits transformed variant while minimizing similarity to all\\nother images in a training batch. We used the same n-pair /\\nInfoNCE loss as CLIP but with a fixed temperature of 0.07.', 'We selected a ResNet-50 as the model architecture. We\\nmodified the base ResNet-50 with the anti-alias improve-\\nments from (Zhang, 2019) and used weight norm (Sali-\\nmans & Kingma, 2016) instead of batch norm (Ioffe &\\nSzegedy, 2015) to avoid leaking information about dupli-\\ncates via batch statistics - a problem previously noted in\\n(Henaff, 2020). We also found the GELU activation func-\\ntion (Hendrycks & Gimpel, 2016) to perform better for this\\ntask. We trained the model with a total batch size of 1,712\\nfor approximately 30 million images sampled from our pre-\\ntraining dataset. At the end of training it achieves nearly\\n100% accuracy on its proxy training task.', 'Linear Classifier Zero Shot\\nDataset YFCC WIT ∆ YFCC WIT ∆', 'Birdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\\nCountry211 23.1 17.3 +5.8 5.2 5.3 +0.1\\nFlowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\\nGTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1\\nUCF101 69.2 74.9 −5.7 22.9 32.0 −9.1\\nStanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1', 'ImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7\\nDataset Average 65.5 66.6 −1.1 29.6 30.0 −0.4\\nDataset “Wins” 10 15 −5 19 18 +1', 'Table 12. CLIP performs similarly when trained on only\\nYFCC100M. Comparing a ResNet-50 trained on only\\nYFCC100M with a same sized subset of WIT shows simi-\\nlar average performance and number of wins on zero shot and\\nlinear classifier evals. However, large differences in dataset\\nspecific performance occur. We include performance on the 3\\ndatasets where YFCC does best and worst compared to WIT\\naccording to a linear probe in order to highlight this as well as\\naggregate performance across all linear and zero-shot evals and\\nthe canonical ImageNet dataset.', 'D. Dataset Ablation on YFCC100M\\nTo study whether our custom dataset is critical to the perfor-\\nmance of CLIP, we trained a model on a filtered subset of\\nthe YFCC100M dataset (details described in Section 2.2)\\nand compared its performance to the same model trained\\non an equally sized subset of WIT. We train each model for\\n32 epochs at which point transfer performance begins to\\nplateau due to overfitting. Results are shown in Table 12.\\nAcross our whole eval suite, YFCC and WIT perform simi-\\nlarly on average for both zero-shot and linear probe settings.\\nHowever, performance on specific fine-grained classifica-\\ntion datasets can vary widely - sometimes by over 10%.\\nOur speculation is that these differences in performance re-\\nflect the relative density of relevant data in each pre-training\\ndataset. For instance, pre-training on YFCC100M, which\\nmight contain many photos of birds and flowers (common\\nsubjects for photographers), results in better performance on\\nBirdsnap and Flowers102, while pre-training on WIT results\\nin better car and pet classifiers (which appear common in\\nour dataset).', 'Overall, these results are encouraging as they suggest our\\napproach can use any reasonably filtered collection of paired\\n(text, image) data. This mirrors recent work which reported\\npositive results using the same contrastive pre-training ob-\\njective on the relatively different domain of medical imaging\\n(Zhang et al., 2020). It also is similar to the findings of noisy\\nstudent self-training which reported only slight improve-\\nments when using their JFT300M dataset over YFCC100M\\n(Xie et al., 2020). We suspect the major advantage of our\\ndataset over the already existing YFCC100M is its much\\nlarger size.', 'Learning Transferable Visual Models From Natural Language Supervision 45', 'Finally, we caution that WIT includes this filtered subset\\nof YFCC100M. This could result in our ablation under-\\nestimating the size of performance differences between\\nYFCC100M and the rest of WIT. We do not think this is\\nlikely as YFCC100M is only 3.7% of the overall WIT data\\nblend and it did not noticeably change the performance of\\nmodels when it was added to the existing data blend during\\nthe creation of WIT.', 'E. Selected Task and Dataset Results\\nDue to the large variety of datasets and experiments consid-\\nered in this work, the main body focuses on summarizing\\nand analyzing overall results. In the following subsections\\nwe report details of performance for specific groups of tasks,\\ndatasets, and evaluation settings.', 'E.1. Image and Text Retrieval', 'CLIP pre-trains for the task of image-text retrieval on our\\nnoisy web-scale dataset. Although the focus of this paper\\nis on representation learning and task learning for the pur-\\npose of transfer to a wide variety of downstream datasets,\\nvalidating that CLIP is able to achieve high transfer perfor-\\nmance transfer on exactly what it is pre-trained for is an\\nimportant sanity check / proof of concept. In Table 13 we\\ncheck the zero-shot transfer performance of CLIP for both\\ntext and image retrieval on the Flickr30k and MSCOCO\\ndatsets. Zero-shot CLIP matches or outperforms all prior\\nzero-shot results on these two datasets. Zero-shot CLIP is\\nalso competitive with the current overall SOTA for the task\\nof text retrieval on Flickr30k. On image retrieval, CLIP’s\\nperformance relative to the overall state of the art is notice-\\nably lower. However, zero-shot CLIP is still competitive\\nwith a fine-tuned Unicoder-VL. On the larger MS-COCO\\ndataset fine-tuning improves performance significantly and\\nzero-shot CLIP is not competitive with the most recent work.\\nFor both these datasets we prepend the prompt “a photo\\nof” to the description of each image which we found boosts\\nCLIP’s zero-shot R@1 performance between 1 and 2 points.', 'E.2. Optical Character Recognition', 'Although visualizations have shown that ImageNet models\\ncontain features that respond to the presence of text in an\\nimage (Zeiler & Fergus, 2014), these representations are\\nnot sufficiently fine-grained to use for the task of optical\\ncharacter recognition (OCR). To compensate, models are\\naugmented with the outputs of custom OCR engines and\\nfeatures to boost performance on tasks where this capability\\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\\ning the development of CLIP, we noticed that CLIP began to\\nlearn primitive OCR capabilities which appeared to steadily\\nimprove over the course of the project. To evaluate this\\nqualitatively noticed behavior, we measured performance', 'on 5 datasets requiring the direct and indirect use of OCR.\\nThree of these datasets MNIST (LeCun), SVHN (Netzer\\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\\nthe ability of a model to perform low-level character and\\nword recognition, while Hateful Memes (Kiela et al., 2020)\\nand SST-2 (Socher et al., 2013) check the ability of a model\\nto use OCR to perform a semantic task. Results are reported\\nin Table 14.', 'CLIP’s performance is still highly variable and appears to\\nbe sensitive to some combination of the domain (rendered or\\nnatural images) and the type of text to be recognized (num-\\nbers or words). CLIP’s OCR performance is strongest Hate-\\nful Memes and SST-2 - datasets where the text is digitally\\nrendered and consists mostly of words. On IIIT5K, which\\nis natural images of individually cropped words, zero-shot\\nCLIP performs a bit more respectively and its performance\\nis similar to Jaderberg et al. (2014) early work combining\\ndeep learning and structured prediction to perform open-\\nvocabulary OCR. However, performance is noticeably lower\\non two datasets involving recognition of hand written and\\nstreet view numbers. CLIP’s 51% accuracy on full number\\nSVHN is well below any published results. Inspection sug-\\ngests CLIP struggles with repeated characters as well as the\\nlow resolution and blurry images of SVHN. CLIP’s zero-\\nshot MNIST performance is also poor and is outperformed\\nby supervised logistic regression on raw pixels, one of the\\nsimplest possible machine learning baselines.', 'SST-2 is a sentence level NLP dataset which we render into\\nimages. We include SST-2 in order to check whether CLIP\\nis able to convert low level OCR capability into a higher\\nlevel representation. Fitting a linear classifier on CLIP’s rep-\\nresentation of rendered sentences achives 80.5% accuracy.\\nThis is on par with the 80% accuracy of a continuous bag\\nof words baseline using GloVe word vectors pre-trained on\\n840 billion tokens (Pennington et al., 2014). While this is a\\nsimple NLP baseline by today’s standard, and well below\\nthe 97.5% of the current SOTA, it is encouraging to see\\nthat CLIP is able to turn an image of rendered text into a\\nnon-trivial sentence level representation. Fully supervised\\nCLIP is also surprisingly strong on Hateful Meme detec-\\ntion, where CLIP is only 0.7 points behind the current single\\nmodel SOTA and several points above the best baseline from\\nthe original paper. Similar to SST-2, these other results on\\nHateful Memes use the ground truth text which CLIP does\\nnot have access to. Finally, we note that zero-shot CLIP\\noutperforms the best results using fully supervised linear\\nprobes across all other 56 models included in our evaluation\\nsuite. This suggests CLIP’s OCR capability is at least some-\\nwhat unique compared to existing work on self-supervised\\nand supervised representation learning.', 'Learning Transferable Visual Models From Natural Language Supervision 46\\n\\nText Retrieval Image Retrieval\\nFlickr30k MSCOCO Flickr30k MSCOCO\\n\\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\\n\\nFi\\nne\\n\\ntu\\nne', 'Unicoder-VLa 86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\\nUniterb 87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\\nVILLAc 87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\\nOscard - - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\\nERNIE-ViLe 88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -', 'Z\\ner\\n\\no-\\nSh', 'ot Visual N-Gramsf 15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\\nImageBERTg - - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\\nUnicoder-VLa 64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - -\\nUniterb 83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\\nCLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2', 'Table 13. CLIP improves zero-shot retrieval and is competitive with the best fine-tuned result on Flickr30k text retrieval. Bold\\nindicates best overall performance while an underline indicates best in category performance (zero-shot or fine-tuned). For all other\\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\\na(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f (Li et al., 2017) g(Qi et al., 2020)', 'IIIT5K Hateful\\nMNIST SVHN 1k Memes SST-2\\n\\nFi\\nne\\n\\ntu\\nne SOTA 99.8a 96.4b 98.9c 78.0d 97.5e\\n\\nJOINTf - - 89.6 - -\\nCBoWg - - - - 80.0\\n\\nL\\nin\\n\\nea\\nr Raw Pixels 92.5 - - - -\\n\\nES Best 98.9h - - 58.6h 59.0i\\n\\nCLIP 99.2 - - 77.3 80.5\\n\\nZ\\nS', 'Z\\nS\\n\\nCLIP 88.4 51.0 90.0 63.3 67.9', 'Table 14. OCR performance on 5 datasets. All metrics are accuracy\\non the test set except for Hateful Memes which reports ROC AUC\\non the dev set. Single model SOTA reported to best of knowledge.\\nES Best reports the best performance across the 56 non-CLIP\\nmodels in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al.,\\n2015) c(Wang et al., 2020) d(Lippe et al., 2020) f (Jaderberg et al.,\\n2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\\n2018)', 'E.3. Action Recognition in Videos', 'For the purpose of learning, a potentially important aspect\\nof natural language is its ability to express, and therefore su-\\npervise, an extremely wide set of concepts. A CLIP model,\\nsince it is trained to pair semi-arbitrary text with images, is\\nlikely to receive supervision for a wide range of visual con-\\ncepts involving both common and proper nouns, verbs, and\\nadjectives. ImageNet-1K, by contrast, only labels common\\nnouns. Does the lack of broader supervision in ImageNet\\nresult in weaker transfer of ImageNet models to tasks involv-\\ning the recognition of visual concepts that are not nouns?', 'To investigate this, we measure and compare the perfor-\\nmance of CLIP and ImageNet models on several video\\n\\nUCF101 K700 RareAct\\nTop-1 AVG mWAP mWSAP\\n\\nFi\\nne\\n\\ntu\\nne R(2+1)D-BERTa 98.7 - - -', 'NS ENet-L2b - 84.8 - -\\nHT100M S3Dd 91.3 - - -\\nBaseline I3De - 70.2 - -\\n\\nL\\nin\\n\\nea\\nr MMV FACf 91.8 - - -\\n\\nNS ENet-L2c 89.4c 68.2c - -\\nCLIP 92.0 73.0 - -\\n\\nZ\\nS HT100M S3Dd - - 30.5 34.8\\n\\nCLIP 80.3 69.6 40.7 44.8', 'Table 15. Action recognition performance on 3 video datasets. Sin-\\ngle model SOTA reported to best of knowledge. Note that linear\\nCLIP and linear NS ENet-L2 are trained and evaluated on a single\\nframe subsampled version of each dataset and not directly compa-\\nrable to prior work. On Kinetics-700, we report the ActivityNet\\ncompetition metric which is the average of top-1 and top-5 per-\\nformance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al.,\\n2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f (Alayrac\\net al., 2020)', 'action classification datasets which measure the ability of a\\nmodel to recognize verbs. In Table 15 we report results on\\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\\net al., 2019), two common datasets for the task. Unfortu-\\nnately, our CPU based linear classifier takes a prohibitively\\nlong time to evaluate on a video dataset due to the very large\\nnumber of training frames. To deal with this, we aggres-\\nsively sub-sample each video to only a single center frame,\\neffectively turning it into an image classification dataset.\\nAs a result, our reported performance in a linear evaluation\\nsetting likely under estimates performance by a moderate\\namount.', 'Learning Transferable Visual Models From Natural Language Supervision 47\\n\\nIN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\\nTop-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10', 'NS EfficientNet-L2a 88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\\nFixResNeXt101-32x48d V2b 86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\\nLinear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\\nZero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5', 'Table 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)', 'Despite this handicap, CLIP features transfer surprisingly\\nwell to this task. CLIP matches the best prior result on UCF-\\n101 in a linear probe evaluation setting and also outperforms\\nall other models in our evaluation suite. On Kinetics-700,\\nCLIP also outperforms the fine-tuned I3D baseline from the\\noriginal paper. Since it does not require a training stage,\\nwe report CLIP’s zero-shot performance when averaging\\npredictions across all frames. CLIP also performs well in\\nthis setting and on Kinetics-700 its performance is within\\n1% of the fully supervised I3D baseline which is trained\\non 545000 labeled videos. Encouraged by these results, we\\nalso measure CLIP’s performance on the recently introduced\\nRareAct dataset (Miech et al., 2020a) which was designed\\nto measure zero-shot recognition of unusual actions like\\n“hammering a phone” and “drilling an egg”. CLIP improves\\nover the prior state of the art, a S3D model trained on auto-\\nmatically extracted captions from 100 million instructional\\nvideos, by 10 points.', 'While CLIP has encouragingly strong performance on the\\ntask of action recognition, we note that there are many differ-\\nences between the models being compared beyond just their\\nform of supervision such as model architecture, training\\ndata distribution, dataset size, and compute used. Further\\nwork is needed to more precisely determine what specific\\ndesign decisions contribute to achieving high performance\\non this task.', '1km 25km 200km 750km 2500km\\n\\nISNsa 16.9 43.0 51.9 66.7 80.2\\nCPlaNetb 16.5 37.1 46.4 62.0 78.5\\nCLIP 13.9 32.9 43.0 62.0 79.3\\nDeep-Ret+c 14.4 33.3 47.7 61.6 73.4\\nPlaNetd 8.4 24.5 37.6 53.6 71.3', 'Table 17. Geolocalization performance on the IM2GPS test set.\\nMetric is percent of images localized within a given radius. Models\\nare ordered by average performance. a(Muller-Budack et al., 2018)\\nb(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al.,\\n2016)', 'E.4. Geolocalization', 'Another behavior we noticed during the development of\\nCLIP was its ability to recognize many places and locations.\\nTo quantify this we created the Country211 dataset as de-\\nscribed in Appendix A and report results on it throughout\\nthe paper. However it is a new benchmark so to compare\\nwith prior work on geolocalization we also report results\\non the IM2GPS test set from Hays & Efros (2008) in Table\\n17. Since IM2GPS is a regression benchmark, we guess the\\nGPS coordinates of the nearest image in a set of reference\\nimages using CLIP’s embedding space. This is not a zero-\\nshot result since it uses nearest-neighbor regression. Despite\\nquerying only 1 million images, which is much less than\\nprior work, CLIP performs similarly to several task specific\\nmodels. It is not, however, competitive with the current state\\nof the art.', 'E.5. Robustness to Distribution Shift', 'Section 3.3 provides a high level summary and analysis of\\nImageNet-related robustness results. We briefly provide\\nsome additional numerical details in this appendix. Per-\\nformance results per dataset are provided in Table 16 and\\ncompared with the current state of the art results reported\\nin Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\\nBB. CLIP’s improvements are largest on ImageNet-Vid and\\nYoutube-BB due to its flexible zero-shot capability and on\\nImageNet-R, which likely reflects CLIP’s pre-training dis-\\ntribution including significant amounts of creative content.\\nA similar behavior has been documented for the Instagram\\npre-trained ResNeXt models as discussed in Taori et al.\\n(2020).', 'Learning Transferable Visual Models From Natural Language Supervision 48\\n\\nF. Model Hyperparameters\\n\\nHyperparameter Value', 'Hyperparameter Value\\n\\nBatch size 32768\\nVocabulary size 49408\\nTraining epochs 32\\nMaximum temperature 100.0\\nWeight decay 0.2\\nWarm-up iterations 2000\\nAdam β1 0.9\\nAdam β2 0.999 (ResNet), 0.98 (ViT)\\nAdam ε 10−8 (ResNet), 10−6 (ViT)', 'Table 18. Common CLIP hyperparameters\\n\\nLearning Embedding Input ResNet Text Transformer\\nModel rate dimension resolution blocks width layers width heads', 'RN50 5× 10−4 1024 224 (3, 4, 6, 3) 2048 12 512 8\\nRN101 5× 10−4 512 224 (3, 4, 23, 3) 2048 12 512 8\\nRN50x4 5× 10−4 640 288 (4, 6, 10, 6) 2560 12 640 10\\nRN50x16 4× 10−4 768 384 (6, 8, 18, 8) 3072 12 768 12\\nRN50x64 3.6× 10−4 1024 448 (3, 15, 36, 10) 4096 12 1024 16', 'Table 19. CLIP-ResNet hyperparameters\\n\\nLearning Embedding Input Vision Transformer Text Transformer\\nModel rate dimension resolution layers width heads layers width heads', 'ViT-B/32 5× 10−4 512 224 12 768 12 12 512 8\\nViT-B/16 5× 10−4 512 224 12 768 12 12 512 8\\nViT-L/14 4× 10−4 768 224 24 1024 16 12 768 12\\nViT-L/14-336px 2× 10−5 768 336 24 1024 16 12 768 12\\n\\nTable 20. CLIP-ViT hyperparameters']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size = 256\n",
    "chunk_overlap  = 20\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap\n",
    ")\n",
    "docs = text_splitter.create_documents(sentences)\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    docs = text_splitter.create_documents([doc.page_content])\n",
    "    chunks.extend([doc.page_content for doc in docs])\n",
    "print(\"printing chunks\\n\")\n",
    "print (chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.\\n\\n“Beware the Jabberwock, my son!\\n      The jaws that bite, the claws that catch!\\nBeware the Jubjub bird, and shun\\n      The frumious Bandersnatch!”\\n\\nHe took his vorpal sword in hand;\\n      Long time the manxome foe he sought—\\nSo rested he by the Tumtum tree\\n      And stood awhile in thought.\\n\\nAnd, as in uffish thought he stood,\\n      The Jabberwock, with eyes of flame,\\nCame whiffling through the tulgey wood,\\n      And burbled as it came!\\n\\nOne, two! One, two! And through and through\\n      The vorpal blade went snicker-snack!\\nHe left it dead, and with its head\\n      He went galumphing back.\\n\\n“And hast thou slain the Jabberwock?\\n      Come to my arms, my beamish boy!\\nO frabjous day! Callooh! Callay!”\\n      He chortled in his joy.\\n\\n’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.\\n\\n',\n",
       " '\\nIt was the best of times, it was the worst of times, \\nit was the age of wisdom, it was the age of foolishness, \\nit was the epoch of belief, it was the epoch of incredulity, \\nit was the season of light, it was the season of darkness, \\nit was the spring of hope, it was the winter of despair, \\nwe had everything before us, we had nothing before us, \\nwe were all going direct to heaven, \\nwe were all going direct the other way–in short, \\nthe period was so far like the present period, \\nthat some of its noisiest authorities insisted on its being received, \\nfor good or for evil, in the superlative degree of comparison only.\\n',\n",
       " '\\nI wish you to know that you have been the last dream of my soul. \\nIn my degradation I have not been so degraded but that the sight \\nof you with your father, and of this home made such a home by you, \\nhas stirred old shadows that I thought had died out of me. \\nSince I knew you, I have been troubled by a remorse that I \\nthought would never reproach me again, and have heard whispers \\nfrom old voices impelling me upward, that I thought were silent \\nfor ever. I have had unformed ideas of striving afresh, beginning anew, \\nshaking off sloth and sensuality, and fighting out the abandoned fight. \\nA dream, all a dream, that ends in nothing, and leaves the sleeper \\nwhere he lay down, but I wish you to know that you inspired it.\\n',\n",
       " '\\nA wonderful fact to reflect upon, that every human creature is \\nconstituted to be that profound secret and mystery to every other. \\n',\n",
       " \"\\nThe dog is a gentleman; I hope to go to his heaven not man's.\\n\",\n",
       " 'If a man aspires towards a righteous life, his first act of abstinence is from injury to animals.',\n",
       " '\\nTweedledum and Tweedledee: She then meets the fat twin brothers \\nTweedledum and Tweedledee, whom she knows from the nursery rhyme. \\nAfter reciting the long poem \"The Walrus and the Carpenter\", \\nthey draw Alice\\'s attention to the Red King—loudly snoring away \\nunder a nearby tree—and maliciously provoke her with idle philosophical \\nbanter that she exists only as an imaginary figure in the Red King\\'s dreams. \\nFinally, the brothers begin suiting up for battle, only to be frightened \\naway by an enormous crow, as the nursery rhyme about them predicts.\\n',\n",
       " '\\nGolden retrievers are not bred to be guard dogs, and considering the size of their hearts and their irrepressible joy and life, they are less likely to bite than to bark, less likely to bark than to lick a hand in greeting. In spite of their size, they think they are lap dogs, and in spite of being dogs, they think they’re also human, and nearly every human they meet is judged to have the potential to be a boon companion who might at any moment, cry, “Let’s go!” and lead them on a great adventure.\\n',\n",
       " '\\nIf you’re lucky, a golden retriever will come into your life, steal your heart, and change everything\\n',\n",
       " '\\nMy friend Phil has a theory that the Lord, having made teenagers, felt constrained to make amends and so created the golden retriever.\\n',\n",
       " '\\nIf you don’t believe that dogs have souls, you haven’t looked into their eyes long enough.\\n',\n",
       " \"\\nA thing of beauty is a joy for ever:\\nIts loveliness increases; it will never\\nPass into nothingness; but still will keep\\nA bower quiet for us, and a sleep\\nFull of sweet dreams, and health, and quiet breathing.\\nTherefore, on every morrow, are we wreathing\\nA flowery band to bind us to the earth,\\nSpite of despondence, of the inhuman dearth\\nOf noble natures, of the gloomy days,\\nOf all the unhealthy and o'er-darkn'd ways\\nMade for our searching: yes, in spite of all,\\nSome shape of beauty moves away the pall\\nFrom our dark spirits. Such the sun, the moon,\\nTrees old and young, sprouting a shady boon\\nFor simple sheep; and such are daffodils\\nWith the green world they live in; and clear rills\\nThat for themselves a cooling covert make\\n'Gainst the hot season; the mid-forest brake,\\nRich with a sprinkling of fair musk-rose blooms:\\nAnd such too is the grandeur of the dooms\\nWe have imagined for the mighty dead;\\nAn endless fountain of immortal drink,\\nPouring unto us from the heaven's brink\\n\",\n",
       " '\\nThe dominant sequence transduction models are based on \\ncomplex recurrent or convolutional neural networks in an encoder-decoder configuration. \\nThe best performing models also connect the encoder and decoder through \\nan attention mechanism. We propose a new simple network architecture, \\nthe Transformer, based solely on attention mechanisms, \\ndispensing with recurrence and convolutions entirely. \\nExperiments on two machine translation tasks show these models \\nto be superior in quality while being more parallelizable \\nand requiring significantly less time to train. \\nOur model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, \\nimproving over the existing best results, including ensembles by over 2 BLEU. \\nOn the WMT 2014 English-to-French translation task, our model establishes \\na new single-model state-of-the-art BLEU score of 41.8 after training for \\n3.5 days on eight GPUs, a small fraction of the training costs of the \\nbest models from the literature. We show that the Transformer \\ngeneralizes well to other tasks by applying it successfully to \\nEnglish constituency parsing both with large and limited training data.\\n\\n',\n",
       " '\\nIn machine learning, backpropagation\\n(backprop,[1] BP) is a widely used\\nalgorithm for training feedforward\\nartificial neural networks.\\nGeneralizations of backpropagation\\nexist for other artificial neural\\nnetworks (ANNs), and for functions\\ngenerally. These classes of algorithms\\nare all referred to generically as\\n\"backpropagation\".[2] In fitting a\\nneural network, backpropagation\\ncomputes the gradient of the loss\\nfunction with respect to the weights of\\nthe network for a single input–output\\nexample, and does so efficiently,\\nunlike a naive direct computation of\\nthe gradient with respect to each\\nweight individually. This efficiency\\nmakes it feasible to use gradient\\nmethods for training multilayer\\nnetworks, updating weights to minimize\\nloss; gradient descent, or variants\\nsuch as stochastic gradient descent,\\nare commonly used. The backpropagation\\nalgorithm works by computing the\\ngradient of the loss function with\\nrespect to each weight by the chain\\nrule, computing the gradient one layer\\nat a time, iterating backward from the\\nlast layer to avoid redundant\\ncalculations of intermediate terms in\\nthe chain rule; this is an example of\\ndynamic programming.[3]\\n',\n",
       " '\\nShe dwelt among the untrodden ways\\nBeside the springs of Dove,\\nA Maid whom there were none to praise\\nAnd very few to love:\\n\\nA violet by a mossy stone\\nHalf hidden from the eye!\\n—Fair as a star, when only one\\nIs shining in the sky.\\n\\nShe lived unknown, and few could know\\nWhen Lucy ceased to be;\\nBut she is in her grave, and, oh,\\nThe difference to me!\\n\\n',\n",
       " \"\\nWhat is this life if, full of care,\\nWe have no time to stand and stare.\\n\\nNo time to stand beneath the boughs\\nAnd stare as long as sheep or cows.\\n\\nNo time to see, when woods we pass,\\nWhere squirrels hide their nuts in grass.\\n\\nNo time to see, in broad daylight,\\nStreams full of stars, like skies at night.\\n\\nNo time to turn at Beauty's glance,\\nAnd watch her feet, how they can dance.\\n\\nNo time to wait till her mouth can\\nEnrich that smile her eyes began.\\n\\nA poor life this if, full of care,\\nWe have no time to stand and stare.\\n\\n \\n\",\n",
       " '\\nKapil Dev Ramlal Nikhanj (Pronunciation: [kəpiːl deːʋ] born 6 January 1959) is an Indian former cricketer. One of the greatest all-rounders in the history of cricket, he was a fast-medium bowler and a hard-hitting middle-order batsman. Dev is the only player in the history of cricket to have taken more than 400 wickets (434 wickets) and scored more than 5,000 runs in Test.[4]\\n\\nDev captained the Indian cricket team that won the 1983 Cricket World Cup,[5] becoming the first Indian captain to win the Cricket World Cup. He is still the youngest captain (at the age of 24) to win the World Cup for any team.[6] He retired in 1994, as the first player to take 200 ODI wickets,[7] and holding the world record for the highest number of wickets taken in Test cricket, a record subsequently broken by Courtney Walsh in 2000.[8] Kapil Dev still holds the record for the highest individual score i.e. 175* scored by a batsman batting at number 5 or lower in ODIs.[9]\\n\\nAfter retiring, he coached the Indian national team between September 1999 and September 2000.[10][11]\\n\\nIn 1982, Dev was awarded the Padma Shri, and in 1991 the Padma Bhushan. In 2002, he was named by Wisden as the Indian Cricketer of the Century. On 11 March 2010, Dev was inducted into the ICC Cricket Hall of Fame.[12] In 2013, he received the C. K. Nayudu Lifetime Achievement Award, the highest honour conferred by BCCI on a former player.[13]\\n\\nEarly life\\nKapildev Ramlal Nikhanj[14] was born in a[15] family from Punjab to his father Ram Lal Nikhanj, a teak merchant and his wife, Rajkumari in Chandigarh on 6 January 1959. His family moved to Fazilka after the Partition before eventually moving to Chandigarh. His paternal family is from Montgomery (now known as Sahiwal) and his mother was born in Pakpattan, Okara.[16][17][18] Dev was a student at D.A.V.College.[19][20]\\n',\n",
       " '\\nSachin Ramesh Tendulkar, (/ˌsʌtʃɪn tɛnˈduːlkər/ i; pronounced [sətɕin teːɳɖulkəɾ]; born 24 April 1973) is an Indian former international cricketer who captained the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket.[4] He is the all-time highest run-scorer in both ODI and Test cricket with more than 18,000 runs and 15,000 runs, respectively.[5] He also holds the record for receiving the most man-of-the-match awards in international cricket.[6] Tendulkar was a Member of Parliament, Rajya Sabha by nomination from 2012 to 2018.[7][8]\\n\\nTendulkar took up cricket at the age of eleven, made his Test match debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for over 24 years.[9] In 2002, halfway through his career, Wisden ranked him the second-greatest Test batsman of all time, behind Don Bradman, and the second-greatest ODI batsman of all time, behind Viv Richards.[10] The same year, Tendulkar was a part of the team that was one of the joint-winners of the 2002 ICC Champions Trophy. Later in his career, Tendulkar was part of the Indian team that won the 2011 Cricket World Cup, his first win in six World Cup appearances for India.[11] He had previously been named \"Player of the Tournament\" at the 2003 World Cup.\\n\\nTendulkar has received several awards from the government of India: the Arjuna Award (1994), the Khel Ratna Award (1997), the Padma Shri (1998), and the Padma Vibhushan (2008).[12][13] After Tendulkar played his last match in November 2013, the Prime Minister\\'s Office announced the decision to award him the Bharat Ratna, India\\'s highest civilian award.[14][15] He was the first sportsperson to receive the reward and, as of 2023, is the youngest recipient.[16][17][18] In 2010, Time included Tendulkar in its annual list of the most influential people in the world.[19] Tendulkar was awarded the Sir Garfield Sobers Trophy for cricketer of the year at the 2010 International Cricket Council (ICC) Awards.[20]\\n\\nHaving retired from ODI cricket in 2012,[21][22] he retired from all forms of cricket in November 2013 after playing his 200th Test match.[23] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[24] In 2013, Tendulkar was included in an all-time Test World XI to mark the 150th anniversary of Wisden Cricketers\\' Almanack, and he was the only specialist batsman of the post–World War II era, along with Viv Richards, to get featured in the team.[25] In 2019, he was inducted into the ICC Cricket Hall of Fame.[26] On 24 April 2023, the Sydney Cricket Ground unveiled a set of gates named after Tendulkar and Brian Lara on the occasion of Tendulkar\\'s 50th birthday and the 30th anniversary of Lara\\'s inning of 277 at the ground.[27][28][29]\\n',\n",
       " '\\nShubman Gill (born 8 September 1999) is an Indian cricketer. Representing Indian cricket team at the international level, he also plays for Gujarat Titans in the Indian Premier League and Punjab in domestic cricket. Gill served as the vice-captain of the Indian Under-19 cricket team in the 2018 Under-19 Cricket World Cup and won Player of the Tournament award. A right-handed opening batsman, he is considered one of the best young cricketers in the world. He is nicknamed \"The Prince\" for the success he achieved in his career still now.[1] Gill holds the record for youngest cricketer to score a double century in One Day International cricket.[2] and for the highest T20 score by an individual for the Indian team.\\n\\nHe made his List-A debut against Vidharbha[3] in 2017 and first-class debut for Punjab against Bengal in the 2017–18 Ranji Trophy, in late 2017, with a half-century in the game,[4] and 129 runs in the LAST match against Services.[5] He made his international debut for the Indian cricket team in January 2019.[6]\\n\\nHe was drafted into India\\'s Under-19 side as the vice-captain for the 2018 Under-19 Cricket World Cup. Shubman scored 372 runs at an average of 124.00 at the tournament, where he batted at number three to play a crucial role in India\\'s record fourth world title and was adjudged the edition\\'s Player of the Tournament.[7] His match-winning 102 not out in the semi-final against arch-rivals Pakistan U-19 drew praises from batting greats such as Rahul Dravid, Sachin Tendulkar, VVS Laxman, and Sourav Ganguly.[8][9]\\n\\nIn 2022, Gill was part of the IPL championship winning Gujarat Titans team. Gill would go on to win the 2023 IPL Orange Cap, scoring 890 runs, the second highest total in IPL history, including three centuries. Gill scored 129 in the IPL Qualifies 2, the highest individual score in IPL Playoffs history. In 2023, Gill made his acting debut as the voice of Pavitr \"Pav\" Prabhakar / Spider-Man India in the Hindi and Punjabi dubs of the Sony Pictures Animation film Spider-Man: Across the Spider-Verse.\\n',\n",
       " '\\nVirat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] i; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays for Royal Challengers Bangalore in the IPL and Delhi in domestic cricket. Considered to be one of the best cricketers in the world, he is widely regarded as one of the greatest batsmen in the history of the sport.[4] Nicknamed \"The King\", due to his dominant style of play and popularity, Kohli holds numerous records in his career across all formats. In 2020, the International Cricket Council named him the male cricketer of the decade. Kohli has also contributed to India\\'s successes, captaining the team from 2014 to 2022, and winning the 2011 World Cup and the 2013 Champions trophy. He is among the only four Indian cricketers who have played over 500 matches for India.[5]\\n\\nBorn and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team. He made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011. In 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time. During 2014 T20 World Cup, he set a record for the most runs scored in the tournament. In 2018, he achieved yet another milestone, becoming the world\\'s top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game. His form continued in 2019, when he became the first player to score 20,000 international runs in a single decade. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\\n\\nHe has received many accolades for his performances on the cricket field. He was recognized as the ICC ODI Player of the Year in 2012 and has won the Sir Garfield Sobers Trophy, given to the ICC Cricketer of the Year, on two occasions, in 2017 and 2018 respectively. Subsequently, Kohli also won ICC Test Player of the Year and ICC ODI Player of the Year awards in 2018, becoming the first player to win both awards in the same year. Additionally, he was named the Wisden Leading Cricketer in the World for three consecutive years, from 2016 to 2018. At the national level, Kohli was honoured with the Arjuna Award in 2013, the Padma Shri under the sports category in 2017 and the Khel Ratna award, India\\'s highest sporting honour, in 2018.\\n\\nIn 2016, he was ranked as one of the world\\'s most famous athletes by ESPN, and one of the most valuable athlete brands by Forbes. In 2018, Time magazine included him on its list of the 100 most influential people in the world. In 2020, he was ranked 66th in Forbes list of the top 100 highest-paid athletes in the world for the year 2020 with estimated earnings of over $26 million. Kohli has been deemed one of the most commercially viable cricketers, with estimated earnings of ₹165 crore (US$21 million) in the year 2022.\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision\\n\\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\\n\\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1\\n\\nAbstract\\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw\\ntext about images is a promising alternative which\\nleverages a much broader source of supervision.\\nWe demonstrate that the simple pre-training task\\nof predicting which caption goes with which im-\\nage is an efficient and scalable way to learn SOTA\\nimage representations from scratch on a dataset\\nof 400 million (image, text) pairs collected from\\nthe internet. After pre-training, natural language\\nis used to reference learned visual concepts (or\\ndescribe new ones) enabling zero-shot transfer\\nof the model to downstream tasks. We study\\nthe performance of this approach by benchmark-\\ning on over 30 different existing computer vi-\\nsion datasets, spanning tasks such as OCR, ac-\\ntion recognition in videos, geo-localization, and\\nmany types of fine-grained object classification.\\nThe model transfers non-trivially to most tasks\\nand is often competitive with a fully supervised\\nbaseline without the need for any dataset spe-\\ncific training. For instance, we match the ac-\\ncuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28\\nmillion training examples it was trained on. We\\nrelease our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.\\n\\n1. Introduction and Motivating Work\\nPre-training methods which learn directly from raw text\\nhave revolutionized NLP over the last few years (Dai &\\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\\n\\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\\nCorrespondence to: <{alec, jongwook}@openai.com>.\\n\\nTask-agnostic objectives such as autoregressive and masked\\nlanguage modeling have scaled across many orders of mag-\\nnitude in compute, model capacity, and data, steadily im-\\nproving capabilities. The development of “text-to-text” as\\na standardized input-output interface (McCann et al., 2018;\\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\\nagnostic architectures to zero-shot transfer to downstream\\ndatasets removing the need for specialized output heads or\\ndataset specific customization. Flagship systems like GPT-3\\n(Brown et al., 2020) are now competitive across many tasks\\nwith bespoke models while requiring little to no dataset\\nspecific training data.\\n\\nThese results suggest that the aggregate supervision acces-\\nsible to modern pre-training methods within web-scale col-\\nlections of text surpasses that of high-quality crowd-labeled\\nNLP datasets. However, in other fields such as computer\\nvision it is still standard practice to pre-train models on\\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\\nCould scalable pre-training methods which learn directly\\nfrom web text result in a similar breakthrough in computer\\nvision? Prior work is encouraging.\\n\\nOver 20 years ago Mori et al. (1999) explored improving\\ncontent based image retrieval by training a model to pre-\\ndict the nouns and adjectives in text documents paired with\\nimages. Quattoni et al. (2007) demonstrated it was possi-\\nble to learn more data efficient image representations via\\nmanifold learning in the weight space of classifiers trained\\nto predict words in captions associated with images. Sri-\\nvastava & Salakhutdinov (2012) explored deep represen-\\ntation learning by training multimodal Deep Boltzmann\\nMachines on top of low-level image and text tag features.\\nJoulin et al. (2016) modernized this line of work and demon-\\nstrated that CNNs trained to predict words in image cap-\\ntions learn useful image representations. They converted\\nthe title, description, and hashtag metadata of images in the\\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\\nwords multi-label classification task and showed that pre-\\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\\nlabels learned representations which preformed similarly\\nto ImageNet-based pre-training on transfer tasks. Li et al.\\n(2017) then extended this approach to predicting phrase n-\\ngrams in addition to individual words and demonstrated the\\nability of their system to zero-shot transfer to other image\\n\\nar\\nX\\n\\niv\\n:2\\n\\n10\\n3.\\n\\n00\\n02\\n\\n0v\\n1 \\n\\n [\\ncs\\n\\n.C\\nV\\n\\n] \\n 2\\n\\n6 \\nFe\\n\\nb \\n20\\n\\n21\\n\\nhttps://github.com/OpenAI/CLIP\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 2\\n\\nI1·T2 I1·T3 …\\n\\nI2·T1 I2·T3 …\\n\\nI3·T1 I3·T2 …\\n\\n⋮ ⋮ ⋮\\n\\nI1·T1\\n\\nI2·T2\\n\\nI3·T3\\n\\n(1) Contrastive pre-training\\n\\nImage\\nEncoder\\n\\nText\\nEncoderPepper\\tthe\\n\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nT1 T2 T3 …\\n\\nI1\\n\\nI2\\n\\nI3\\n\\n⋮\\n\\n(2) Create dataset classifier from label text\\n\\nplane\\n\\ncar\\n\\ndog\\n\\n⋮\\n\\nbird\\n\\nA\\tphoto\\tof\\na\\t{object}.\\n\\n⋮\\n\\nText\\nEncoder\\n\\nT1 T2 T3 TN\\n\\n…\\n\\n(3) Use for zero-shot prediction\\n\\nImage\\nEncoder\\n\\nI1 I1·T2 I1·TNI1·T1\\n\\n…\\n\\n…\\n\\nA\\tphoto\\tof\\n\\ta\\tdog.\\n\\nTN\\n\\nIN·T1 IN·T2 IN·T3\\n\\nI1·TN\\n\\nI2·TN\\n\\nI3·TN\\n\\n⋮\\n\\n…IN\\n\\n…\\n\\n⋮ ⋱\\n\\nIN·TN\\n\\nI1·T3\\n\\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\\ntarget dataset’s classes.\\n\\nclassification datasets by scoring target classes based on\\ntheir dictionary of learned visual n-grams and predicting the\\none with the highest score. Adopting more recent architec-\\ntures and pre-training approaches, VirTex (Desai & Johnson,\\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\\nVIRT (Zhang et al., 2020) have recently demonstrated the\\npotential of transformer-based language modeling, masked\\nlanguage modeling, and contrastive objectives to learn im-\\nage representations from text.\\n\\nWhile exciting as proofs of concept, using natural language\\nsupervision for image representation learning is still rare.\\nThis is likely because demonstrated performance on com-\\nmon benchmarks is much lower than alternative approaches.\\nFor example, Li et al. (2017) reach only 11.5% accuracy\\non ImageNet in a zero-shot setting. This is well below the\\n88.4% accuracy of the current state of the art (Xie et al.,\\n2020). It is even below the 50% accuracy of classic com-\\nputer vision approaches (Deng et al., 2012). Instead, more\\nnarrowly scoped but well-targeted uses of weak supervision\\nhave improved performance. Mahajan et al. (2018) showed\\nthat predicting ImageNet-related hashtags on Instagram im-\\nages is an effective pre-training task. When fine-tuned to\\nImageNet these pre-trained models increased accuracy by\\nover 5% and improved the overall state of the art at the time.\\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\\nalso demonstrated large gains on a broader set of transfer\\nbenchmarks by pre-training models to predict the classes of\\nthe noisily labeled JFT-300M dataset.\\n\\nThis line of work represents the current pragmatic middle\\nground between learning from a limited amount of super-\\nvised “gold-labels” and learning from practically unlimited\\namounts of raw text. However, it is not without compro-\\n\\nmises. Both works carefully design, and in the process limit,\\ntheir supervision to 1000 and 18291 classes respectively.\\nNatural language is able to express, and therefore supervise,\\na much wider set of visual concepts through its general-\\nity. Both approaches also use static softmax classifiers to\\nperform prediction and lack a mechanism for dynamic out-\\nputs. This severely curtails their flexibility and limits their\\n“zero-shot” capabilities.\\n\\nA crucial difference between these weakly supervised mod-\\nels and recent explorations of learning image representations\\ndirectly from natural language is scale. While Mahajan et al.\\n(2018) and Kolesnikov et al. (2019) trained their models for\\naccelerator years on millions to billions of images, VirTex,\\nICMLM, and ConVIRT trained for accelerator days on one\\nto two hundred thousand images. In this work, we close\\nthis gap and study the behaviors of image classifiers trained\\nwith natural language supervision at large scale. Enabled\\nby the large amounts of publicly available data of this form\\non the internet, we create a new dataset of 400 million (im-\\nage, text) pairs and demonstrate that a simplified version of\\nConVIRT trained from scratch, which we call CLIP, for Con-\\ntrastive Language-Image Pre-training, is an efficient method\\nof learning from natural language supervision. We study\\nthe scalability of CLIP by training a series of eight models\\nspanning almost 2 orders of magnitude of compute and ob-\\nserve that transfer performance is a smoothly predictable\\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\\n2020). We find that CLIP, similar to the GPT family, learns\\nto perform a wide set of tasks during pre-training including\\nOCR, geo-localization, action recognition, and many others.\\nWe measure this by benchmarking the zero-shot transfer\\nperformance of CLIP on over 30 existing datasets and find\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 3\\n\\n2M 33M 67M 134M 268M 400M\\n# of images processed\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt I\\nm\\n\\nag\\neN\\n\\net\\n A\\n\\ncc\\nur\\n\\nac\\ny\\n\\n3X efficiency4X efficiency\\n\\nBag of Words Contrastive (CLIP)\\nBag of Words Prediction\\nTransformer Language Model\\n\\nFigure 2. CLIP is much more efficient at zero-shot transfer\\nthan our image caption baseline. Although highly expressive,\\nwe found that transformer-based language models are relatively\\nweak at zero-shot ImageNet classification. Here, we see that it\\nlearns 3x slower than a baseline which predicts a bag-of-words\\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\\nprediction objective for the contrastive objective of CLIP further\\nimproves efficiency another 4x.\\n\\nit can be competitive with prior task-specific supervised\\nmodels. We also confirm these findings with linear-probe\\nrepresentation learning analysis and show that CLIP out-\\nperforms the best publicly available ImageNet model while\\nalso being more computationally efficient. We additionally\\nfind that zero-shot CLIP models are much more robust than\\nequivalent accuracy supervised ImageNet models which\\nsuggests that zero-shot evaluation of task-agnostic models is\\nmuch more representative of a model’s capability. These re-\\nsults have significant policy and ethical implications, which\\nwe consider in Section 7.\\n\\n2. Approach\\n2.1. Natural Language Supervision\\n\\nAt the core of our approach is the idea of learning percep-\\ntion from supervision contained in natural language. As\\ndiscussed in the introduction, this is not at all a new idea,\\nhowever terminology used to describe work in this space\\nis varied, even seemingly contradictory, and stated motiva-\\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\\nduce methods which learn visual representations from text\\npaired with images but describe their approaches as unsuper-\\nvised, self-supervised, weakly supervised, and supervised\\nrespectively.\\n\\nWe emphasize that what is common across this line of work\\nis not any of the details of the particular methods used but\\nthe appreciation of natural language as a training signal. All\\nthese approaches are learning from natural language super-\\n\\nvision. Although early work wrestled with the complexity\\nof natural language when using topic model and n-gram\\nrepresentations, improvements in deep contextual represen-\\ntation learning suggest we now have the tools to effectively\\nleverage this abundant source of supervision (McCann et al.,\\n2017).\\n\\nLearning from natural language has several potential\\nstrengths over other training methods. It’s much easier\\nto scale natural language supervision compared to standard\\ncrowd-sourced labeling for image classification since it does\\nnot require annotations to be in a classic “machine learning\\ncompatible format” such as the canonical 1-of-N majority\\nvote “gold label”. Instead, methods which work on natural\\nlanguage can learn passively from the supervision contained\\nin the vast amount of text on the internet. Learning from\\nnatural language also has an important advantage over most\\nunsupervised or self-supervised learning approaches in that\\nit doesn’t “just” learn a representation but also connects that\\nrepresentation to language which enables flexible zero-shot\\ntransfer. In the following subsections, we detail the specific\\napproach we settled on.\\n\\n2.2. Creating a Sufficiently Large Dataset\\n\\nExisting work has mainly used three datasets, MS-COCO\\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\\nYFCC100M (Thomee et al., 2016). While MS-COCO and\\nVisual Genome are high quality crowd-labeled datasets, they\\nare small by modern standards with approximately 100,000\\ntraining photos each. By comparison, other computer vision\\nsystems are trained on up to 3.5 billion Instagram photos\\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\\nis a possible alternative, but the metadata for each image is\\nsparse and of varying quality. Many images use automati-\\ncally generated filenames like 20160716 113957.JPG\\nas “titles” or contain “descriptions” of camera exposure\\nsettings. After filtering to keep only images with natural\\nlanguage titles and/or descriptions in English, the dataset\\nshrunk by a factor of 6 to only 15 million photos. This is\\napproximately the same size as ImageNet.\\n\\nA major motivation for natural language supervision is the\\nlarge quantities of data of this form available publicly on the\\ninternet. Since existing datasets do not adequately reflect\\nthis possibility, considering results only on them would un-\\nderestimate the potential of this line of research. To address\\nthis, we constructed a new dataset of 400 million (image,\\ntext) pairs collected form a variety of publicly available\\nsources on the Internet. To attempt to cover as broad a set\\nof visual concepts as possible, we search for (image, text)\\npairs as part of the construction process whose text includes\\none of a set of 500,000 queries.1 We approximately class\\n\\n1The base query list is all words occurring at least 100 times in\\nthe English version of Wikipedia. This is augmented with bi-grams\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 4\\n\\nbalance the results by including up to 20,000 (image, text)\\npairs per query. The resulting dataset has a similar total\\nword count as the WebText dataset used to train GPT-2. We\\nrefer to this dataset as WIT for WebImageText.\\n\\n2.3. Selecting an Efficient Pre-Training Method\\n\\nState-of-the-art computer vision systems use very large\\namounts of compute. Mahajan et al. (2018) required 19\\nGPU years to train their ResNeXt101-32x48d and Xie et al.\\n(2020) required 33 TPUv3 core-years to train their Noisy\\nStudent EfficientNet-L2. When considering that both these\\nsystems were trained to predict only 1000 ImageNet classes,\\nthe task of learning an open set of visual concepts from\\nnatural language seems daunting. In the course of our ef-\\nforts, we found training efficiency was key to successfully\\nscaling natural language supervision and we selected our\\nfinal pre-training method based on this metric.\\n\\nOur initial approach, similar to VirTex, jointly trained an\\nimage CNN and text transformer from scratch to predict the\\ncaption of an image. However, we encountered difficulties\\nefficiently scaling this method. In Figure 2 we show that a\\n63 million parameter transformer language model, which\\nalready uses twice the compute of its ResNet-50 image\\nencoder, learns to recognize ImageNet classes three times\\nslower than a much simpler baseline that predicts a bag-of-\\nwords encoding of the same text.\\n\\nBoth these approaches share a key similarity. They try to pre-\\ndict the exact words of the text accompanying each image.\\nThis is a difficult task due to the wide variety of descriptions,\\ncomments, and related text that co-occur with images. Re-\\ncent work in contrastive representation learning for images\\nhas found that contrastive objectives can learn better repre-\\nsentations than their equivalent predictive objective (Tian\\net al., 2019). Other work has found that although generative\\nmodels of images can learn high quality image representa-\\ntions, they require over an order of magnitude more compute\\nthan contrastive models with the same performance (Chen\\net al., 2020a). Noting these findings, we explored training\\na system to solve the potentially easier proxy task of pre-\\ndicting only which text as a whole is paired with which\\nimage and not the exact words of that text. Starting with\\nthe same bag-of-words encoding baseline, we swapped the\\npredictive objective for a contrastive objective in Figure 2\\nand observed a further 4x efficiency improvement in the rate\\nof zero-shot transfer to ImageNet.\\n\\nGiven a batch of N (image, text) pairs, CLIP is trained to\\npredict which of the N ×N possible (image, text) pairings\\nacross a batch actually occurred. To do this, CLIP learns a\\n\\nwith high pointwise mutual information as well as the names of\\nall Wikipedia articles above a certain search volume. Finally all\\nWordNet synsets not already in the query list are added.\\n\\nmulti-modal embedding space by jointly training an image\\nencoder and text encoder to maximize the cosine similar-\\nity of the image and text embeddings of the N real pairs\\nin the batch while minimizing the cosine similarity of the\\nembeddings of the N2 − N incorrect pairings. We opti-\\nmize a symmetric cross entropy loss over these similarity\\nscores. In Figure 3 we include pseudocode of the core of an\\nimplementation of CLIP. To our knowledge this batch con-\\nstruction technique and objective was first introduced in the\\narea of deep metric learning as the multi-class N-pair loss\\nSohn (2016), was popularized for contrastive representation\\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\\nrecently adapted for contrastive (text, image) representation\\nlearning in the domain of medical imaging by Zhang et al.\\n(2020).\\n\\nDue to the large size of our pre-training dataset, over-fitting\\nis not a major concern and the details of training CLIP are\\nsimplified compared to the implementation of Zhang et al.\\n(2020). We train CLIP from scratch without initializing the\\nimage encoder with ImageNet weights or the text encoder\\nwith pre-trained weights. We do not use the non-linear\\nprojection between the representation and the contrastive\\nembedding space, a change which was introduced by Bach-\\nman et al. (2019) and popularized by Chen et al. (2020b).\\nWe instead use only a linear projection to map from each en-\\ncoder’s representation to the multi-modal embedding space.\\nWe did not notice a difference in training efficiency between\\nthe two versions and speculate that non-linear projections\\nmay be co-adapted with details of current image only in\\nself-supervised representation learning methods. We also\\nremove the text transformation function tu from Zhang et al.\\n(2020) which samples a single sentence at uniform from\\nthe text since many of the (image, text) pairs in CLIP’s pre-\\ntraining dataset are only a single sentence. We also simplify\\nthe image transformation function tv. A random square\\ncrop from resized images is the only data augmentation\\nused during training. Finally, the temperature parameter\\nwhich controls the range of the logits in the softmax, τ , is\\ndirectly optimized during training as a log-parameterized\\nmultiplicative scalar to avoid turning as a hyper-parameter.\\n\\n2.4. Choosing and Scaling a Model\\n\\nWe consider two different architectures for the image en-\\ncoder. For the first, we use ResNet-50 (He et al., 2016a)\\nas the base architecture for the image encoder due to its\\nwidespread adoption and proven performance. We make sev-\\neral modifications to the original version using the ResNet-\\nD improvements from He et al. (2019) and the antialiased\\nrect-2 blur pooling from Zhang (2019). We also replace\\nthe global average pooling layer with an attention pooling\\nmechanism. The attention pooling is implemented as a sin-\\ngle layer of “transformer-style” multi-head QKV attention\\nwhere the query is conditioned on the global average-pooled\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 5\\n\\n# image_encoder - ResNet or Vision Transformer\\n# text_encoder  - CBOW or Text Transformer\\n# I[n, h, w, c] - minibatch of aligned images\\n# T[n, l]       - minibatch of aligned texts\\n# W_i[d_i, d_e] - learned proj of image to embed\\n# W_t[d_t, d_e] - learned proj of text to embed\\n# t             - learned temperature parameter\\n\\n# extract feature representations of each modality\\nI_f = image_encoder(I) #[n, d_i]\\nT_f = text_encoder(T)  #[n, d_t]\\n\\n# joint multimodal embedding [n, d_e]\\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\\n\\n# scaled pairwise cosine similarities [n, n]\\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\\n\\n# symmetric loss function\\nlabels = np.arange(n)\\nloss_i = cross_entropy_loss(logits, labels, axis=0)\\nloss_t = cross_entropy_loss(logits, labels, axis=1)\\nloss   = (loss_i + loss_t)/2\\n\\nFigure 3. Numpy-like pseudocode for the core of an implementa-\\ntion of CLIP.\\n\\nrepresentation of the image. For the second architecture, we\\nexperiment with the recently introduced Vision Transformer\\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\\nimplementation with only the minor modification of adding\\nan additional layer normalization to the combined patch\\nand position embeddings before the transformer and use a\\nslightly different initialization scheme.\\n\\nThe text encoder is a Transformer (Vaswani et al., 2017)\\nwith the architecture modifications described in Radford\\net al. (2019). As a base size we use a 63M-parameter 12-\\nlayer 512-wide model with 8 attention heads. The trans-\\nformer operates on a lower-cased byte pair encoding (BPE)\\nrepresentation of the text with a 49,152 vocab size (Sen-\\nnrich et al., 2015). For computational efficiency, the max\\nsequence length was capped at 76. The text sequence is\\nbracketed with [SOS] and [EOS] tokens and the activa-\\ntions of the highest layer of the transformer at the [EOS]\\ntoken are treated as the feature representation of the text\\nwhich is layer normalized and then linearly projected into\\nthe multi-modal embedding space. Masked self-attention\\nwas used in the text encoder to preserve the ability to ini-\\ntialize with a pre-trained language model or add language\\nmodeling as an auxiliary objective, though exploration of\\nthis is left as future work.\\n\\nWhile previous computer vision research has often scaled\\nmodels by increasing the width (Mahajan et al., 2018) or\\ndepth (He et al., 2016a) in isolation, for the ResNet image\\nencoders we adapt the approach of Tan & Le (2019) which\\nfound that allocating additional compute across all of width,\\ndepth, and resolution outperforms only allocating it to only\\n\\none dimension of the model. While Tan & Le (2019) tune\\nthe ratio of compute allocated to each dimension for their\\nEfficientNet architecture, we use a simple baseline of allo-\\ncating additional compute equally to increasing the width,\\ndepth, and resolution of the model. For the text encoder, we\\nonly scale the width of the model to be proportional to the\\ncalculated increase in width of the ResNet and do not scale\\nthe depth at all, as we found CLIP’s performance to be less\\nsensitive to the capacity of the text encoder.\\n\\n2.5. Training\\n\\nWe train a series of 5 ResNets and 3 Vision Transformers.\\nFor the ResNets we train a ResNet-50, a ResNet-101, and\\nthen 3 more which follow EfficientNet-style model scaling\\nand use approximately 4x, 16x, and 64x the compute of a\\nResNet-50. They are denoted as RN50x4, RN50x16, and\\nRN50x64 respectively. For the Vision Transformers we\\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\\nmodels for 32 epochs. We use the Adam optimizer (Kingma\\n& Ba, 2014) with decoupled weight decay regularization\\n(Loshchilov & Hutter, 2017) applied to all weights that are\\nnot gains or biases, and decay the learning rate using a\\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\\nparameters were set using a combination of grid searches,\\nrandom search, and manual tuning on the baseline ResNet-\\n50 model when trained for 1 epoch. Hyper-parameters were\\nthen adapted heuristically for larger models due to compu-\\ntational constraints. The learnable temperature parameter\\nτ was initialized to the equivalent of 0.07 from (Wu et al.,\\n2018) and clipped to prevent scaling the logits by more\\nthan 100 which we found necessary to prevent training in-\\nstability. We use a very large minibatch size of 32,768.\\nMixed-precision (Micikevicius et al., 2017) was used to ac-\\ncelerate training and save memory. To save additional mem-\\nory, gradient checkpointing (Griewank & Walther, 2000;\\nChen et al., 2016), half-precision Adam statistics (Dhariwal\\net al., 2020), and half-precision stochastically rounded text\\nencoder weights were used. The calculation of embedding\\nsimilarities was also sharded with individual GPUs comput-\\ning only the subset of the pairwise similarities necessary for\\ntheir local batch of embeddings. The largest ResNet model,\\nRN50x64, took 18 days to train on 592 V100 GPUs while\\nthe largest Vision Transformer took 12 days on 256 V100\\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\\npixel resolution for one additional epoch to boost perfor-\\nmance similar to FixRes (Touvron et al., 2019). We denote\\nthis model as ViT-L/14@336px. Unless otherwise specified,\\nall results reported in this paper as “CLIP” use this model\\nwhich we found to perform best.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 6\\n\\n3. Experiments\\n3.1. Zero-Shot Transfer\\n\\n3.1.1. MOTIVATION\\n\\nIn computer vision, zero-shot learning usually refers to the\\nstudy of generalizing to unseen object categories in image\\nclassification (Lampert et al., 2009). We instead use the\\nterm in a broader sense and study generalization to unseen\\ndatasets. We motivate this as a proxy for performing un-\\nseen tasks, as aspired to in the zero-data learning paper of\\nLarochelle et al. (2008). While much research in the field of\\nunsupervised learning focuses on the representation learn-\\ning capabilities of machine learning systems, we motivate\\nstudying zero-shot transfer as a way of measuring the task-\\nlearning capabilities of machine learning systems. In this\\nview, a dataset evaluates performance on a task on a spe-\\ncific distribution. However, many popular computer vision\\ndatasets were created by the research community primarily\\nas benchmarks to guide the development of generic image\\nclassification methods rather than measuring performance\\non a specific task. While it is reasonable to say that the\\nSVHN dataset measures the task of street number transcrip-\\ntion on the distribution of Google Street View photos, it is\\nunclear what “real” task the CIFAR-10 dataset measures.\\nIt is clear, however, what distribution CIFAR-10 is drawn\\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\\nrobustness to distribution shift and domain generalization\\nrather than task generalization. Please see Section 3.3 for\\nanalysis focused on this.\\n\\nTo our knowledge, Visual N-Grams (Li et al., 2017) first\\nstudied zero-shot transfer to existing image classification\\ndatasets in the manner described above. It is also the only\\nother work we are aware of that has studied zero-shot trans-\\nfer to standard image classification datasets using a gener-\\nically pre-trained model and serves as the best reference\\npoint for contextualizing CLIP. Their approach learns the\\nparameters of a dictionary of 142,806 visual n-grams (span-\\nning 1- to 5- grams) and optimizes these n-grams using a\\ndifferential version of Jelinek-Mercer smoothing to maxi-\\nmize the probability of all text n-grams for a given image.\\nIn order to perform zero-shot transfer, they first convert the\\ntext of each of the dataset’s class names into its n-gram\\nrepresentation and then compute its probability according\\nto their model, predicting the one with the highest score.\\n\\nOur focus on studying zero-shot transfer as an evaluation of\\ntask learning is inspired by work demonstrating task learn-\\ning in the field of NLP. To our knowledge Liu et al. (2018)\\nfirst identified task learning as an “unexpected side-effect”\\nwhen a language model trained to generate Wikipedia ar-\\nticles learned to reliably transliterate names between lan-\\nguages. While GPT-1 (Radford et al., 2018) focused on pre-\\n\\ntraining as a transfer learning method to improve supervised\\nfine-tuning, it also included an ablation study demonstrat-\\ning that the performance of four heuristic zero-shot transfer\\nmethods improved steadily over the course of pre-training,\\nwithout any supervised adaption. This analysis served as the\\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\\nsively on studying the task-learning capabilities of language\\nmodels via zero-shot transfer.\\n\\n3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER\\n\\nCLIP is pre-trained to predict if an image and a text snippet\\nare paired together in its dataset. To perform zero-shot clas-\\nsification, we reuse this capability. For each dataset, we use\\nthe names of all the classes in the dataset as the set of poten-\\ntial text pairings and predict the most probable (image, text)\\npair according to CLIP. In a bit more detail, we first compute\\nthe feature embedding of the image and the feature embed-\\nding of the set of possible texts by their respective encoders.\\nThe cosine similarity of these embeddings is then calculated,\\nscaled by a temperature parameter τ , and normalized into a\\nprobability distribution via a softmax. Note that this predic-\\ntion layer is a multinomial logistic regression classifier with\\nL2-normalized inputs, L2-normalized weights, no bias, and\\ntemperature scaling. When interpreted this way, the image\\nencoder is the computer vision backbone which computes a\\nfeature representation for the image and the text encoder is a\\nhypernetwork (Ha et al., 2016) which generates the weights\\nof a linear classifier based on the text specifying the visual\\nconcepts that the classes represent. Lei Ba et al. (2015) first\\nintroduced a zero-shot image classifier of this form while\\nthe idea of generating a classifier from natural language\\ndates back to at least Elhoseiny et al. (2013). Continuing\\nwith this interpretation, every step of CLIP pre-training can\\nbe viewed as optimizing the performance of a randomly\\ncreated proxy to a computer vision dataset which contains 1\\nexample per class and has 32,768 total classes defined via\\nnatural language descriptions. For zero-shot evaluation, we\\ncache the zero-shot classifier once it has been computed by\\nthe text encoder and reuse it for all subsequent predictions.\\nThis allows the cost of generating it to be amortized across\\nall the predictions in a dataset.\\n\\n3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS\\n\\nIn Table 1 we compare Visual N-Grams to CLIP. The best\\nCLIP model improves accuracy on ImageNet from a proof\\nof concept 11.5% to 76.2% and matches the performance\\nof the original ResNet-50 despite using none of the 1.28\\nmillion crowd-labeled training examples available for this\\ndataset. Additionally, the top-5 accuracy of CLIP models\\nare noticeably higher than their top-1, and this model has a\\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\\n2016). The ability to match the performance of a strong,\\nfully supervised baselines in a zero-shot setting suggests\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 7\\n\\naYahoo ImageNet SUN\\n\\nVisual N-Grams 72.4 11.5 23.0\\nCLIP 98.4 76.2 58.5\\n\\nTable 1. Comparing CLIP to prior zero-shot transfer image classi-\\nfication results. CLIP improves performance on all three datasets\\nby a large amount. This improvement reflects many differences\\nin the 4 years since the development of Visual N-Grams (Li et al.,\\n2017).\\n\\nCLIP is a significant step towards flexible and practical\\nzero-shot computer vision classifiers. As mentioned above,\\nthe comparison to Visual N-Grams is meant for contextu-\\nalizing the performance of CLIP and should not be inter-\\npreted as a direct methods comparison between CLIP and\\nVisual N-Grams as many performance relevant differences\\nbetween the two systems were not controlled for. For in-\\nstance, we train on a dataset that is 10x larger, use a vision\\nmodel that requires nearly 100x more compute per predic-\\ntion, likely used over 1000x their training compute, and\\nuse a transformer-based model which did not exist when\\nVisual N-Grams was published. As a closer comparison, we\\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\\nthat Visual N-Grams was trained on and found it matched\\ntheir reported ImageNet performance within a V100 GPU\\nday. This baseline was also trained from scratch instead of\\nbeing initialized from pre-trained ImageNet weights as in\\nVisual N-Grams.\\n\\nCLIP also outperforms Visual N-Grams on the other 2 re-\\nported datasets. On aYahoo, CLIP achieves a 95% reduction\\nin the number of errors, and on SUN, CLIP more than dou-\\nbles the accuracy of Visual N-Grams. To conduct a more\\ncomprehensive analysis and stress test, we implement a\\nmuch larger evaluation suite detailed in Appendix A. In\\ntotal we expand from the 3 datasets reported in Visual N-\\nGrams to include over 30 datasets and compare to over 50\\nexisting computer vision systems to contextualize results.\\n\\n3.1.4. PROMPT ENGINEERING AND ENSEMBLING\\n\\nMost standard image classification datasets treat the infor-\\nmation naming or describing classes which enables natural\\nlanguage based zero-shot transfer as an afterthought. The\\nvast majority of datasets annotate images with just a numeric\\nid of the label and contain a file mapping these ids back to\\ntheir names in English. Some datasets, such as Flowers102\\nand GTSRB, don’t appear to include this mapping at all\\nin their released versions preventing zero-shot transfer en-\\ntirely.2 For many datasets, we observed these labels may be\\n\\n2Alec learned much more about flower species and German\\ntraffic signs over the course of this project than he originally antic-\\nipated.\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\n4X efficiency gain\\n\\n5 point\\nimprovement\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64\\n\\nPrompt engineering and ensembling\\nContextless class names (Li et al. 2017)\\n\\nFigure 4. Prompt engineering and ensembling improve zero-\\nshot performance. Compared to the baseline of using contextless\\nclass names, prompt engineering and ensembling boost zero-shot\\nclassification performance by almost 5 points on average across\\n36 datasets. This improvement is similar to the gain from using\\n4 times more compute with the baseline zero-shot method but is\\n“free” when amortized over many predictions.\\n\\nchosen somewhat haphazardly and do not anticipate issues\\nrelated to zero-shot transfer which relies on task description\\nin order to transfer successfully.\\n\\nA common issue is polysemy. When the name of a class\\nis the only information provided to CLIP’s text encoder it\\nis unable to differentiate which word sense is meant due to\\nthe lack of context. In some cases multiple meanings of the\\nsame word might be included as different classes in the same\\ndataset! This happens in ImageNet which contains both\\nconstruction cranes and cranes that fly. Another example is\\nfound in classes of the Oxford-IIIT Pet dataset where the\\nword boxer is, from context, clearly referring to a breed of\\ndog, but to a text encoder lacking context could just as likely\\nrefer to a type of athlete.\\n\\nAnother issue we encountered is that it’s relatively rare in\\nour pre-training dataset for the text paired with the image\\nto be just a single word. Usually the text is a full sentence\\ndescribing the image in some way. To help bridge this\\ndistribution gap, we found that using the prompt template\\n“A photo of a {label}.” to be a good default that\\nhelps specify the text is about the content of the image. This\\noften improves performance over the baseline of using only\\nthe label text. For instance, just using this prompt improves\\naccuracy on ImageNet by 1.3%.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 8\\n\\nSimilar to the “prompt engineering” discussion around GPT-\\n3 (Brown et al., 2020; Gao et al., 2020), we have also\\nobserved that zero-shot performance can be significantly\\nimproved by customizing the prompt text to each task. A\\nfew, non exhaustive, examples follow. We found on several\\nfine-grained image classification datasets that it helped to\\nspecify the category. For example on Oxford-IIIT Pets, us-\\ning “A photo of a {label}, a type of pet.”\\nto help provide context worked well. Likewise, on Food101\\nspecifying a type of food and on FGVC Aircraft a type of\\naircraft helped too. For OCR datasets, we found that putting\\nquotes around the text or number to be recognized improved\\nperformance. Finally, we found that on satellite image classi-\\nfication datasets it helped to specify that the images were of\\nthis form and we use variants of “a satellite photo\\nof a {label}.”.\\n\\nWe also experimented with ensembling over multiple zero-\\nshot classifiers as another way of improving performance.\\nThese classifiers are computed by using different context\\nprompts such as ‘A photo of a big {label}” and\\n“A photo of a small {label}”. We construct the\\nensemble over the embedding space instead of probability\\nspace. This allows us to cache a single set of averaged text\\nembeddings so that the compute cost of the ensemble is the\\nsame as using a single classifier when amortized over many\\npredictions. We’ve observed ensembling across many gen-\\nerated zero-shot classifiers to reliably improve performance\\nand use it for the majority of datasets. On ImageNet, we\\nensemble 80 different context prompts and this improves\\nperformance by an additional 3.5% over the single default\\nprompt discussed above. When considered together, prompt\\nengineering and ensembling improve ImageNet accuracy\\nby almost 5%. In Figure 4 we visualize how prompt engi-\\nneering and ensembling change the performance of a set of\\nCLIP models compared to the contextless baseline approach\\nof directly embedding the class name as done in Li et al.\\n(2017).\\n\\n3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE\\n\\nSince task-agnostic zero-shot classifiers for computer vision\\nhave been understudied, CLIP provides a promising oppor-\\ntunity to gain a better understanding of this type of model.\\nIn this section, we conduct a study of various properties of\\nCLIP’s zero-shot classifiers. As a first question, we look\\nsimply at how well zero-shot classifiers perform. To con-\\ntextualize this, we compare to the performance of a simple\\noff-the-shelf baseline: fitting a fully supervised, regularized,\\nlogistic regression classifier on the features of the canonical\\nResNet-50. In Figure 5 we show this comparison across 27\\ndatasets. Please see Appendix A for details of datasets and\\nsetup.\\n\\nZero-shot CLIP outperforms this baseline slightly more of-\\n\\n40 30 20 10 0 10 20 30 40\\n Score (%)\\n\\nZero-Shot CLIP vs. Linear Probe on ResNet50\\n\\nEuroSAT-37.1\\nKITTI Distance-34.0\\nPatchCamelyon-19.5\\nGTSRB-18.4\\nCLEVRCounts-18.2\\nDTD-16.6\\nFlowers102-12.5\\nRESISC45-11.9\\nFGVCAircraft-11.3\\nMNIST-10.0\\nBirdsnap-3.2\\n+0.5PascalVOC2007\\n+1.1OxfordPets\\n+1.9ImageNet\\n+2.0Caltech101\\n+2.8FER2013\\n+3.0STL10\\n+3.0CIFAR100\\n+3.9CIFAR10\\n\\n+6.7HatefulMemes\\n+7.7UCF101\\n+7.8SUN397\\n\\n+12.4SST2\\n+14.5Kinetics700\\n\\n+22.5Food101\\n+23.2Country211\\n\\n+28.9StanfordCars\\n\\nFigure 5. Zero-shot CLIP is competitive with a fully super-\\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\\nclassifier outperforms a fully supervised linear classifier fitted on\\nResNet-50 features on 16 datasets, including ImageNet.\\n\\nten than not and wins on 16 of the 27 datasets. Looking at\\nindividual datasets reveals some interesting behavior. On\\nfine-grained classification tasks, we observe a wide spread\\nin performance. On two of these datasets, Stanford Cars and\\nFood101, zero-shot CLIP outperforms logistic regression\\non ResNet-50 features by over 20% while on two others,\\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\\nforms by over 10%. On OxfordPets and Birdsnap, per-\\nformance is much closer. We suspect these difference are\\nprimarily due to varying amounts of per-task supervision\\nbetween WIT and ImageNet. On “general” object classifica-\\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\\nPascalVOC2007 performance is relatively similar with a\\nslight advantage for zero-shot CLIP in all cases. On STL10,\\nCLIP achieves 99.3% overall which appears to be a new\\nstate of the art despite not using any training examples. Zero-\\nshot CLIP significantly outperforms a ResNet-50 on two\\ndatasets measuring action recognition in videos. On Kinet-\\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\\non UCF101. We speculate this is due to natural language\\nproviding wider supervision for visual concepts involving\\nverbs, compared to the noun-centric object supervision in\\nImageNet.\\n\\nLooking at where zero-shot CLIP notably underperforms,\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 9\\n\\n0 1 2 4 8 16\\n# of labeled training examples per class\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\nAv\\n\\ner\\nag\\n\\ne \\nSc\\n\\nor\\ne \\n\\n(%\\n)\\n\\nZero-Shot\\nCLIP BiT-M (ImageNet-21K)\\n\\nLinear Probe CLIP\\n\\nSimCLRv2\\n\\nResNet50\\n\\nFigure 6. Zero-shot CLIP outperforms few-shot linear probes.\\nZero-shot CLIP matches the average performance of a 4-shot linear\\nclassifier trained on the same feature space and nearly matches the\\nbest results of a 16-shot linear classifier across publicly available\\nmodels. For both BiT-M and SimCLRv2, the best performing\\nmodel is highlighted. Light gray lines are other models in the eval\\nsuite. The 20 datasets with at least 16 examples per class were\\nused in this analysis.\\n\\nwe see that zero-shot CLIP is quite weak on several spe-\\ncialized, complex, or abstract tasks such as satellite image\\nclassification (EuroSAT and RESISC45), lymph node tumor\\ndetection (PatchCamelyon), counting objects in synthetic\\nscenes (CLEVRCounts), self-driving related tasks such as\\nGerman traffic sign recognition (GTSRB), recognizing dis-\\ntance to the nearest car (KITTI Distance). These results\\nhighlight the poor capability of zero-shot CLIP on more\\ncomplex tasks. By contrast, non-expert humans can robustly\\nperform several of these tasks, such as counting, satellite\\nimage classification, and traffic sign recognition, suggesting\\nsignificant room for improvement. However, we caution\\nthat it is unclear whether measuring zero-shot transfer, as\\nopposed to few-shot transfer, is a meaningful evaluation for\\ndifficult tasks that a learner has no prior experience with,\\nsuch as lymph node tumor classification for almost all hu-\\nmans (and possibly CLIP).\\n\\nWhile comparing zero-shot performance to fully supervised\\nmodels contextualizes the task-learning capabilities of CLIP,\\ncomparing to few-shot methods is a more direct compari-\\nson, since zero-shot is its limit. In Figure 6, we visualize\\nhow zero-shot CLIP compares to few-shot logistic regres-\\nsion on the features of many image models including the\\nbest publicly available ImageNet models, self-supervised\\nlearning methods, and CLIP itself. While it is intuitive to\\n\\nexpect zero-shot to underperform one-shot, we instead find\\nthat zero-shot CLIP matches the performance of 4-shot lo-\\ngistic regression on the same feature space. This is likely\\ndue to an important difference between the zero-shot and\\nfew-shot approach. First, CLIP’s zero-shot classifier is gen-\\nerated via natural language which allows for visual concepts\\nto be directly specified (“communicated”). By contrast,\\n“normal” supervised learning must infer concepts indirectly\\nfrom training examples. Context-less example-based learn-\\ning has the drawback that many different hypotheses can\\nbe consistent with the data, especially in the one-shot case.\\nA single image often contains many different visual con-\\ncepts. Although a capable learner is able to exploit visual\\ncues and heuristics, such as assuming that the concept being\\ndemonstrated is the primary object in an image, there is no\\nguarantee.\\n\\nA potential resolution of this discrepancy between zero-\\nshot and few-shot performance is to use CLIP’s zero-shot\\nclassifier as a prior for the weights of the few-shot classifier.\\nWhile adding an L2 penalty towards the generated weights\\nis a straightforward implementation of this idea, we found\\nthat hyperparameter optimization would often select for\\nsuch a large value of this regularizer that the resulting few-\\nshot classifier was “just” the zero-shot classifier. Research\\ninto better methods of combining the strength of zero-shot\\ntransfer with flexibility of few-shot learning is a promising\\ndirection for future work.\\n\\nWhen comparing zero-shot CLIP to few-shot logistic re-\\ngression on the features of other models, zero-shot CLIP\\nroughly matches the performance of the best performing\\n16-shot classifier in our evaluation suite, which uses the fea-\\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\\nWe are certain that a BiT-L model trained on JFT-300M\\nwould perform even better but these models have not been\\npublicly released. That a BiT-M ResNet-152x2 performs\\nbest in a 16-shot setting is somewhat surprising since, as\\nanalyzed in Section 3.2, the Noisy Student EfficientNet-L2\\noutperforms it in a fully supervised setting by almost 5% on\\naverage across 27 datasets.\\n\\nIn addition to studying the average performance of zero-shot\\nCLIP and few-shot logistic regression, we also examine\\nperformance on individual datasets. In Figure 7, we show\\nestimates for the number of labeled examples per class that\\na logistic regression classifier on the same feature space\\nrequires to match the performance of zero-shot CLIP. Since\\nzero-shot CLIP is also a linear classifier, this estimates the\\neffective data efficiency of zero-shot transfer in this setting.\\nIn order to avoid training thousands of linear classifiers,\\nwe estimate the effective data efficiency based on a log-\\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\\nshot (when possible), and a fully supervised linear classifier\\ntrained on each dataset. We find that zero-shot transfer can\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 10\\n\\n0 25 50 75 100 125 150 175 200\\n# of labeled examples per class\\n\\nrequired to match zero-shot\\n\\nFlowers102\\nEuroSAT\\n\\nRESISC45\\nCLEVRCounts\\n\\nGTSRB\\nFGVCAircraft\\n\\nDTD\\nBirdsnap\\nUCF101\\n\\nKITTI Distance\\nCaltech101\\n\\nSUN397\\nMNIST\\n\\nStanfordCars\\nHatefulMemes\\n\\nCIFAR100\\nSTL10\\n\\nKinetics700\\nSST2\\n\\nPCam\\nImageNet\\n\\nCountry211\\nOxfordPets\\n\\nFood101\\nCIFAR10\\nFER2013\\n\\n0.9\\n0.9\\n1.5\\n1.5\\n1.6\\n2.0\\n2.6\\n2.7\\n2.9\\n2.9\\n3.5\\n3.9\\n4.8\\n6.0\\n9.8\\n12.0\\n12.7\\n13.6\\n14.4\\n14.7\\n16.0\\n\\n32\\n48\\n\\n64\\n81\\n\\n184\\n\\nMedian: 5.4\\nMean:  20.8\\n\\nFigure 7. The data efficiency of zero-shot transfer varies\\nwidely. Calculating the number of labeled examples per class\\na linear classifier on the same CLIP feature space requires to match\\nthe performance of the zero-shot classifier contextualizes the ef-\\nfectiveness of zero-shot transfer. Values are estimated based on\\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\\nresults. Performance varies widely from still underperforming a\\none-shot classifier on two datasets to matching an estimated 184\\nlabeled examples per class.\\n\\nhave widely varying efficiency per dataset from less than 1\\nlabeled example per class to 184. Two datasets, Flowers102\\nand EuroSAT underperform one-shot models. Half of the\\ndatasets require less than 5 examples per class with a median\\nof 5.4. However, the mean estimated data efficiency is 20.8\\nexamples per class. This is due to the 20% of datasets\\nwhere supervised classifiers require many labeled examples\\nper class in order to match performance. On ImageNet,\\nzero-shot CLIP matches the performance of a 16-shot linear\\nclassifier trained on the same feature space.\\n\\nIf we assume that evaluation datasets are large enough that\\nthe parameters of linear classifiers trained on them are well\\nestimated, then, because CLIP’s zero-shot classifier is also\\na linear classifier, the performance of the fully supervised\\nclassifiers roughly sets an upper bound for what zero-shot\\ntransfer can achieve. In Figure 8 we compare CLIP’s zero-\\nshot performance with fully supervised linear classifiers\\nacross datasets. The dashed, y = x line represents an “op-\\ntimal” zero-shot classifier that matches the performance of\\nits fully supervised equivalent. For most datasets, the per-\\nformance of zero-shot classifiers still underperform fully su-\\npervised classifiers by 10% to 25%, suggesting that there is\\nstill plenty of headroom for improving CLIP’s task-learning\\nand zero-shot transfer capabilities.\\n\\nThere is a positive correlation of 0.82 (p-value < 10−6)\\nbetween zero-shot performance and fully supervised perfor-\\n\\n20 30 40 50 60 70 80 90 100\\nLinear Probe CLIP Performance\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt C\\nLI\\n\\nP \\nPe\\n\\nrfo\\nrm\\n\\nan\\nce\\n\\nr = 0.82\\n\\nVOC2007\\n\\nCountry211\\n\\nHatefulMemes\\n\\nMNIST\\n\\nCIFAR10\\n\\nSST2\\n\\nDTD\\n\\nPCAM\\n\\nRESISC45\\n\\nEuroSAT\\n\\nGTSRB\\n\\nCLEVRCounts\\n\\nFER2013\\n\\nUCF101\\n\\nBirdsnap\\n\\nOxfordPets\\n\\nCIFAR100\\n\\nFGVCAircraft\\n\\nFood101\\n\\nFlowers102Stanford Cars\\n\\nCaltech101\\n\\nSUN397\\n\\nImageNet\\n\\nSTL10\\n\\nKITTI Distance\\n\\nKinetics700\\n\\nFigure 8. Zero-shot performance is correlated with linear\\nprobe performance but still mostly sub-optimal. Comparing\\nzero-shot and linear probe performance across datasets shows a\\nstrong correlation with zero-shot performance mostly shifted 10 to\\n25 points lower. On only 5 datasets does zero-shot performance\\napproach linear probe performance (≤3 point difference).\\n\\nmance, suggesting that CLIP is relatively consistent at con-\\nnecting underlying representation and task learning to zero-\\nshot transfer. However, zero-shot CLIP only approaches\\nfully supervised performance on 5 datasets: STL10, CI-\\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\\ndatasets, both zero-shot accuracy and fully supervised accu-\\nracy are over 90%. This suggests that CLIP may be more\\neffective at zero-shot transfer for tasks where its underly-\\ning representations are also high quality. The slope of a\\nlinear regression model predicting zero-shot performance\\nas a function of fully supervised performance estimates that\\nfor every 1% improvement in fully supervised performance,\\nzero-shot performance improves by 1.28%. However, the\\n95th-percentile confidence intervals still include values of\\nless than 1 (0.93-1.79).\\n\\nOver the past few years, empirical studies of deep learning\\nsystems have documented that performance is predictable as\\na function of important quantities such as training compute\\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\\nThe GPT family of models has so far demonstrated consis-\\ntent improvements in zero-shot performance across a 1000x\\nincrease in training compute. In Figure 9, we check whether\\nthe zero-shot performance of CLIP follows a similar scaling\\npattern. We plot the average error rate of the 5 ResNet CLIP\\nmodels across 39 evaluations on 36 different datasets and\\nfind that a similar log-log linear scaling trend holds for CLIP\\nacross a 44x increase in model compute. While the overall\\ntrend is smooth, we found that performance on individual\\nevaluations can be much noisier. We are unsure whether\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 11\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nEr\\nro\\n\\nr (\\n%\\n\\n)\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64\\n\\nFigure 9. Zero-shot CLIP performance scales smoothly as a\\nfunction of model compute. Across 39 evals on 36 different\\ndatasets, average zero-shot error is well modeled by a log-log lin-\\near trend across a 44x range of compute spanning 5 different CLIP\\nmodels. Lightly shaded lines are performance on individual evals,\\nshowing that performance is much more varied despite the smooth\\noverall trend.\\n\\nthis is caused by high variance between individual training\\nruns on sub-tasks (as documented in D’Amour et al. (2020))\\nmasking a steadily improving trend or whether performance\\nis actually non-monotonic as a function of compute on some\\ntasks.\\n\\n3.2. Representation Learning\\n\\nWhile we have extensively analyzed the task-learning ca-\\npabilities of CLIP through zero-shot transfer in the previ-\\nous section, it is more common to study the representation\\nlearning capabilities of a model. There exist many ways to\\nevaluate the quality of representations as well as disagree-\\nments over what properties an “ideal” representation should\\nhave (Locatello et al., 2020). Fitting a linear classifier on\\na representation extracted from the model and measuring\\nits performance on various datasets is a common approach.\\nAn alternative is measuring the performance of end-to-end\\nfine-tuning of the model. This increases flexibility, and\\nprior work has convincingly demonstrated that fine-tuning\\noutperforms linear classification on most image classifi-\\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\\nWhile the high performance of fine-tuning motivates its\\nstudy for practical reasons, we still opt for linear classifier\\nbased evaluation for several reasons. Our work is focused\\non developing a high-performing task and dataset-agnostic\\npre-training approach. Fine-tuning, because it adapts rep-\\nresentations to each dataset during the fine-tuning phase,\\ncan compensate for and potentially mask failures to learn\\ngeneral and robust representations during the pre-training\\nphase. Linear classifiers, because of their limited flexibility,\\ninstead highlight these failures and provide clear feedback\\nduring development. For CLIP, training supervised linear\\n\\nclassifiers has the added benefit of being very similar to the\\napproach used for its zero-shot classifiers which enables\\nextensive comparisons and analysis in Section 3.1. Finally,\\nwe aim to compare CLIP to a comprehensive set of existing\\nmodels across many tasks. Studying 66 different models on\\n27 different datasets requires tuning 1782 different evalua-\\ntions. Fine-tuning opens up a much larger design and hyper-\\nparameter space, which makes it difficult to fairly evaluate\\nand computationally expensive to compare a diverse set of\\ntechniques as discussed in other large scale empirical studies\\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\\nclassifiers require minimal hyper-parameter tuning and have\\nstandardized implementations and evaluation procedures.\\nPlease see Appendix A for further details on evaluation.\\n\\nFigure 10 summarizes our findings. To minimize selection\\neffects that could raise concerns of confirmation or reporting\\nbias, we first study performance on the 12 dataset evaluation\\nsuite from Kornblith et al. (2019). While small CLIP mod-\\nels such as a ResNet-50 and ResNet-101 outperform other\\nResNets trained on ImageNet-1K (BiT-S and the originals),\\nthey underperform ResNets trained on ImageNet-21K (BiT-\\nM). These small CLIP models also underperform models\\nin the EfficientNet family with similar compute require-\\nments. However, models trained with CLIP scale very well\\nand the largest model we trained (ResNet-50x64) slightly\\noutperforms the best performing existing model (a Noisy\\nStudent EfficientNet-L2) on both overall score and compute\\nefficiency. We also find that CLIP vision transformers are\\nabout 3x more compute efficient than CLIP ResNets, which\\nallows us to reach higher overall performance within our\\ncompute budget. These results qualitatively replicate the\\nfindings of Dosovitskiy et al. (2020) which reported that\\nvision transformers are more compute efficient than con-\\nvnets when trained on sufficiently large datasets. Our best\\noverall model is a ViT-L/14 that is fine-tuned at a higher res-\\nolution of 336 pixels on our dataset for 1 additional epoch.\\nThis model outperforms the best existing model across this\\nevaluation suite by an average of 2.6%.\\n\\nAs Figure 21 qualitatively shows, CLIP models learn a wider\\nset of tasks than has previously been demonstrated in a sin-\\ngle computer vision model trained end-to-end from random\\ninitialization. These tasks include geo-localization, optical\\ncharacter recognition, facial emotion recognition, and action\\nrecognition. None of these tasks are measured in the evalua-\\ntion suite of Kornblith et al. (2019). This could be argued\\nto be a form of selection bias in Kornblith et al. (2019)’s\\nstudy towards tasks that overlap with ImageNet. To address\\nthis, we also measure performance on a broader 27 dataset\\nevaluation suite. This evaluation suite, detailed in Appendix\\nA includes datasets representing the aforementioned tasks,\\nGerman Traffic Signs Recognition Benchmark (Stallkamp\\net al., 2011), as well as several other datasets adapted from\\nVTAB (Zhai et al., 2019).\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 12\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\nLinear probe average over Kornblith et al.\\'s 12 datasets\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\nLinear probe average over all 27 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\\nEfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or\\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.\\n\\nOn this broader evaluation suite, the benefits of CLIP are\\nmore clear. All CLIP models, regardless of scale, outper-\\nform all evaluated systems in terms of compute efficiency.\\nThe improvement in average score of the best model over\\nprevious systems increases from 2.6% to 5%. We also find\\nthat self-supervised systems do noticeably better on our\\nbroader evaluation suite. For instance, while SimCLRv2\\nstill underperforms BiT-M on average on the 12 datasets\\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\\non our 27 dataset evaluation suite. These findings suggest\\ncontinuing to expand task diversity and coverage in order\\nto better understand the “general” performance of systems.\\nWe suspect additional evaluation efforts along the lines of\\nVTAB to be valuable.\\n\\nIn addition to the aggregate analysis above, we visualize\\nper-dataset differences in the performance of the best CLIP\\nmodel and the best model in our evaluation suite across\\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\\nStudent EfficientNet-L2 on 21 of the 27 datasets. CLIP\\nimproves the most on tasks which require OCR (SST2\\n\\nand HatefulMemes), geo-localization and scene recognition\\n(Country211, SUN397), and activity recognition in videos\\n(Kinetics700 and UCF101). In addition CLIP also does\\nmuch better on fine-grained car and traffic sign recognition\\n(Stanford Cars and GTSRB). This may reflect a problem\\nwith overly narrow supervision in ImageNet. A result such\\nas the 14.7% improvement on GTSRB could be indicative\\nof an issue with ImageNet-1K, which has only a single la-\\nbel for all traffic and street signs. This could encourage\\na supervised representation to collapse intra-class details\\nand hurt accuracy on a fine-grained downstream task. As\\nmentioned, CLIP still underperforms the EfficientNet on\\nseveral datasets. Unsurprisingly, the dataset that the Effi-\\ncientNet does best relative to CLIP on is the one it was\\ntrained on: ImageNet. The EffcientNet also slightly outper-\\nforms CLIP on low-resolution datasets such as CIFAR10\\nand CIFAR100. We suspect this is at least partly due to the\\nlack of scale-based data augmentation in CLIP. The Effi-\\ncientNet also does slightly better on PatchCamelyon and\\nCLEVRCounts, datasets where overall performance is still\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 13\\n\\n10 5 0 5 10 15 20 25\\n Score (%)\\n\\nLogistic Regression on CLIP vs. EfficientNet L2 NS\\n\\nImageNet-3.0\\nCLEVRCounts-2.4\\nCIFAR100-1.7\\nPatchCamelyon-1.2\\nCIFAR10-0.8\\nOxfordPets-0.5\\n+0.0STL10\\n+0.5VOC2007\\n+0.5DTD\\n+0.6MNIST\\n+0.9EuroSAT\\n+1.3Caltech101\\n+1.4Flowers102\\n+1.4Birdsnap\\n+2.3KITTI Distance\\n+3.1UCF101\\n+3.2FGVCAircraft\\n+3.9Food101\\n+4.5FER2013\\n+5.1RESISC45\\n\\n+6.2Kinetics700\\n+6.5SUN397\\n\\n+14.7GTSRB\\n+15.9StanfordCars\\n\\n+18.8HatefulMemes\\n+22.7Country211\\n\\n+23.6SST2\\n\\nFigure 11. CLIP’s features outperform the features of the best\\nImageNet model on a wide variety of datasets. Fitting a linear\\nclassifier on CLIP’s features outperforms using the Noisy Student\\nEfficientNet-L2 on 21 out of 27 datasets.\\n\\nlow for both approaches.\\n\\n3.3. Robustness to Natural Distribution Shift\\n\\nIn 2015, it was announced that a deep learning model ex-\\nceeded human performance on the ImageNet test set (He\\net al., 2015). However, research in the subsequent years\\nhas repeatedly found that these models still make many sim-\\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\\nAlcorn et al., 2019), and new benchmarks testing these sys-\\ntems has often found their performance to be much lower\\nthan both their ImageNet accuracy and human accuracy\\n(Recht et al., 2019; Barbu et al., 2019). What explains this\\ndiscrepancy? Various ideas have been suggested and stud-\\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\\ntheme of proposed explanations is that deep learning models\\nare exceedingly adept at finding correlations and patterns\\nwhich hold across their training dataset and thus improve\\nin-distribution performance. However many of these corre-\\nlations and patterns are actually spurious and do not hold for\\nother distributions and result in large drops in performance\\non other datasets.\\n\\nWe caution that, to date, most of these studies limit their\\nevaluation to models trained on ImageNet. Recalling the\\ntopic of discussion, it may be a mistake to generalize too\\nfar from these initial findings. To what degree are these\\nfailures attributable to deep learning, ImageNet, or some\\n\\ncombination of the two? CLIP models, which are trained via\\nnatural language supervision on a very large dataset and are\\ncapable of high zero-shot performance, are an opportunity\\nto investigate this question from a different angle.\\n\\nTaori et al. (2020) is a recent comprehensive study mov-\\ning towards quantifying and understanding these behaviors\\nfor ImageNet models. Taori et al. (2020) study how the\\nperformance of ImageNet models change when evaluated\\non natural distribution shifts. They measure performance\\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\\nand ImageNet Rendition (Hendrycks et al., 2020a). They\\ndistinguish these datasets, which all consist of novel images\\ncollected from a variety of sources, from synthetic distri-\\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\\nsarial attacks (Goodfellow et al., 2014) which are created by\\nperturbing existing images in various ways. They propose\\nthis distinction because in part because they find that while\\nseveral techniques have been demonstrated to improve per-\\nformance on synthetic distribution shifts, they often fail to\\nyield consistent improvements on natural distributions.3\\n\\nAcross these collected datasets, the accuracy of ImageNet\\nmodels drop well below the expectation set by the Ima-\\ngeNet validation set. For the following summary discussion\\nwe report average accuracy across all 7 natural distribution\\nshift datasets and average accuracy across the correspond-\\ning class subsets of ImageNet unless otherwise specified.\\nAdditionally, for Youtube-BB and ImageNet-Vid, which\\nhave two different evaluation settings, we use the average\\nof pm-0 and pm-10 accuracy.\\n\\nA ResNet-101 makes 5 times as many mistakes when eval-\\nuated on these natural distribution shifts compared to the\\nImageNet validation set. Encouragingly however, Taori et al.\\n(2020) find that accuracy under distribution shift increases\\npredictably with ImageNet accuracy and is well modeled\\nas a linear function of logit-transformed accuracy. Taori\\net al. (2020) use this finding to propose that robustness\\nanalysis should distinguish between effective and relative\\nrobustness. Effective robustness measures improvements\\nin accuracy under distribution shift above what is predicted\\nby the documented relationship between in-distribution and\\nout-of-distribution accuracy. Relative robustness captures\\nany improvement in out-of-distribution accuracy. Taori et al.\\n(2020) argue that robustness techniques should aim to im-\\nprove both effective robustness and relative robustness.\\n\\nAlmost all models studied in Taori et al. (2020) are trained\\n3We refer readers to Hendrycks et al. (2020a) for additional\\n\\nexperiments and discussion on this claim.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 14\\n\\n65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\\nLinear probe average over Kornblith et al.\\'s 12 datasets\\n\\n65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\\n\\nLinear probe average over 26 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.\\n\\nor fine-tuned on the ImageNet dataset. Returning to the\\ndiscussion in the introduction to this section - is training\\nor adapting to the ImageNet dataset distribution the cause\\nof the observed robustness gap? Intuitively, a zero-shot\\nmodel should not be able to exploit spurious correlations\\nor patterns that hold only on a specific distribution, since it\\nis not trained on that distribution. 4 Thus it is reasonable\\nto expect zero-shot models to have much higher effective\\nrobustness. In Figure 13, we compare the performance of\\nzero-shot CLIP with existing ImageNet models on natural\\ndistribution shifts. All zero-shot CLIP models improve\\neffective robustness by a large amount and reduce the size\\nof the gap between ImageNet accuracy and accuracy under\\ndistribution shift by up to 75%.\\n\\nWhile these results show that zero-shot models can be much\\nmore robust, they do not necessarily mean that supervised\\nlearning on ImageNet causes a robustness gap. Other details\\nof CLIP, such as its large and diverse pre-training dataset\\nor use of natural language supervision could also result\\n\\n4We caution that a zero-shot model can still exploit spurious\\ncorrelations that are shared between the pre-training and evaluation\\ndistributions.\\n\\nin much more robust models regardless of whether they\\nare zero-shot or fine-tuned. As an initial experiment to\\npotentially begin narrowing this down, we also measure\\nhow the performance of CLIP models change after adapting\\nto the ImageNet distribution via a L2 regularized logistic\\nregression classifier fit to CLIP features on the ImageNet\\ntraining set. We visualize how performance changes from\\nthe zero-shot classifier in Figure 14. Although adapting\\nCLIP to the ImageNet distribution increases its ImageNet\\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\\nof the 2018 SOTA from Mahajan et al. (2018), average\\naccuracy under distribution shift slightly decreases.\\n\\nIt is surprising to see a 9.2% increase in accuracy, which cor-\\nresponds to roughly 3 years of improvement in SOTA, fail\\nto translate into any improvement in average performance\\nunder distribution shift. We also break down the differences\\nbetween zero-shot accuracy and linear classifier accuracy\\nper dataset in Figure 14 and find performance still increases\\nsignificantly on one dataset, ImageNetV2. ImageNetV2\\nclosely followed the creation process of the original Ima-\\ngeNet dataset which suggests that gains in accuracy from\\nsupervised adaptation are closely concentrated around the\\nImageNet distribution. Performance decreases by 4.7% on\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 15\\n\\n65 70 75 80 85 90 95 100\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n\\n100\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\nIdeal robust model (y = x)\\nZero-Shot CLIP\\nStandard ImageNet training\\nExisiting robustness techniques ImageNet\\n\\nImageNetV2\\n\\nImageNet-A\\n\\nImageNet-R\\n\\nObjectNet\\n\\nImageNet \\nSketch\\n\\n76.2 76.2\\n\\n64.3 70.1\\n\\n2.7 77.1\\n\\n37.7 88.9\\n\\n32.6 72.3\\n\\n25.2 60.2\\n\\nImageNet\\nResNet101\\n\\nZero-Shot\\nCLIP\\n\\n0%\\n\\n+5.8%\\n\\n+74.4%\\n\\n+51.2%\\n\\n+39.7%\\n\\n+35.0%\\n\\nΔ ScoreDataset Examples\\n\\nFigure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\\nthis “robustness gap” by up to 75%. Linear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals.\\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\\nset, ResNet-101.\\n\\nImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\\nand 1.9% on ImageNet-A. The change in accuracy on the\\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\\nsignificant.\\n\\nHow is it possible to improve accuracy by 9.2% on the Im-\\nageNet dataset with little to no increase in accuracy under\\ndistribution shift? Is the gain primarily from “exploiting\\nspurious correlations”? Is this behavior unique to some com-\\nbination of CLIP, the ImageNet datatset, and the distribution\\nshifts studied, or a more general phenomena? Does it hold\\nfor end-to-end finetuning as well as linear classifiers? We\\ndo not have confident answers to these questions at this time.\\nPrior work has also pre-trained models on distributions other\\nthan ImageNet, but it is common to study and release mod-\\nels only after they have been fine-tuned to ImageNet. As a\\nstep towards understanding whether pre-trained zero-shot\\nmodels consistently have higher effective robustness than\\nfine-tuned models, we encourage the authors of Mahajan\\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\\n(2020) to, if possible, study these questions on their models\\nas well.\\n\\nWe also investigate another robustness intervention enabled\\nby flexible zero-shot natural-language-based image classi-\\nfiers. The target classes across the 7 transfer datasets are\\nnot always perfectly aligned with those of ImageNet. Two\\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\\nclasses of ImageNet. This presents a problem when trying\\nto use the fixed 1000-way classifier of an ImageNet model\\nto make predictions. Taori et al. (2020) handle this by max-\\n\\npooling predictions across all sub-classes according to the\\nImageNet class hierarchy. Sometimes this mapping is much\\nless than perfect. For the person class in Youtube-BB, pre-\\ndictions are made by pooling over the ImageNet classes for\\na baseball player, a bridegroom, and a scuba diver. With\\nCLIP we can instead generate a custom zero-shot classi-\\nfier for each dataset directly based on its class names. In\\nFigure 14 we see that this improves average effective ro-\\nbustness by 5% but is concentrated in large improvements\\non only a few datasets. Curiously, accuracy on ObjectNet\\nalso increases by 2.3%. Although the dataset was designed\\nto closely overlap with ImageNet classes, using the names\\nprovided for each class by ObjectNet’s creators still helps a\\nsmall amount compared to using ImageNet class names and\\npooling predictions when necessary.\\n\\nWhile zero-shot CLIP improves effective robustness, Figure\\n14 shows that the benefit is almost entirely gone in a fully\\nsupervised setting. To better understand this difference, we\\ninvestigate how effective robustness changes on the contin-\\nuum from zero-shot to fully supervised. In Figure 15 we\\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\\n..., 128-shot, and fully supervised logistic regression classi-\\nfiers on the best CLIP model’s features. We see that while\\nfew-shot models also show higher effective robustness than\\nexisting models, this benefit fades as in-distribution per-\\nformance increases with more training data and is mostly,\\nthough not entirely, gone for the fully supervised model.\\nAdditionally, zero-shot CLIP is notably more robust than\\na few-shot model with equivalent ImageNet performance.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 16\\n\\n70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\nAdapt to class shift\\n\\nAdapt to ImageNet\\n\\nIdeal robust model (y = x)\\nAdaptive Zero-Shot CLIP\\nImageNet Zero-Shot CLIP\\nLogistic Regression CLIP\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\n\\n10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\nImageNet-R-4.7\\nObjectNet-3.8\\nImageNet Sketch-2.8\\nImageNet-A-1.9\\nImageNet Vid-0.5\\n+0.6Youtube-BB\\n\\n+5.8ImageNetV2\\n+9.2ImageNet\\n\\nAdapt to ImageNet\\n\\n10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\n0ImageNet\\n0ImageNetV2\\n0ImageNet-A\\n0ImageNet-R\\n0ImageNet Sketch\\n\\n+2.3ObjectNet\\n+8.3ImageNet Vid\\n\\n+26.9Youtube-BB\\nAdapt to class shift\\n\\nFigure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier\\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot\\nclassifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\\nwith ImageNet categories.\\n\\nAcross our experiments, high effective robustness seems to\\nresult from minimizing the amount of distribution specific\\ntraining data a model has access to, but this comes at a cost\\nof reducing dataset-specific performance.\\n\\nTaken together, these results suggest that the recent shift\\ntowards large-scale task and dataset agnostic pre-training\\ncombined with a reorientation towards zero-shot and few-\\nshot benchmarking on broad evaluation suites (as advocated\\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\\ndevelopment of more robust systems and provides a more\\naccurate assessment of performance. We are curious to see\\nif the same results hold for zero-shot models in the field\\nof NLP such as the GPT family. While Hendrycks et al.\\n(2020b) has reported that pre-training improves relative ro-\\nbustness on sentiment analysis, Miller et al. (2020)’s study\\nof the robustness of question answering models under nat-\\nural distribution shift finds, similar to Taori et al. (2020),\\nlittle evidence of effective robustness improvements to date.\\n\\n4. Comparison to Human Performance\\nHow does CLIP compare to human performance and human\\nlearning? To get a better understanding of how well humans\\nperform in similar evaluation settings to CLIP, we evaluated\\n\\nhumans on one of our tasks. We wanted to get a sense of\\nhow strong human zero-shot performance is at these tasks,\\nand how much human performance is improved if they are\\nshown one or two image samples. This can help us to\\ncompare task difficulty for humans and CLIP, and identify\\ncorrelations and differences between them.\\n\\nWe had five different humans look at each of 3669 images\\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\\n2012) and select which of the 37 cat or dog breeds best\\nmatched the image (or ‘I don’t know’ if they were com-\\npletely uncertain). In the zero-shot case the humans were\\ngiven no examples of the breeds and asked to label them\\nto the best of their ability without an internet search. In\\nthe one-shot experiment the humans were given one sample\\nimage of each breed and in the two-shot experiment they\\nwere given two sample images of each breed.5\\n\\nOne possible concern was that the human workers were not\\nsufficiently motivated in the zero-shot task. High human\\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)\\n\\n5There is not a perfect correspondence between the human\\nfew-shot tasks and the model’s few-shot performance since the\\nmodel cannot refer to sample images in the way that the humans\\ncan.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 17\\n\\n65 70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\n1 shot\\n\\n2 shot\\n\\n4 shot\\n\\n8 shot\\n\\n16 shot\\n\\n32\\n64\\n\\n128\\nall0 shot\\n\\nIdeal robust model (y = x)\\nFew-Shot CLIP (best model)\\nZero-Shot CLIP (best model)\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\n\\nFigure 15. Few-shot CLIP also increases effective robustness\\ncompared to existing ImageNet models but is less robust than\\nzero-shot CLIP. Minimizing the amount of ImageNet training\\ndata used for adaption increases effective robustness at the cost of\\ndecreasing relative robustness. 16-shot logistic regression CLIP\\nmatches zero-shot CLIP on ImageNet, as previously reported in\\nFigure 7, but is less robust.\\n\\nand 97-100% accuracy on the subset of attention check\\nimages increased our trust in the human workers.\\n\\nInterestingly, humans went from a performance average of\\n54% to 76% with just one training example per class, and\\nthe marginal gain from an additional training example is\\nminimal. The gain in accuracy going from zero to one shot\\nis almost entirely on images that humans were uncertain\\nabout. This suggests that humans “know what they don’t\\nknow” and are able to update their priors on the images they\\nare most uncertain in based on a single example. Given this,\\nit seems that while CLIP is a promising training strategy\\nfor zero-shot performance (Figure 5) and does well on tests\\nof natural distribution shift (Figure 13), there is a large\\ndifference between how humans learn from a few examples\\nand the few-shot methods in this paper.\\n\\nThis suggests that there are still algorithmic improvements\\nwaiting to be made to decrease the gap between machine\\nand human sample efficiency, as noted by Lake et al. (2016)\\nand others. Because these few-shot evaluations of CLIP\\ndon’t make effective use of prior knowledge and the humans\\ndo, we speculate that finding a method to properly integrate\\nprior knowledge into few-shot learning is an important step\\nin algorithmic improvements to CLIP. To our knowledge,\\nusing a linear classifier on top of the features of a high-\\n\\nAccuracy Majority Vote\\non Full Dataset\\n\\nAccuracy\\non Guesses\\n\\nMajority Vote\\nAccuracy\\n\\non Guesses\\n\\nZero-shot human 53.7 57.0 69.7 63.9\\nZero-shot CLIP 93.5 93.5 93.5 93.5\\nOne-shot human 75.7 80.3 78.5 81.2\\nTwo-shot human 75.7 85.0 79.2 86.1\\n\\nTable 2. Comparison of human performance on Oxford IIT Pets.\\nAs in Parkhi et al. (2012), the metric is average per-class classifica-\\ntion accuracy. Most of the gain in performance when going from\\nthe human zero shot case to the human one shot case is on images\\nthat participants were highly uncertain on. “Guesses” refers to\\nrestricting the dataset to where participants selected an answer\\nother than “I don’t know”, the “majority vote” is taking the most\\nfrequent (exclusive of ties) answer per image.\\n\\nquality pre-trained model is near state-of-the-art for few\\nshot learning (Tian et al., 2020), which suggests that there is\\na gap between the best few-shot machine learning methods\\nand human few-shot learning.\\n\\nIf we plot human accuracy vs CLIP’s zero shot accuracy\\n(Figure 16), we see that the hardest problems for CLIP are\\nalso hard for humans. To the extent that errors are consistent,\\nour hypothesis is that this is due to at least a two factors:\\nnoise in the dataset (including mislabeled images) and out of\\ndistribution images being hard for both humans and models.\\n\\n5. Data Overlap Analysis\\nA concern with pre-training on a very large internet dataset\\nis unintentional overlap with downstream evals. This is\\nimportant to investigate since, in a worst-case scenario, a\\ncomplete copy of an evaluation dataset could leak into the\\npre-training dataset and invalidate the evaluation as a mean-\\ningful test of generalization. One option to prevent this is to\\nidentify and remove all duplicates before training a model.\\nWhile this guarantees reporting true hold-out performance,\\nit requires knowing all possible data which a model might\\nbe evaluated on ahead of time. This has the downside of\\nlimiting the scope of benchmarking and analysis. Adding a\\nnew evaluation would require an expensive re-train or risk\\nreporting an un-quantified benefit due to overlap.\\n\\nInstead, we document how much overlap occurs and how\\nperformance changes due to these overlaps. To do this, we\\nuse the following procedure:\\n\\n1) For each evaluation dataset, we run a duplicate detector\\n(see Appendix C) on its examples. We then manually inspect\\nthe found nearest neighbors and set a per dataset threshold\\nto keep high precision while maximizing recall. Using\\nthis threshold, we then create two new subsets, Overlap,\\nwhich contains all examples which have a similarity to a\\ntraining example above the threshold, and Clean, which\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 18\\n\\npu\\ng\\n\\nsp\\nhy\\n\\nnx\\nge\\n\\nrm\\nan\\n\\n_s\\nho\\n\\nrth\\nai\\n\\nre\\nd\\n\\nsh\\nib\\n\\na_\\nin\\n\\nu\\nbe\\n\\nag\\nle\\n\\ngr\\nea\\n\\nt_\\npy\\n\\nre\\nne\\n\\nes\\nen\\n\\ngl\\nish\\n\\n_s\\net\\n\\nte\\nr\\n\\nsa\\nm\\n\\noy\\ned\\n\\nsa\\nin\\n\\nt_\\nbe\\n\\nrn\\nar\\n\\nd\\npo\\n\\nm\\ner\\n\\nan\\nia\\n\\nn\\nne\\n\\nwf\\nou\\n\\nnd\\nla\\n\\nnd\\nwh\\n\\nea\\nte\\n\\nn_\\nte\\n\\nrri\\ner\\n\\nsc\\not\\n\\ntis\\nh_\\n\\nte\\nrri\\n\\ner\\nyo\\n\\nrk\\nsh\\n\\nire\\n_t\\n\\ner\\nrie\\n\\nr\\nsia\\n\\nm\\nes\\n\\ne\\nm\\n\\nin\\nia\\n\\ntu\\nre\\n\\n_p\\nin\\n\\nsc\\nhe\\n\\nr\\nha\\n\\nva\\nne\\n\\nse\\nke\\n\\nes\\nho\\n\\nnd\\nbo\\n\\nm\\nba\\n\\ny\\nm\\n\\nai\\nne\\n\\n_c\\noo\\n\\nn\\nch\\n\\nih\\nua\\n\\nhu\\na\\n\\nba\\nss\\n\\net\\n_h\\n\\nou\\nnd\\n\\nja\\npa\\n\\nne\\nse\\n\\n_c\\nhi\\n\\nn\\nru\\n\\nss\\nia\\n\\nn_\\nbl\\n\\nue\\nam\\n\\ner\\nica\\n\\nn_\\nbu\\n\\nlld\\nog\\n\\npe\\nrs\\n\\nia\\nn\\n\\nbe\\nng\\n\\nal\\nle\\n\\non\\nbe\\n\\nrg\\ner\\n\\nab\\nys\\n\\nsin\\nia\\n\\nn\\nbo\\n\\nxe\\nr\\n\\nbr\\niti\\n\\nsh\\n_s\\n\\nho\\nrth\\n\\nai\\nr\\n\\nst\\naf\\n\\nfo\\nrd\\n\\nsh\\nire\\n\\n_b\\nul\\n\\nl_t\\ner\\n\\nrie\\nr\\n\\nam\\ner\\n\\nica\\nn_\\n\\npi\\nt_\\n\\nbu\\nll_\\n\\nte\\nrri\\n\\ner\\neg\\n\\nyp\\ntia\\n\\nn_\\nm\\n\\nau\\nbi\\n\\nrm\\nan\\n\\nen\\ngl\\n\\nish\\n_c\\n\\noc\\nke\\n\\nr_\\nsp\\n\\nan\\nie\\n\\nl\\nra\\n\\ngd\\nol\\n\\nl\\n\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n (%\\n)\\n\\nZero-Shot CLIP\\nOne-Shot Human\\nZero-Shot Human\\n\\nFigure 16. The hardest problems for CLIP also tend to be the hard-\\nest problems for humans. Here we rank image categories by diffi-\\nculty for CLIP as measured as probability of the correct label.\\n\\ncontains all examples that are below this threshold. We\\ndenote the unaltered full dataset All for reference. From\\nthis we first record the degree of data contamination as the\\nratio of the number of examples in Overlap to the size of\\nAll.\\n\\n2) We then compute the zero-shot accuracy of CLIP\\nRN50x64 on the three splits and report All - Clean\\nas our main metric. This is the difference in accuracy due\\nto contamination. When positive it is our estimate of how\\nmuch the overall reported accuracy on the dataset was in-\\nflated by over-fitting to overlapping data.\\n\\n3) The amount of overlap is often small so we also run a\\nbinomial significance test where we use the accuracy on\\nClean as the null hypothesis and compute the one-tailed\\n(greater) p-value for the Overlap subset. We also calculate\\n99.5% Clopper-Pearson confidence intervals on Dirty as\\nanother check.\\n\\nA summary of this analysis is presented in Figure 17. Out\\nof 35 datasets studied, 9 datasets have no detected overlap\\nat all. Most of these datasets are synthetic or specialized\\nmaking them unlikely to be posted as normal images on\\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\\nare guaranteed to have no overlap due to containing novel\\ndata from after the date our dataset was created (ObjectNet\\nand Hateful Memes). This demonstrates our detector has\\na low-false positive rate which is important as false posi-\\ntives would under-estimate the effect of contamination in\\n\\nour analysis. There is a median overlap of 2.2% and an av-\\nerage overlap of 3.2%. Due to this small amount of overlap,\\noverall accuracy is rarely shifted by more than 0.1% with\\nonly 7 datasets above this threshold. Of these, only 2 are\\nstatistically significant after Bonferroni correction. The max\\ndetected improvement is only 0.6% on Birdsnap which has\\nthe second largest overlap at 12.1%. The largest overlap is\\nfor Country211 at 21.5%. This is due to it being constructed\\nout of YFCC100M, which our pre-training dataset contains\\na filtered subset of. Despite this large overlap there is only\\na 0.2% increase in accuracy on Country211. This may be\\nbecause the training text accompanying an example is often\\nnot related to the specific task a downstream eval measures.\\nCountry211 measures geo-localization ability, but inspect-\\ning the training text for these duplicates showed they often\\ndo not mention the location of the image.\\n\\nWe are aware of two potential concerns with our analysis.\\nFirst our detector is not perfect. While it achieves near\\n100% accuracy on its proxy training task and manual in-\\nspection + threshold tuning results in very high precision\\nwith good recall among the found nearest-neighbors, we can\\nnot tractably check its recall across 400 million examples.\\nAnother potential confounder of our analysis is that the un-\\nderlying data distribution may shift between the Overlap\\nand Clean subsets. For example, on Kinetics-700 many\\n“overlaps” are in fact all black transition frames. This ex-\\nplains why Kinetics-700 has an apparent 20% accuracy drop\\non Overlap. We suspect more subtle distribution shifts\\nlikely exist. One possibility we noticed on CIFAR-100 is\\nthat, due to the very low resolution of its images, many\\nduplicates were false positives of small objects such as birds\\nor planes. Changes in accuracy could instead be due to\\nchanges in the class distribution or difficulty of the dupli-\\ncates. Unfortunately, these distribution and difficulty shifts\\ncould also mask the effects of over-fitting.\\n\\nHowever, these results closely follow the findings of simi-\\nlar duplicate analysis in previous work on large scale pre-\\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\\ndetected similar overlap rates and found minimal changes in\\noverall performance. Importantly, Kolesnikov et al. (2019)\\nalso compared the alternative de-duplication strategy dis-\\ncussed in the introduction to this section with the approach\\nwe settled on and observed little difference between the two\\napproaches.\\n\\n6. Limitations\\nThere are still many limitations to CLIP. While several of\\nthese are discussed as part of analysis in various sections,\\nwe summarize and collect them here.\\n\\nOn datasets with training splits, the performance of zero-\\nshot CLIP is on average competitive with the simple su-\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 19\\n\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-20\\n\\n-10\\n\\n0\\n\\n10\\n\\n20\\n\\nDi\\nffe\\n\\nre\\nnc\\n\\ne \\nin\\n\\n A\\ncc\\n\\nur\\nac\\n\\ny \\non\\n\\n O\\nve\\n\\nrla\\npp\\n\\nin\\ng \\n\\nvs\\n. C\\n\\nle\\nan\\n\\n D\\nat\\n\\na \\n(%\\n\\n)\\n\\nSUN397\\n\\nCIFAR-100\\n\\nImageNet Sketch\\n\\nSUN\\n\\nKinetics-700\\n\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-0.75\\n\\n-0.5\\n\\n-0.25\\n\\n0\\n\\n0.25\\n\\n0.5\\n\\n0.75\\n\\nOv\\ner\\n\\nal\\nl A\\n\\ncc\\nur\\n\\nac\\ny \\n\\nCh\\nan\\n\\nge\\n D\\n\\nue\\n T\\n\\no \\nOv\\n\\ner\\nla\\n\\np \\n(%\\n\\n)\\n\\nStanford CarsSUN397\\n\\nBirdsnap\\nCIFAR-100\\n\\nFER2013\\n\\nCountry211\\nSUN\\n\\np < 1e-3\\np < 0.05\\np > 0.05\\n\\nFigure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have\\nup to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\\n99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\\nimprovements statistically significant when calculated using a one-sided binomial test.\\n\\npervised baseline of a linear classifier on top of ResNet-50\\nfeatures. On most of these datasets, the performance of\\nthis baseline is now well below the overall state of the art.\\nSignificant work is still needed to improve the task learning\\nand transfer capabilities of CLIP. While scaling has so far\\nsteadily improved performance and suggests a route for con-\\ntinued improvement, we estimate around a 1000x increase\\nin compute is required for zero-shot CLIP to reach overall\\nstate-of-the-art performance. This is infeasible to train with\\ncurrent hardware. Further research into improving upon the\\ncomputational and data efficiency of CLIP will be necessary.\\n\\nAnalysis in Section 3.1 found that CLIP’s zero-shot perfor-\\nmance is still quite weak on several kinds of tasks. When\\ncompared to task-specific models, the performance of CLIP\\nis poor on several types of fine-grained classification such\\nas differentiating models of cars, species of flowers, and\\nvariants of aircraft. CLIP also struggles with more abstract\\nand systematic tasks such as counting the number of objects\\nin an image. Finally for novel tasks which are unlikely to be\\nincluded in CLIP’s pre-training dataset, such as classifying\\nthe distance to the nearest car in a photo, CLIP’s perfor-\\nmance can be near random. We are confident that there are\\nstill many, many, tasks where CLIP’s zero-shot performance\\nis near chance level.\\n\\nWhile zero-shot CLIP generalizes well to many natural im-\\nage distributions as investigated in Section 3.3, we’ve ob-\\nserved that zero-shot CLIP still generalizes poorly to data\\nthat is truly out-of-distribution for it. An illustrative exam-\\nple occurs for the task of OCR as reported in Appendix E.\\n\\nCLIP learns a high quality semantic OCR representation that\\nperforms well on digitally rendered text, which is common\\nin its pre-training dataset, as evidenced by performance on\\nRendered SST2. However, CLIP only achieves 88% accu-\\nracy on the handwritten digits of MNIST. An embarrassingly\\nsimple baseline of logistic regression on raw pixels outper-\\nforms zero-shot CLIP. Both semantic and near-duplicate\\nnearest-neighbor retrieval verify that there are almost no im-\\nages that resemble MNIST digits in our pre-training dataset.\\nThis suggests CLIP does little to address the underlying\\nproblem of brittle generalization of deep learning models.\\nInstead CLIP tries to circumvent the problem and hopes that\\nby training on such a large and varied dataset that all data\\nwill be effectively in-distribution. This is a naive assumption\\nthat, as MNIST demonstrates, is easy to violate.\\n\\nAlthough CLIP can flexibly generate zero-shot classifiers\\nfor a wide variety of tasks and datasets, CLIP is still limited\\nto choosing from only those concepts in a given zero-shot\\nclassifier. This is a significant restriction compared to a\\ntruly flexible approach like image captioning which could\\ngenerate novel outputs. Unfortunately, as described in Sec-\\ntion 2.3 we found the computational efficiency of the image\\ncaption baseline we tried to be much lower than CLIP. A\\nsimple idea worth trying is joint training of a contrastive\\nand generative objective with the hope of combining the\\nefficiency of CLIP with the flexibility of a caption model.\\nAs another alternative, search could be performed at infer-\\nence time over many natural language explanations of a\\ngiven image, similar to approach proposed in Learning with\\nLatent Language Andreas et al. (2017).\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 20\\n\\nCLIP also does not address the poor data efficiency of deep\\nlearning. Instead CLIP compensates by using a source of\\nsupervision that can be scaled to hundreds of millions of\\ntraining examples. If every image seen during training of\\na CLIP model was presented at a rate of one per second,\\nit would take 405 years to iterate through the 12.8 billion\\nimages seen over 32 training epochs. Combining CLIP\\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\\nself-training (Lee; Xie et al., 2020) methods is a promising\\ndirection given their demonstrated ability to improve data\\nefficiency over standard supervised learning.\\n\\nOur methodology has several significant limitations. De-\\nspite our focus on zero-shot transfer, we repeatedly queried\\nperformance on full validation sets to guide the develop-\\nment of CLIP. These validation sets often have thousands\\nof examples, which is unrealistic for true zero-shot sce-\\nnarios. Similar concerns have been raised in the field of\\nsemi-supervised learning (Oliver et al., 2018). Another po-\\ntential issue is our selection of evaluation datasets. While\\nwe have reported results on Kornblith et al. (2019)’s 12\\ndataset evaluation suite as a standardized collection, our\\nmain results use a somewhat haphazardly assembled col-\\nlection of 27 datasets that is undeniably co-adapted with\\nthe development and capabilities of CLIP. Creating a new\\nbenchmark of tasks designed explicitly to evaluate broad\\nzero-shot transfer capabilities, rather than re-using existing\\nsupervised datasets, would help address these issues.\\n\\nCLIP is trained on text paired with images on the internet.\\nThese image-text pairs are unfiltered and uncurated and\\nresult in CLIP models learning many social biases. This\\nhas been previously demonstrated for image caption models\\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\\nfor detailed analysis and quantification of these behaviors for\\nCLIP as well as discussion of potential mitigation strategies.\\n\\nWhile we have emphasized throughout this work that speci-\\nfying image classifiers through natural language is a flexible\\nand general interface, it has its own limitations. Many com-\\nplex tasks and visual concepts can be difficult to specify\\njust through text. Actual training examples are undeniably\\nuseful but CLIP does not optimize for few-shot performance\\ndirectly. In our work, we fall back to fitting linear classifiers\\non top of CLIP’s features. This results in a counter-intuitive\\ndrop in performance when transitioning from a zero-shot\\nto a few-shot setting. As discussed in Section 4, this is\\nnotably different from human performance which shows a\\nlarge increase from a zero to a one shot setting. Future work\\nis needed to develop methods that combine CLIP’s strong\\nzero-shot performance with efficient few-shot learning.\\n\\n7. Broader Impacts\\nCLIP has a wide range of capabilities due to its ability to\\ncarry out arbitrary image classification tasks. One can give\\nit images of cats and dogs and ask it to classify cats, or give\\nit images taken in a department store and ask it to classify\\nshoplifters–a task with significant social implications and\\nfor which AI may be unfit. Like any image classification\\nsystem, CLIP’s performance and fitness for purpose need to\\nbe evaluated, and its broader impacts analyzed in context.\\nCLIP also introduces a capability that will magnify and alter\\nsuch issues: CLIP makes it possible to easily create your\\nown classes for categorization (to ‘roll your own classifier’)\\nwithout a need for re-training. This capability introduces\\nchallenges similar to those found in characterizing other,\\nlarge-scale generative models like GPT-3 (Brown et al.,\\n2020); models that exhibit non-trivial zero-shot (or few-\\nshot) generalization can have a vast range of capabilities,\\nmany of which are made clear only after testing for them.\\n\\nOur studies of CLIP in a zero-shot setting show that the\\nmodel displays significant promise for widely-applicable\\ntasks like image retrieval or search. For example, it can find\\nrelevant images in a database given text, or relevant text\\ngiven an image. Further, the relative ease of steering CLIP\\ntoward bespoke applications with little or no additional data\\nor training could unlock a variety of novel applications that\\nare hard for us to envision today, as has occurred with large\\nlanguage models over the past few years.\\n\\nIn addition to the more than 30 datasets studied in earlier\\nsections of this paper, we evaluate CLIP’s performance on\\nthe FairFace benchmark and undertake exploratory bias\\nprobes. We then characterize the model’s performance in\\na downstream task, surveillance, and discuss its usefulness\\nas compared with other available systems. Many of CLIP’s\\ncapabilities are omni-use in nature (e.g. OCR can be used\\nto make scanned documents searchable, to power screen\\nreading technologies, or to read license plates). Several\\nof the capabilities measured, from action recognition, ob-\\nject classification, and geo-localization, to facial emotion\\nrecognition, can be used in surveillance. Given its social\\nimplications, we address this domain of use specifically in\\nthe Surveillance section.\\n\\nWe have also sought to characterize the social biases inher-\\nent to the model. Our bias tests represent our initial efforts\\nto probe aspects of how the model responds in different sce-\\nnarios, and are by nature limited in scope. CLIP and models\\nlike it will need to be analyzed in relation to their specific\\ndeployments to understand how bias manifests and iden-\\ntify potential interventions. Further community exploration\\nwill be required to develop broader, more contextual, and\\nmore robust testing schemes so that AI developers can bet-\\nter characterize biases in general purpose computer vision\\nmodels.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 21\\n\\nModel Race Gender Age\\n\\nFairFace Model 93.7 94.2 59.7\\nLinear Probe CLIP 93.4 96.5 63.8\\nZero-Shot CLIP 58.3 95.9 57.1\\nLinear Probe Instagram 90.8 93.2 54.2\\n\\nTable 3. Percent accuracy on Race, Gender, and Age classification\\nof images in FairFace category ‘White’\\n\\nModel Race Gender Age\\n\\nFairFace Model 75.4 94.4 60.7\\nLinear Probe CLIP 92.8 97.7 63.1\\nZero-Shot CLIP 91.3 97.2 54.3\\nLinear Probe Instagram 87.2 93.9 54.1\\n\\nTable 4. Percent accuracy on Race, Gender, and Age classification\\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\\ngether as FairFace category ‘Non-White’)\\n\\nMiddle Southeast East\\nModel Gender Black White Indian Latino Eastern Asian Asian Average\\n\\nMale 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\\nLinear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\\n\\n97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5\\n\\nMale 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\\nZero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\\n\\n96.7 95.9 98.0 97.5 98.0 96.3 96.6\\n\\nMale 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\\nLinear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\\n\\n91.3 93.2 95.6 94.0 95.6 93.4 93.9\\n\\nTable 5. Percent accuracy on gender classification of images by FairFace race category\\n\\n7.1. Bias\\n\\nAlgorithmic decisions, training data, and choices about how\\nclasses are defined and taxonomized (which we refer to in-\\nformally as “class design”) can all contribute to and amplify\\nsocial biases and inequalities resulting from the use of AI\\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\\n& Star, 2000). Class design is particularly relevant to mod-\\nels like CLIP, since any developer can define a class and the\\nmodel will provide some result.\\n\\nIn this section, we provide preliminary analysis of some\\nof the biases in CLIP, using bias probes inspired by those\\noutlined in Buolamwini & Gebru (2018) and Kärkkäinen\\n& Joo (2019). We also conduct exploratory bias research\\nintended to find specific examples of biases in the model,\\nsimilar to that conducted by Solaiman et al. (2019).\\n\\nWe start by analyzing the performance of Zero-Shot CLIP on\\nthe face image dataset FairFace (Kärkkäinen & Joo, 2019)6\\n\\n6FairFace is a face image dataset designed to balance age, gen-\\nder, and race, in order to reduce asymmetries common in previous\\nface datasets. It categorizes gender into 2 groups: female and male\\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\\nAsian, Middle Eastern, and Latino. There are inherent problems\\nwith race and gender classifications, as e.g. Bowker & Star (2000)\\n\\nas an initial bias probe, then probe the model further to\\nsurface additional biases and sources of biases, including\\nclass design.\\n\\nWe evaluated two versions of CLIP on the FairFace dataset:\\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\\nsion classifier fitted to FairFace’s dataset on top of CLIP’s\\nfeatures (“LR CLIP”). We find that LR CLIP gets higher\\naccuracy on the FairFace dataset than both the ResNext-101\\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\\nhajan et al., 2018) and FairFace’s own model on most of the\\nclassification tests we ran7. ZS CLIP’s performance varies\\nby category and is worse than that of FairFace’s model for a\\nfew categories, and better for others. (See Table 3 and Table\\n4).\\n\\nand Keyes (2018) have shown. While FairFace’s dataset reduces\\nthe proportion of White faces, it still lacks representation of entire\\nlarge demographic groups, effectively erasing such categories. We\\nuse the 2 gender categories and 7 race categories defined in the\\nFairFace dataset in a number of our experiments not in order to\\nreinforce or endorse the use of such reductive categories, but in\\norder to enable us to make comparisons to prior work.\\n\\n7One challenge with this comparison is that the FairFace model\\nuses binary classes for race (“White” and “Non-White”), instead\\nof breaking down races into finer-grained sub-groups.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 22\\n\\nMiddle Southeast East\\nCategory Black White Indian Latino Eastern Asian Asian\\n\\nCrime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\\nNon-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0\\n\\nTable 6. Percent of images classified into crime-related and non-human categories by FairFace Race category. The label set included 7\\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\\n\\nCategory Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\\n\\nDefault Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\\nDefault Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4\\n\\nTable 7. Percent of images classified into crime-related and non-human categories by FairFace Age category, showing comparison between\\nresults obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\\n\\nAdditionally, we test the performance of the LR CLIP and\\nZS CLIP models across intersectional race and gender cate-\\ngories as they are defined in the FairFace dataset. We find\\nthat model performance on gender classification is above\\n95% for all race categories. Table 5 summarizes these re-\\nsults.\\n\\nWhile LR CLIP achieves higher accuracy than the Linear\\nProbe Instagram model on the FairFace benchmark dataset\\nfor gender, race and age classification of images by intersec-\\ntional categories, accuracy on benchmarks offers only one\\napproximation of algorithmic fairness, as Raji et al. (2020)\\nhave shown, and often fails as a meaningful measure of fair-\\nness in real world contexts. Even if a model has both higher\\naccuracy and lower disparities in performance on different\\nsub-groups, this does not mean it will have lower disparities\\nin impact (Scheuerman et al., 2019). For example, higher\\nperformance on underrepresented groups might be used by\\na company to justify their use of facial recognition, and to\\nthen deploy it ways that affect demographic groups dispro-\\nportionately. Our use of facial classification benchmarks to\\nprobe for biases is not intended to imply that facial classi-\\nfication is an unproblematic task, nor to endorse the use of\\nrace, age, or gender classification in deployed contexts.\\n\\nWe also probed the model using classification terms with\\nhigh potential to cause representational harm, focusing on\\ndenigration harms in particular (Crawford, 2017). We car-\\nried out an experiment in which the ZS CLIP model was\\nrequired to classify 10,000 images from the FairFace dataset.\\nIn addition to the FairFace classes, we added in the follow-\\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\\n‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\\nexperiment was to check if harms of denigration dispropor-\\ntionately impact certain demographic subgroups.\\n\\nWe found that 4.9% (confidence intervals between 4.6%\\nand 5.4%) of the images were misclassified into one of\\nthe non-human classes we used in our probes (‘animal’,\\n‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\\nimages had the highest misclassification rate (approximately\\n14%; confidence intervals between [12.6% and 16.4%])\\nwhile all other races had misclassification rates under 8%.\\nPeople aged 0-20 years had the highest proportion being\\nclassified into this category at 14% .\\n\\nWe also found that 16.5% of male images were misclassified\\ninto classes related to crime (‘thief’, ‘suspicious person’ and\\n‘criminal’) as compared to 9.8% of female images. Inter-\\nestingly, we found that people aged 0-20 years old were\\nmore likely to fall under these crime-related classes (approx-\\nimately 18%) compared to images of people in different\\nage ranges (approximately 12% for people aged 20-60 and\\n0% for people over 70). We found significant disparities in\\nclassifications across races for crime related terms, which is\\ncaptured in Table 6.\\n\\nGiven that we observed that people under 20 were the most\\nlikely to be classified in both the crime-related and non-\\nhuman animal categories, we carried out classification for\\nthe images with the same classes but with an additional\\ncategory ‘child’ added to the categories. Our goal here\\nwas to see if this category would significantly change the\\nbehaviour of the model and shift how the denigration harms\\nare distributed by age. We found that this drastically reduced\\nthe number of images of people under 20 classified in either\\ncrime-related categories or non-human animal categories\\n(Table 7). This points to how class design has the potential\\nto be a key factor determining both the model performance\\nand the unwanted biases or behaviour the model may exhibit\\nwhile also asks overarching questions about the use of face\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 23\\n\\nimages to automatically classify people along such lines\\n(y Arcas et al., 2017).\\n\\nThe results of these probes can change based on the class\\ncategories one chooses to include as well as the specific\\nlanguage one uses to describe each class. Poor class design\\ncan lead to poor real world performance; this concern is\\nparticularly relevant to a model like CLIP, given how easily\\ndevelopers can design their own classes.\\n\\nWe also carried out experiments similar to those outlined by\\nSchwemmer et al. (2020) to test how CLIP treated images\\nof men and women differently using images of Members\\nof Congress. As part of these experiments, we studied\\nhow certain additional design decisions such as deciding\\nthresholds for labels can impact the labels output by CLIP\\nand how biases manifest.\\n\\nWe carried out three experiments - we tested for accuracy\\non gender classification and we tested for how labels were\\ndifferentially distributed across two different label sets. For\\nour first label set, we used a label set of 300 occupations and\\nfor our second label set we used a combined set of labels that\\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\\nAzure Computer Vision returned for all the images.\\n\\nWe first simply looked into gender prediction performance\\nof the model on the images of Members of Congress, in\\norder to check to see if the model correctly recognized\\nmen as men and women as women given the image of a\\nperson who appeared to be in an official setting/position of\\npower. We found that the model got 100% accuracy on the\\nimages. This is slightly better performance than the model’s\\nperformance on the FairFace dataset. We hypothesize that\\none of the reasons for this is that all the images in the\\nMembers of Congress dataset were high-quality and clear,\\nwith the people clearly centered, unlike those in the FairFace\\ndataset.\\n\\nIn order to study how the biases in returned labels depend on\\nthe thresholds set for label probability, we did an experiment\\nin which we set threshold values at 0.5% and 4.0%. We\\nfound that the lower threshold led to lower quality of labels.\\nHowever, even the differing distributions of labels under\\nthis threshold can hold signals for bias. For example, we\\nfind that under the 0.5% threshold labels such as ‘nanny’\\nand ‘housekeeper’ start appearing for women whereas labels\\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\\nThis points to gendered associations similar to those that\\nhave previously been found for occupations (Schwemmer\\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\\n\\nAt the higher 4% threshold, the labels with the highest prob-\\nability across both genders include “lawmaker”, “legislator”\\nand “congressman”. However, the presence of these biases\\namongst lower probability labels nonetheless point to larger\\nquestions about what ‘sufficiently’ safe behaviour may look\\n\\nlike for deploying such systems.\\n\\nWhen given the combined set of labels that Google Cloud\\nVision (GCV), Amazon Rekognition and Microsoft returned\\nfor all the images, similar to the biases Schwemmer et al.\\n(2020) found in GCV systems, we found our system also\\ndisproportionately attached labels to do with hair and ap-\\npearance in general to women more than men. For ex-\\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\\nappeared significantly more often for women. Additionally,\\nCLIP attached some labels that described high status occu-\\npations disproportionately more often to men such as ‘ex-\\necutive’ and ‘doctor’. Out of the only four occupations that\\nit attached more often to women, three were ‘newscaster’,\\n‘television presenter’ and ‘newsreader’ and the fourth was\\n‘Judge’. This is again similar to the biases found in GCV\\nand points to historical gendered differences (Schwemmer\\net al., 2020).\\n\\nInterestingly, when we lowered the threshold to 0.5% for\\nthis set of labels, we found that the labels disproportionately\\ndescribing men also shifted to appearance oriented words\\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\\ntion oriented words such as ‘military person’ and ‘executive’\\n- which were not used to describe images of women at the\\nhigher 4% threshold - were used for both men and women\\nat the lower 0.5% threshold, which could have caused the\\nchange in labels for men. The reverse was not true. Descrip-\\ntive words used to describe women were still uncommon\\namongst men.\\n\\nDesign decisions at every stage of building a model impact\\nhow biases manifest and this is especially true for CLIP\\ngiven the flexibility it offers. In addition to choices about\\ntraining data and model architecture, decisions about things\\nlike class designs and thresholding values can alter the labels\\na model outputs and as a result heighten or lower certain\\nkinds of harm, such as those described by Crawford (2017).\\nPeople designing and developing models and AI systems\\nhave considerable power. Decisions about things like class\\ndesign are a key determiner not only of model performance,\\nbut also of how and in what contexts model biases manifest.\\n\\nThese experiments are not comprehensive. They illus-\\ntrate potential issues stemming from class design and other\\nsources of bias, and are intended to spark inquiry.\\n\\n7.2. Surveillance\\n\\nWe next sought to characterize model performance in re-\\nlation to a downstream task for which there is significant\\nsocietal sensitivity: surveillance. Our analysis aims to better\\nembody the characterization approach described above and\\nto help orient the research community towards the potential\\nfuture impacts of increasingly general purpose computer\\nvision models and aid the development of norms and checks\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 24\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nblouse\\npurple\\n\\nnewsreader\\nbangs\\n\\npink\\npixie cut\\n\\nblack hair\\nbob cut\\n\\nmagenta\\nhot\\n\\nlaughing\\nblazer\\n\\nspokesperson\\nblonde\\n\\npublic speaking\\nsenior citizen\\n\\nlooking\\nfemale\\n\\nlady\\nwoman\\n\\nTop labels,\\nimages of women\\n\\nWomen\\nMen\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nyellow\\nnecktie\\n\\nkid\\nfrown\\n\\nshoulder\\ntie\\n\\ndisplay\\nelder\\n\\nphotograph\\nwalking\\n\\nmilitary officer\\nphoto\\n\\nsuit\\nfacial expression\\n\\nhead\\nblack\\n\\nplayer\\nface\\n\\nmale\\nman\\n\\nTop labels,\\nimages of men\\n\\nWomen\\nMen\\n\\nFigure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\\nidentified with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\\ncertain label by gender.\\n\\naround such systems. Our inclusion of surveillance is not\\nintended to indicate enthusiasm for this domain - rather, we\\nthink surveillance is an important domain to try to make\\npredictions about given its societal implications (Zuboff,\\n2015; Browne, 2015).\\n\\nWe measure the model’s performance on classification of\\nimages from CCTV cameras and zero-shot celebrity identifi-\\ncation. We first tested model performance on low-resolution\\nimages captured from surveillance cameras (e.g. CCTV\\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\\ndata captured by Varadarajan & Odobez (2009), which both\\nconsist of real world outdoor scenes with non-actors.\\n\\nGiven CLIP’s flexible class construction, we tested 515\\nsurveillance images captured from 12 different video se-\\nquences on self-constructed general classes for coarse and\\nfine grained classification. Coarse classification required the\\nmodel to correctly identify the main subject of the image (i.e.\\ndetermine if the image was a picture of an empty parking\\nlot, school campus, etc.). For fine-grained classification, the\\nmodel had to choose between two options constructed to\\ndetermine if the model could identify the presence/absence\\nof smaller features in the image such as a person standing\\nin the corner.\\n\\nFor coarse classification, we constructed the classes by hand-\\ncaptioning the images ourselves to describe the contents\\nof the image and there were always at least 6 options for\\n\\nthe model to choose from. Additionally, we carried out a\\n‘stress test’ where the class set included at least one more\\ncaption for something that was ‘close’ to the image (for\\nexample, ‘parking lot with white car’ vs. ‘parking lot with\\nred car’). We found that the model had a top-1 accuracy\\nof 91.8% on the CCTV images for the initial evaluation.\\nThe accuracy dropped significantly to 51.1% for the second\\nevaluation, with the model incorrectly choosing the ‘close’\\nanswer 40.7% of the time.\\n\\nFor fine-grained detection, the zero-shot model performed\\npoorly, with results near random. Note that this experiment\\nwas targeted only towards detecting the presence or absence\\nof small objects in image sequences.\\n\\nWe also tested CLIP’s zero-shot performance for ‘in the\\nwild’ identity detection using the CelebA dataset8. We did\\nthis to evaluate the model’s performance for identity detec-\\ntion using just the publicly available data it was pre-trained\\non. While we tested this on a dataset of celebrities who have\\na larger number of images on the internet, we hypothesize\\nthat the number of images in the pre-training data needed\\nfor the model to associate faces with names will keep de-\\ncreasing as models get more powerful (see Table 8), which\\nhas significant societal implications (Garvie, 2019). This\\n\\n8Note: The CelebA dataset is more representative of faces with\\nlighter skin tones. Due to the nature of the dataset, we were not\\nable to control for race, gender, age, etc.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 25\\n\\nModel 100 Classes 1k Classes 2k Classes\\n\\nCLIP L/14 59.2 43.3 42.2\\nCLIP RN50x64 56.4 39.5 38.4\\nCLIP RN50x16 52.7 37.4 36.3\\nCLIP RN50x4 52.8 38.1 37.3\\n\\nTable 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy\\n\\nmirrors recent developments in natural language processing,\\nin which recent large language models trained on Internet\\ndata often exhibit a surprising ability to provide informa-\\ntion related to relatively minor public figures (Brown et al.,\\n2020).\\n\\nWe found that the model had 59.2% top-1 accuracy out\\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\\nages. However, this performance dropped to 43.3% when\\nwe increased our class sizes to 1k celebrity names. This\\nperformance is not competitive when compared to produc-\\ntion level models such as Google’s Celebrity Recognition\\n(Google). However, what makes these results noteworthy is\\nthat this analysis was done using only zero-shot identifica-\\ntion capabilities based on names inferred from pre-training\\ndata - we didn’t use any additional task-specific dataset, and\\nso the (relatively) strong results further indicate that before\\ndeploying multimodal models, people will need to carefully\\nstudy them for behaviors in a given context and domain.\\n\\nCLIP offers significant benefit for tasks that have relatively\\nlittle data given its zero-shot capabilities. However, large\\ndatasets and high performing supervised models exist for\\nmany in-demand surveillance tasks such as facial recogni-\\ntion. As a result, CLIP’s comparative appeal for such uses\\nis low. Additionally, CLIP is not designed for common\\nsurveillance-relevant tasks like object detection and seman-\\ntic segmentation. This means it has limited use for certain\\nsurveillance tasks when models that are designed with these\\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\\navailable.\\n\\nHowever, CLIP does unlock a certain aspect of usability\\ngiven how it removes the need for training data. Thus, CLIP\\nand similar models could enable bespoke, niche surveillance\\nuse cases for which no well-tailored models or datasets exist,\\nand could lower the skill requirements to build such appli-\\ncations. As our experiments show, ZS CLIP displays non-\\ntrivial, but not exceptional, performance on a few surveil-\\nlance relevant tasks today.\\n\\n7.3. Future Work\\n\\nThis preliminary analysis is intended to illustrate some of\\nthe challenges that general purpose computer vision models\\npose and to give a glimpse into their biases and impacts.\\n\\nWe hope that this work motivates future research on the\\ncharacterization of the capabilities, shortcomings, and biases\\nof such models, and we are excited to engage with the\\nresearch community on such questions.\\n\\nWe believe one good step forward is community exploration\\nto further characterize the capabilities of models like CLIP\\nand - crucially - identify application areas where they have\\npromising performance and areas where they may have\\nreduced performance9. This process of characterization can\\nhelp researchers increase the likelihood models are used\\nbeneficially by:\\n\\n• Identifying potentially beneficial downstream uses of\\nmodels early in the research process, enabling other\\nresearchers to think about applications.\\n\\n• Surfacing tasks with significant sensitivity and a large\\nset of societal stakeholders, which may call for inter-\\nvention by policymakers.\\n\\n• Better characterizing biases in models, alerting other\\nresearchers to areas of concern and areas for interven-\\ntions.\\n\\n• Creating suites of tests to evaluate systems like CLIP\\non, so we can better characterize model capabilities\\nearlier in the development cycle.\\n\\n• Identifying potential failure modes and areas for further\\nwork.\\n\\nWe plan to contribute to this work, and hope this analysis\\nprovides some motivating examples for subsequent research.\\n\\n8. Related Work\\nAny model that leverages written, spoken, signed or any\\nother form of human language as part of its training signal\\nis arguably using natural language as a source of supervi-\\nsion. This is an admittedly extremely broad area and covers\\nmost work in the field of distributional semantics including\\ntopic models (Blei et al., 2003), word, sentence, and para-\\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\\nMikolov, 2014), and language models (Bengio et al., 2003).\\nIt also includes much of the broader field of NLP that deals\\nwith predicting or modeling sequences of natural language\\nin some way. Work in NLP intentionally leveraging natural\\nlanguage supervision in the form of explanations, feedback,\\ninstructions, and advice for tasks such as classification (as\\nopposed to the commonly used representation of supervision\\nas a set of arbitrarily encoded discrete category labels) has\\n\\n9A model could be unfit for use due to inadequate performance\\nor due to the inappropriateness of AI use in the application area\\nitself.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 26\\n\\nbeen explored in many creative and advanced ways. Dialog\\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\\n2019) develops techniques to learn from interactive natural\\nlanguage feedback in dialog. Several papers have leveraged\\nsemantic parsing to convert natural language explanations\\ninto features (Srivastava et al., 2017) or additional training\\nlabels (Hancock et al., 2018). More recently, ExpBERT\\n(Murty et al., 2020) uses feature representations produced\\nby conditioning a deep contextual language model on nat-\\nural language explanations and descriptions of relations to\\nimprove performance on the task of relation extraction.\\n\\nCLIP is an example of using natural language as a training\\nsignal for learning about a domain other than language. In\\nthis context, the earliest use of the term natural language\\nsupervision that we are aware of is the work of Ramanathan\\net al. (2013) which showed that natural language descrip-\\ntions could be used along side other sources of supervision\\nto improve performance on the task of video event under-\\nstanding. However, as mentioned in the introduction and\\napproach section, methods of leveraging natural language\\ndescriptions in computer vision well predate the use of this\\nspecific term, especially for image retrieval (Mori et al.,\\n1999) and object classification (Wang et al., 2009). Other\\nearly work leveraged tags (but not natural language) asso-\\nciated with images for the task of semantic segmentation\\n(Barnard et al., 2003). More recently, He & Peng (2017)\\nand Liang et al. (2020) demonstrated using natural language\\ndescriptions and explanations to improve fine-grained vi-\\nsual classification of birds. Others have investigated how\\ngrounded language can be used to improve visual represen-\\ntations and classifiers on the ShapeWorld dataset (Kuhnle\\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\\nFinally, techniques which combine natural language with\\nreinforcement learning environments (Narasimhan et al.,\\n2015) have demonstrated exciting emergent behaviors such\\nas systematically accomplishing zero-shot tasks (Hill et al.,\\n2019).\\n\\nCLIP’s pre-training task optimizes for text-image retrieval.\\nThis areas of research dates back to the mid-90s with the\\npreviously mentioned Mori et al. (1999) as representative of\\nearly work. While initial efforts focused primarily on predic-\\ntive objectives over time research shifted towards learning\\njoint multi-modal embedding spaces with techniques like\\nkernel Canonical Correlation Analysis and various ranking\\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\\nHodosh et al., 2013). Over time work explored many combi-\\nnations of training objective, transfer, and more expressive\\nmodels and steadily improved performance (Frome et al.,\\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\\n2014; Faghri et al., 2017).\\n\\nOther work has leveraged natural language supervision for\\ndomains other than images. Stroud et al. (2020) explores\\n\\nlarge scale representation learning by training a system to\\npair descriptive text with videos instead of images. Several\\nworks have explored using dense spoken natural language\\nsupervision for videos (Miech et al., 2019; 2020b). When\\nconsidered together with CLIP, these works suggest that\\nlarge scale natural language supervision is a promising way\\nto learn high quality perceptual systems for many domains.\\nAlayrac et al. (2020) extended this line of work to an addi-\\ntional modality by adding raw audio as an additional super-\\nvision source and demonstrated benefits from combining all\\nthree sources of supervision.\\n\\nAs part of our work on CLIP we also construct a new dataset\\nof image-text pairs. Modern work on image-text retrieval\\nhas relied on a set of crowd-sourced sentence level im-\\nage caption evaluation datasets like Pascal1K (Rashtchian\\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\\n(Young et al., 2014). However, these datasets are still rel-\\natively small and limit achievable performance. Several\\nmethods have been proposed to create larger datasets au-\\ntomatically with Ordonez et al. (2011) as a notable early\\nexample. In the deep learning era, Mithun et al. (2018)\\ndemonstrated an additional set of (image, text) pairs col-\\nlected from the internet could improve retrieval performance\\nand several new automatically constructed datasets such as\\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\\n2020), and OCR-CC (Yang et al., 2020) have been created.\\nHowever, these datasets still use significantly more aggres-\\nsive filtering or are designed for a specific task such as OCR\\nand as a result are still much smaller than WIT with between\\n1 and 10 million training examples.\\n\\nA related idea to CLIP is webly supervised learning. This\\nline of work queries image search engines to build image\\ndatasets by querying for terms and uses the queries as the\\nlabels for the returned images (Fergus et al., 2005). Classi-\\nfiers trained on these large but noisily labeled datasets can\\nbe competitive with those trained on smaller carefully la-\\nbeled datasets. These image-query pairs are also often used\\nto improve performance on standard datasets as additional\\ntraining data (Chen & Gupta, 2015). CLIP also uses search\\nqueries as part of its dataset creation process. However\\nCLIP only uses full text sequences co-occuring with images\\nas supervision rather than just the queries, which are often\\nonly a single word or short n-gram. We also restrict this step\\nin CLIP to text only querying for sub-string matches while\\nmost webly supervised work uses standard image search\\nengines which have their own complex retrieval and filter-\\ning pipelines that often involve computer vision systems.\\nOf this line of work, Learning Everything about Anything:\\nWebly-Supervised Visual Concept Learning (Divvala et al.,\\n2014) has a notably similar ambition and goal as CLIP.\\n\\nFinally, CLIP is related to a recent burst of activity on learn-\\ning joint models of vision and language (Lu et al., 2019; Tan\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 27\\n\\n& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\\n2020). This line of work focuses on richly connecting vision\\nand language in order to solve complex downstream tasks\\nsuch as visual question answering, visual commonsense\\nreasoning, or multimodal entailment. These approaches\\nleverage impressively engineered models which combine 3\\n(or more) pre-trained subsystems, typically an image feature\\nmodel, a region proposal / object detection model, and a\\npre-trained masked language model such as BERT. These\\nsystems are then jointly fine-tuned via various training objec-\\ntives on image-text pairs and applied to the aforementioned\\ntasks and achieve impressive results. CLIP is instead fo-\\ncused on learning visual models from scratch via natural\\nlanguage supervision and does not densely connect the two\\ndomains with a joint attention model. The only interaction\\nin a CLIP model between the image and text domain is a\\nsingle dot product in a learned joint embedding space. We\\nare excited to see CLIP hybridized with this line of work.\\n\\n9. Conclusion\\nWe have investigated whether it is possible to transfer the\\nsuccess of task-agnostic web-scale pre-training in NLP to\\nanother domain. We find that adopting this formula re-\\nsults in similar behaviors emerging in the field of computer\\nvision and discuss the social implications of this line of\\nresearch. In order to optimize their training objective, CLIP\\nmodels learn to perform a wide variety of tasks during pre-\\ntraining. This task learning can then be leveraged via natural\\nlanguage prompting to enable zero-shot transfer to many\\nexisting datasets. At sufficient scale, the performance of this\\napproach can be competitive with task-specific supervised\\nmodels although there is still room for much improvement.\\n\\nACKNOWLEDGMENTS\\n\\nWe’d like to thank the millions of people involved in creating\\nthe data CLIP is trained on. We’d also like to thank Susan\\nZhang for her work on image conditional language models\\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\\nGillian Hadfield for their thoughtful feedback on the broader\\nimpacts section of the paper. We are also grateful to the\\nAcceleration and Supercomputing teams at OpenAI for their\\ncritical work on software and hardware infrastructure this\\nproject used. Finally, we’d also like to thank the developers\\nof the many software packages used throughout this project\\nincluding, but not limited, to Numpy (Harris et al., 2020),\\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\\npandas (pandas development team, 2020), and scikit-learn\\n(Pedregosa et al., 2011).\\n\\nReferences\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\\n\\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\\nTensorflow: A system for large-scale machine learning. In\\n12th {USENIX} symposium on operating systems design\\nand implementation ({OSDI} 16), pp. 265–283, 2016.\\n\\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelović,\\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\\nand Zisserman, A. Self-supervised multimodal versatile\\nnetworks. arXiv preprint arXiv:2006.16228, 2020.\\n\\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\\nS., and Nguyen, A. Strike (with) a pose: Neural networks\\nare easily fooled by strange poses of familiar objects. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 4845–4854, 2019.\\n\\nAndreas, J., Klein, D., and Levine, S. Learning with latent\\nlanguage. arXiv preprint arXiv:1711.00482, 2017.\\n\\nAssiri, Y. Stochastic optimization of plain convolutional\\nneural networks with simple methods. arXiv preprint\\narXiv:2001.08856, 2020.\\n\\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning\\nrepresentations by maximizing mutual information across\\nviews. In Advances in Neural Information Processing\\nSystems, pp. 15535–15545, 2019.\\n\\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\\nlarge-scale bias-controlled dataset for pushing the lim-\\nits of object recognition models. In Advances in Neural\\nInformation Processing Systems, pp. 9453–9463, 2019.\\n\\nBarnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\\nD. M., and Jordan, M. I. Matching words and pictures.\\nJournal of machine learning research, 3(Feb):1107–1135,\\n2003.\\n\\nBechmann, A. and Bowker, G. C. Unsupervised by any\\nother name: Hidden layers of knowledge production in\\nartificial intelligence on social media. Big Data & Society,\\n6(1):205395171881956, January 2019. doi: 10.1177/\\n2053951718819569. URL https://doi.org/10.\\n1177/2053951718819569.\\n\\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\\nneural probabilistic language model. Journal of machine\\nlearning research, 3(Feb):1137–1155, 2003.\\n\\nBhargava, S. and Forsyth, D. Exposing and correcting the\\ngender bias in image captioning datasets and models.\\narXiv preprint arXiv:1912.00578, 2019.\\n\\nhttps://doi.org/10.1177/2053951718819569\\nhttps://doi.org/10.1177/2053951718819569\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 28\\n\\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\\nallocation. Journal of machine Learning research, 3(Jan):\\n993–1022, 2003.\\n\\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\\nKalai, A. T. Man is to computer programmer as woman\\nis to homemaker? debiasing word embeddings. Advances\\nin neural information processing systems, 29:4349–4357,\\n2016.\\n\\nBowker, G. C. and Star, S. L. Sorting things out: Classifica-\\ntion and its consequences. MIT press, 2000.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\narXiv preprint arXiv:2005.14165, 2020.\\n\\nBrowne, S. Dark Matters: Surveillance of Blackness. Duke\\nUniversity Press, 2015.\\n\\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\\nvisual representations with caption annotations. arXiv\\ne-prints, pp. arXiv–2008, 2020.\\n\\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\\ntional accuracy disparities in commercial gender classi-\\nfication. In Conference on fairness, accountability and\\ntransparency, pp. 77–91, 2018.\\n\\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\\nshort note on the kinetics-700 human action dataset. arXiv\\npreprint arXiv:1907.06987, 2019.\\n\\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\\nD., and Sutskever, I. Generative pretraining from pixels.\\nIn International Conference on Machine Learning, pp.\\n1691–1703. PMLR, 2020a.\\n\\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\\ndeep nets with sublinear memory cost. arXiv preprint\\narXiv:1604.06174, 2016.\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\\nsimple framework for contrastive learning of visual rep-\\nresentations. arXiv preprint arXiv:2002.05709, 2020b.\\n\\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\\nHinton, G. Big self-supervised models are strong semi-\\nsupervised learners. arXiv preprint arXiv:2006.10029,\\n2020c.\\n\\nChen, X. and Gupta, A. Webly supervised learning of\\nconvolutional networks. In Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 1431–\\n1439, 2015.\\n\\nChen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning. arXiv\\npreprint arXiv:2003.04297, 2020d.\\n\\nChen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\\nCheng, Y., and Liu, J. Uniter: Learning universal image-\\ntext representations. arXiv preprint arXiv:1909.11740,\\n2019.\\n\\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\\nclassification: Benchmark and state of the art. Proceed-\\nings of the IEEE, 105(10):1865–1883, 2017.\\n\\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\\nand Dahl, G. E. On empirical comparisons of optimiz-\\ners for deep learning. arXiv preprint arXiv:1910.05446,\\n2019.\\n\\nCoates, A., Ng, A., and Lee, H. An analysis of single-\\nlayer networks in unsupervised feature learning. In Pro-\\nceedings of the fourteenth international conference on\\nartificial intelligence and statistics, pp. 215–223, 2011.\\n\\nCrawford, K. The trouble with bias. NIPS 2017\\nKeynote, 2017. URL https://www.youtube.com/\\nwatch?v=fMym_BKWQzk.\\n\\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning.\\nIn Advances in neural information processing systems,\\npp. 3079–3087, 2015.\\n\\nD’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al. Underspecification presents\\nchallenges for credibility in modern machine learning.\\narXiv preprint arXiv:2011.03395, 2020.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\\nFei, L. ImageNet: A Large-Scale Hierarchical Image\\nDatabase. In CVPR09, 2009.\\n\\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\\nimage-net.org/challenges/LSVRC/2012/.\\n\\nDesai, K. and Johnson, J. Virtex: Learning visual rep-\\nresentations from textual annotations. arXiv preprint\\narXiv:2006.06666, 2020.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n\\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\\nand Sutskever, I. Jukebox: A generative model for music.\\narXiv preprint arXiv:2005.00341, 2020.\\n\\nhttps://www.youtube.com/watch?v=fMym_BKWQzk\\nhttps://www.youtube.com/watch?v=fMym_BKWQzk\\nhttp://www.image-net.org/challenges/LSVRC/2012/\\nhttp://www.image-net.org/challenges/LSVRC/2012/\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 29\\n\\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning\\neverything about anything: Webly-supervised visual con-\\ncept learning. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 3270–\\n3277, 2014.\\n\\nDodge, S. and Karam, L. A study and comparison of human\\nand deep learning recognition performance under visual\\ndistortions. In 2017 26th international conference on\\ncomputer communication and networks (ICCCN), pp. 1–\\n7. IEEE, 2017.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929, 2020.\\n\\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\\nfier: Zero-shot learning using purely textual descriptions.\\nIn Proceedings of the IEEE International Conference on\\nComputer Vision, pp. 2584–2591, 2013.\\n\\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\\nproving visual-semantic embeddings with hard negatives.\\narXiv preprint arXiv:1707.05612, 2017.\\n\\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\\ning object categories from google’s image search. In\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\\n2005.\\n\\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\\nRanzato, M., and Mikolov, T. Devise: A deep visual-\\nsemantic embedding model. In Advances in neural infor-\\nmation processing systems, pp. 2121–2129, 2013.\\n\\nGan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\\nLarge-scale adversarial training for vision-and-language\\nrepresentation learning. arXiv preprint arXiv:2006.06195,\\n2020.\\n\\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\\nguage models better few-shot learners. arXiv preprint\\narXiv:2012.15723, 2020.\\n\\nGarvie, C., May 2019. URL https://www.\\nflawedfacedata.com/.\\n\\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for\\nautonomous driving? the kitti vision benchmark suite. In\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), 2012.\\n\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\\nmann, F. A., and Brendel, W. Imagenet-trained cnns are\\n\\nbiased towards texture; increasing shape bias improves ac-\\ncuracy and robustness. arXiv preprint arXiv:1811.12231,\\n2018.\\n\\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\\ncut learning in deep neural networks. arXiv preprint\\narXiv:2004.07780, 2020.\\n\\nGomez, L., Patel, Y., Rusiñol, M., Karatzas, D., and Jawahar,\\nC. Self-supervised learning of visual features through\\nembedding images into text topic spaces. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 4230–4239, 2017.\\n\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\\ning and harnessing adversarial examples. arXiv preprint\\narXiv:1412.6572, 2014.\\n\\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\\nD., Lee, D.-H., et al. Challenges in representation learn-\\ning: A report on three machine learning contests. Neural\\nNetworks, 64:59–63, 2015.\\n\\nGoogle. Google cloud api: Celebrity recognition. URL\\nhttps://cloud.google.com/vision/docs/\\ncelebrity-recognition.\\n\\nGriewank, A. and Walther, A. Algorithm 799: revolve: an\\nimplementation of checkpointing for the reverse or ad-\\njoint mode of computational differentiation. ACM Trans-\\nactions on Mathematical Software (TOMS), 26(1):19–45,\\n2000.\\n\\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,\\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020.\\n\\nHa, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv\\npreprint arXiv:1609.09106, 2016.\\n\\nHancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\\nS., and Ré, C. Training classifiers with natural language\\nexplanations. In Proceedings of the conference. Associ-\\nation for Computational Linguistics. Meeting, volume\\n2018, pp. 1884. NIH Public Access, 2018.\\n\\nHancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\\nLearning from dialogue after deployment: Feed yourself,\\nchatbot! arXiv preprint arXiv:1901.05415, 2019.\\n\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\\nKerkwijk, M. H., Brett, M., Haldane, A., Fernández del\\n\\nhttps://www.flawedfacedata.com/\\nhttps://www.flawedfacedata.com/\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 30\\n\\nRı́o, J., Wiebe, M., Peterson, P., Gérard-Marchant, P.,\\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\\nGohlke, C., and Oliphant, T. E. Array programming\\nwith NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\\ns41586-020-2649-2.\\n\\nHays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image. In 2008 ieee confer-\\nence on computer vision and pattern recognition, pp. 1–8.\\nIEEE, 2008.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\\ninto rectifiers: Surpassing human-level performance on\\nimagenet classification. In Proceedings of the IEEE inter-\\nnational conference on computer vision, pp. 1026–1034,\\n2015.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016a.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016b.\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pp. 9729–\\n9738, 2020.\\n\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 558–\\n567, 2019.\\n\\nHe, X. and Peng, Y. Fine-grained image classification via\\ncombining vision and language. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition, pp. 5994–6002, 2017.\\n\\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\\nA novel dataset and deep learning benchmark for land\\nuse and land cover classification. IEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote\\nSensing, 12(7):2217–2226, 2019.\\n\\nHenaff, O. Data-efficient image recognition with contrastive\\npredictive coding. In International Conference on Ma-\\nchine Learning, pp. 4182–4192. PMLR, 2020.\\n\\nHendrycks, D. and Dietterich, T. Benchmarking neural\\nnetwork robustness to common corruptions and perturba-\\ntions. arXiv preprint arXiv:1903.12261, 2019.\\n\\nHendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\\nSong, D. Natural adversarial examples. arXiv preprint\\narXiv:1907.07174, 2019.\\n\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\\net al. The many faces of robustness: A critical analy-\\nsis of out-of-distribution generalization. arXiv preprint\\narXiv:2006.16241, 2020a.\\n\\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\\nR., and Song, D. Pretrained transformers improve out-of-\\ndistribution robustness. arXiv preprint arXiv:2004.06100,\\n2020b.\\n\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\\nY. Deep learning scaling is predictable, empirically. arXiv\\npreprint arXiv:1712.00409, 2017.\\n\\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\\nM., McClelland, J. L., and Santoro, A. Environmental\\ndrivers of systematicity and generalization in a situated\\nagent. In International Conference on Learning Repre-\\nsentations, 2019.\\n\\nHodosh, M., Young, P., and Hockenmaier, J. Framing image\\ndescription as a ranking task: Data, models and evaluation\\nmetrics. Journal of Artificial Intelligence Research, 47:\\n853–899, 2013.\\n\\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\\nEnhancing image geolocalization by combinatorial parti-\\ntioning of maps. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV), pp. 536–551, 2018.\\n\\nHoward, J. and Ruder, S. Universal language model\\nfine-tuning for text classification. arXiv preprint\\narXiv:1801.06146, 2018.\\n\\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\\nB., and Madry, A. Adversarial examples are not bugs,\\nthey are features. In Advances in Neural Information\\nProcessing Systems, pp. 125–136, 2019.\\n\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\narXiv preprint arXiv:1502.03167, 2015.\\n\\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\\nA. Deep structured output learning for unconstrained text\\nrecognition. arXiv preprint arXiv:1412.5903, 2014.\\n\\nJaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\\ntransformer networks. Advances in neural information\\nprocessing systems, 28:2017–2025, 2015.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 31\\n\\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\\nnostic dataset for compositional language and elementary\\nvisual reasoning. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pp.\\n2901–2910, 2017.\\n\\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\\nLearning visual features from large weakly supervised\\ndata. In European Conference on Computer Vision, pp.\\n67–84. Springer, 2016.\\n\\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\\nmodeling in 3d cnn architectures with bert for action\\nrecognition. arXiv preprint arXiv:2008.01232, 2020.\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361, 2020.\\n\\nKarpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\\nembeddings for bidirectional image sentence mapping.\\nIn Advances in neural information processing systems,\\npp. 1889–1897, 2014.\\n\\nKeyes, O. The misgendering machines: Trans/hci implica-\\ntions of automatic gender recognition. Proceedings of the\\nACM on Human-Computer Interaction, 2(CSCW):1–22,\\n2018.\\n\\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\narXiv preprint arXiv:2005.04790, 2020.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\\nvisual-semantic embeddings with multimodal neural lan-\\nguage models. arXiv preprint arXiv:1411.2539, 2014.\\n\\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\\nAdvances in neural information processing systems, 28:\\n3294–3302, 2015.\\n\\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\\nJ., Gelly, S., and Houlsby, N. Large scale learning of\\ngeneral visual representations for transfer. arXiv preprint\\narXiv:1912.11370, 2019.\\n\\nKornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\\nmodels transfer better? In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 2661–2671, 2019.\\n\\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\\nD. A., et al. Visual genome: Connecting language and\\nvision using crowdsourced dense image annotations. In-\\nternational journal of computer vision, 123(1):32–73,\\n2017.\\n\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassification with deep convolutional neural networks.\\nIn Advances in neural information processing systems,\\npp. 1097–1105, 2012.\\n\\nKuhnle, A. and Copestake, A. Shapeworld-a new test\\nmethodology for multimodal language understanding.\\narXiv preprint arXiv:1704.04517, 2017.\\n\\nKärkkäinen, K. and Joo, J. Fairface: Face attribute dataset\\nfor balanced race, gender, and age, 2019.\\n\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.\\n\\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning\\nto detect unseen object classes by between-class attribute\\ntransfer. In 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 951–958. IEEE, 2009.\\n\\nLarochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\\nof new tasks. 2008.\\n\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents. In International conference on\\nmachine learning, pp. 1188–1196, 2014.\\n\\nLeCun, Y. The mnist database of handwritten digits.\\nhttp://yann. lecun. com/exdb/mnist/.\\n\\nLee, D.-H. Pseudo-label: The simple and efficient semi-\\nsupervised learning method for deep neural networks.\\n\\nLei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\\nzero-shot convolutional neural networks using textual\\ndescriptions. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 4247–4255, 2015.\\n\\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\\nvisual n-grams from web data. In Proceedings of the\\nIEEE International Conference on Computer Vision, pp.\\n4183–4192, 2017.\\n\\nLi, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\\nUnicoder-vl: A universal encoder for vision and language\\nby cross-modal pre-training. 2020a.\\n\\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\\nLearning through dialogue interactions by asking ques-\\ntions. arXiv preprint arXiv:1612.04936, 2016.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 32\\n\\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\\nsemantics aligned pre-training for vision-language tasks.\\narXiv preprint arXiv:2004.06165, 2020b.\\n\\nLiang, W., Zou, J., and Yu, Z. Alice: Active learning with\\ncontrastive natural language explanations. arXiv preprint\\narXiv:2009.10259, 2020.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context. In European conference on\\ncomputer vision, pp. 740–755. Springer, 2014.\\n\\nLinzen, T. How can we accelerate progress towards\\nhuman-like linguistic generalization? arXiv preprint\\narXiv:2005.00955, 2020.\\n\\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\\ntimodal framework for the detection of hateful memes.\\narXiv preprint arXiv:2012.12871, 2020.\\n\\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\\nssi, R., Kaiser, L., and Shazeer, N. Generating\\nwikipedia by summarizing long sequences. arXiv preprint\\narXiv:1801.10198, 2018.\\n\\nLocatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S.,\\nSchölkopf, B., and Bachem, O. A sober look at the\\nunsupervised learning of disentangled representations\\nand their evaluation. arXiv preprint arXiv:2010.14766,\\n2020.\\n\\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\\ndient descent with warm restarts. arXiv preprint\\narXiv:1608.03983, 2016.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization. arXiv preprint arXiv:1711.05101, 2017.\\n\\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\\ntask-agnostic visiolinguistic representations for vision-\\nand-language tasks. In Advances in Neural Information\\nProcessing Systems, pp. 13–23, 2019.\\n\\nLu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\\nweakly supervised data and pose representation for action\\nrecognition, 2020. URL https://www.youtube.\\ncom/watch?v=KOQFxbPPLOE&t=1390s.\\n\\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\\nquet, O. Are gans created equal? a large-scale study.\\nAdvances in neural information processing systems, 31:\\n700–709, 2018.\\n\\nMahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\\nM., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\\nploring the limits of weakly supervised pretraining. In\\n\\nProceedings of the European Conference on Computer\\nVision (ECCV), pp. 181–196, 2018.\\n\\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\\nLearned in translation: Contextualized word vectors. In\\nAdvances in neural information processing systems, pp.\\n6294–6305, 2017.\\n\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\\nnatural language decathlon: Multitask learning as ques-\\ntion answering. arXiv preprint arXiv:1806.08730, 2018.\\n\\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\\nVenkatesh, G., et al. Mixed precision training. arXiv\\npreprint arXiv:1710.03740, 2017.\\n\\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\\nI., and Sivic, J. Howto100m: Learning a text-video em-\\nbedding by watching hundred million narrated video clips.\\nIn Proceedings of the IEEE international conference on\\ncomputer vision, pp. 2630–2640, 2019.\\n\\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\\nman, A. Rareact: A video dataset of unusual interactions.\\narXiv preprint arXiv:2008.01018, 2020a.\\n\\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\\nand Zisserman, A. End-to-end learning of visual represen-\\ntations from uncurated instructional videos. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 9879–9889, 2020b.\\n\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality. Advances in neural informa-\\ntion processing systems, 26:3111–3119, 2013.\\n\\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\\nof natural distribution shift on question answering models.\\narXiv preprint arXiv:2004.14444, 2020.\\n\\nMishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\\ntion using higher order language priors. 2012.\\n\\nMithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\\nChowdhury, A. K. Webly supervised joint embedding for\\ncross-modal image-text retrieval. In Proceedings of the\\n26th ACM international conference on Multimedia, pp.\\n1856–1864, 2018.\\n\\nMori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\\nformation based on dividing and vector quantizing images\\nwith words. Citeseer, 1999.\\n\\nMu, J., Liang, P., and Goodman, N. Shaping visual represen-\\ntations with language for few-shot classification. arXiv\\npreprint arXiv:1911.02683, 2019.\\n\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 33\\n\\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\\ncation estimation of photos using a hierarchical model\\nand scene classification. In Proceedings of the European\\nConference on Computer Vision (ECCV), pp. 563–579,\\n2018.\\n\\nMurty, S., Koh, P. W., and Liang, P. Expbert: Representation\\nengineering with natural language explanations. arXiv\\npreprint arXiv:2005.01932, 2020.\\n\\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language\\nunderstanding for text-based games using deep reinforce-\\nment learning. arXiv preprint arXiv:1506.08941, 2015.\\n\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y. Reading digits in natural images with\\nunsupervised feature learning. 2011.\\n\\nNoble, S. U. Algorithms of oppression: How search engines\\nreinforce racism. 2018.\\n\\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site. Group Dynamics: Theory, Research, and\\nPractice, 6(1):101, 2002.\\n\\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\\nA large-scale benchmark dataset for event recognition in\\nsurveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\\n2011.\\n\\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\\nfellow, I. Realistic evaluation of deep semi-supervised\\nlearning algorithms. Advances in neural information pro-\\ncessing systems, 31:3235–3246, 2018.\\n\\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\\ning with contrastive predictive coding. arXiv preprint\\narXiv:1807.03748, 2018.\\n\\nOrdonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. Advances\\nin neural information processing systems, 24:1143–1151,\\n2011.\\n\\npandas development team, T. pandas-dev/pandas: Pan-\\ndas, February 2020. URL https://doi.org/10.\\n5281/zenodo.3509134.\\n\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\\nC. V. Cats and dogs. In IEEE Conference on Computer\\nVision and Pattern Recognition, 2012.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\\n\\nBai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library. In Advances\\nin Neural Information Processing Systems 32, pp. 8024–\\n8035, 2019.\\n\\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research, 12:2825–2830, 2011.\\n\\nPennington, J., Socher, R., and Manning, C. D. Glove:\\nGlobal vectors for word representation. In Proceedings\\nof the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pp. 1532–1543, 2014.\\n\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. arXiv preprint arXiv:1802.05365,\\n2018.\\n\\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\\nA. Imagebert: Cross-modal pre-training with large-\\nscale weak-supervised image-text data. arXiv preprint\\narXiv:2001.07966, 2020.\\n\\nQuattoni, A., Collins, M., and Darrell, T. Learning visual\\nrepresentations using images with captions. In 2007 IEEE\\nConference on Computer Vision and Pattern Recognition,\\npp. 1–8. IEEE, 2007.\\n\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\\n\\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\\nJ., and Denton, E. Saving face: Investigating the ethical\\nconcerns of facial recognition auditing, 2020.\\n\\nRamanathan, V., Liang, P., and Fei-Fei, L. Video event\\nunderstanding using natural language descriptions. In\\nProceedings of the IEEE International Conference on\\nComputer Vision, pp. 905–912, 2013.\\n\\nRashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\\nCollecting image annotations using amazon’s mechanical\\nturk. In Proceedings of the NAACL HLT 2010 Workshop\\non Creating Speech and Language Data with Amazon’s\\nMechanical Turk, pp. 139–147, 2010.\\n\\nhttps://doi.org/10.5281/zenodo.3509134\\nhttps://doi.org/10.5281/zenodo.3509134\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 34\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\\nagenet classifiers generalize to imagenet? arXiv preprint\\narXiv:1902.10811, 2019.\\n\\nSalimans, T. and Kingma, D. P. Weight normalization: A\\nsimple reparameterization to accelerate training of deep\\nneural networks. In Advances in neural information pro-\\ncessing systems, pp. 901–909, 2016.\\n\\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\\ncomputers see gender: An evaluation of gender classifica-\\ntion in commercial facial analysis services. Proceedings\\nof the ACM on Human-Computer Interaction, 3(CSCW):\\n1–33, 2019.\\n\\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\\ngender bias in image recognition systems. Socius, 6:\\n2378023120967171, 2020.\\n\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909, 2015.\\n\\nShankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\\nand Schmidt, L. Do image classifiers generalize across\\ntime? arXiv preprint arXiv:1906.02168, 2019.\\n\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Proceedings\\nof the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\\n2565, 2018.\\n\\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\\nmodels that can read. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, pp.\\n8317–8326, 2019.\\n\\nSocher, R. and Fei-Fei, L. Connecting modalities: Semi-\\nsupervised segmentation and annotation of images using\\nunaligned text corpora. In 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition,\\npp. 966–973. IEEE, 2010.\\n\\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\\nsemantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods\\nin natural language processing, pp. 1631–1642, 2013.\\n\\nSocher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\\nA. Y. Grounded compositional semantics for finding and\\ndescribing images with sentences. Transactions of the\\nAssociation for Computational Linguistics, 2:207–218,\\n2014.\\n\\nSohn, K. Improved deep metric learning with multi-class\\nn-pair loss objective. In Advances in neural information\\nprocessing systems, pp. 1857–1865, 2016.\\n\\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\\nVoss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\\nMcGuffie, K., and Wang, J. Release strategies and the\\nsocial impacts of language models, 2019.\\n\\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\\nof 101 human actions classes from videos in the wild.\\narXiv preprint arXiv:1212.0402, 2012.\\n\\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/\\n10.5281/zenodo.2591652. Version 5.5.\\n\\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\\nwith deep boltzmann machines. In NIPS, 2012.\\n\\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept\\nlearning and semantic parsing from natural language ex-\\nplanations. In Proceedings of the 2017 conference on\\nempirical methods in natural language processing, pp.\\n1527–1536, 2017.\\n\\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Traffic Sign Recognition Benchmark: A multi-\\nclass classification competition. In IEEE International\\nJoint Conference on Neural Networks, pp. 1453–1460,\\n2011.\\n\\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\\nand Schmid, C. Learning video representations from tex-\\ntual web supervision. arXiv preprint arXiv:2007.14937,\\n2020.\\n\\nSzegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\\nA. Inception-v4, inception-resnet and the impact\\nof residual connections on learning. arXiv preprint\\narXiv:1602.07261, 2016.\\n\\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\\nencoder representations from transformers. arXiv preprint\\narXiv:1908.07490, 2019.\\n\\nTan, M. and Le, Q. V. Efficientnet: Rethinking model\\nscaling for convolutional neural networks. arXiv preprint\\narXiv:1905.11946, 2019.\\n\\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\\nand Schmidt, L. Measuring robustness to natural dis-\\ntribution shifts in image classification. arXiv preprint\\narXiv:2007.00644, 2020.\\n\\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\\nnew data in multimedia research. Communications of the\\nACM, 59(2):64–73, 2016.\\n\\nhttps://doi.org/10.5281/zenodo.2591652\\nhttps://doi.org/10.5281/zenodo.2591652\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 35\\n\\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview\\ncoding. arXiv preprint arXiv:1906.05849, 2019.\\n\\nTian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\\nIsola, P. Rethinking few-shot image classification: a\\ngood embedding is all you need? arXiv preprint\\narXiv:2003.11539, 2020.\\n\\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\\nimages: A large data set for nonparametric object and\\nscene recognition. IEEE transactions on pattern analysis\\nand machine intelligence, 30(11):1958–1970, 2008.\\n\\nTouvron, H., Vedaldi, A., Douze, M., and Jégou, H. Fix-\\ning the train-test resolution discrepancy. In Advances in\\nneural information processing systems, pp. 8252–8262,\\n2019.\\n\\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\\nanalysis and abnormality detection. In 2009 IEEE 12th\\nInternational Conference on Computer Vision Workshops,\\nICCV Workshops, pp. 1338–1345. IEEE, 2009.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in neural information\\nprocessing systems, pp. 5998–6008, 2017.\\n\\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\\nWelling, M. Rotation equivariant CNNs for digital pathol-\\nogy. June 2018.\\n\\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, İ.,\\nFeng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\\n1.0: Fundamental Algorithms for Scientific Computing\\nin Python. Nature Methods, 17:261–272, 2020. doi:\\n10.1038/s41592-019-0686-2.\\n\\nVo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\\ndeep learning era. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pp. 2621–2630,\\n2017.\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461, 2018.\\n\\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\\nbust global representations by penalizing local predictive\\npower. In Advances in Neural Information Processing\\nSystems, pp. 10506–10518, 2019.\\n\\nWang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\\nM., Wang, Y., and Liu, W. All you need is boundary: To-\\nward arbitrary-shaped text spotting. In Proceedings of the\\nAAAI Conference on Artificial Intelligence, volume 34,\\npp. 12160–12167, 2020.\\n\\nWang, J., Markert, K., and Everingham, M. Learning mod-\\nels for object recognition from natural language descrip-\\ntions. In BMVC, volume 1, pp. 2, 2009.\\n\\nWeston, J., Bengio, S., and Usunier, N. Large scale im-\\nage annotation: learning to rank with joint word-image\\nembeddings. Machine learning, 81(1):21–35, 2010.\\n\\nWeston, J. E. Dialog-based language learning. In Advances\\nin Neural Information Processing Systems, pp. 829–837,\\n2016.\\n\\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\\ncation with convolutional neural networks. In European\\nConference on Computer Vision, pp. 37–55. Springer,\\n2016.\\n\\nWu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\\nshick, R. Detectron2. https://github.com/\\nfacebookresearch/detectron2, 2019.\\n\\nWu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\\nlearning via non-parametric instance-level discrimination.\\narXiv preprint arXiv:1805.01978, 2018.\\n\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\\nwith noisy student improves imagenet classification. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 10687–10698, 2020.\\n\\ny Arcas, B. A., Mitchell, M., and Todorov,\\nA. Physiognomy’s new clothes. 2017.\\nURL https://medium.com/@blaisea/\\nphysiognomys-new-clothes-f2d4b59fdd6a.\\n\\nYang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\\npre-training for text-vqa and text-caption. arXiv preprint\\narXiv:2012.04638, 2020.\\n\\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\\nYu, L., Dyer, C., et al. Learning and evaluating general\\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\\n2019.\\n\\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\\nimage descriptions to visual denotations: New similarity\\nmetrics for semantic inference over event descriptions.\\nTransactions of the Association for Computational Lin-\\nguistics, 2:67–78, 2014.\\n\\nhttps://github.com/facebookresearch/detectron2\\nhttps://github.com/facebookresearch/detectron2\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 36\\n\\nYu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\\nlanguage representations through scene graph. arXiv\\npreprint arXiv:2006.16934, 2020.\\n\\nZeiler, M. D. and Fergus, R. Visualizing and understand-\\ning convolutional networks. In European conference on\\ncomputer vision, pp. 818–833. Springer, 2014.\\n\\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\\nmann, M., Dosovitskiy, A., et al. A large-scale study of\\nrepresentation learning with the visual task adaptation\\nbenchmark. arXiv preprint arXiv:1910.04867, 2019.\\n\\nZhang, R. Making convolutional networks shift-invariant\\nagain. arXiv preprint arXiv:1904.11486, 2019.\\n\\nZhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\\nglotz, C. P. Contrastive learning of medical visual repre-\\nsentations from paired images and text. arXiv preprint\\narXiv:2010.00747, 2020.\\n\\nZuboff, S. Big other: surveillance capitalism and the\\nprospects of an information civilization. Journal of Infor-\\nmation Technology, 30(1):75–89, 2015.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 37\\n\\nA. Linear-probe evaluation\\nWe provide additional details for linear probe experiments\\npresented in this paper, including the list of the datasets and\\nmodels used for evaluation.\\n\\nA.1. Datasets\\n\\nWe use the 12 datasets from the well-studied evaluation\\nsuite introduced by (Kornblith et al., 2019) and add 15\\nadditional datasets in order to assess the performance of\\nmodels on a wider variety of distributions and tasks. These\\ndatasets include MNIST, the Facial Expression Recognition\\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\\nfic Sign Recognition Benchmark (GTSRB) dataset (Stal-\\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\\nreira et al., 2019), 2,500 random samples of the CLEVR\\ndataset (Johnson et al., 2017), the Hateful Memes dataset\\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\\net al., 2012). For the two video datasets (UCF101 and Ki-\\nnetics700), we use the middle frame of each video clip as\\nthe input image. STL-10 and UCF101 have multiple pre-\\ndefined train/validation/test splits, 10 and 3 respectively, and\\nwe report the average over all splits. Details on each dataset\\nand the corresponding evaluation metrics are provided in\\nTable 9.\\n\\nAdditionally, we created two datasets that we call Coun-\\ntry211 and Rendered SST2. The Country211 dataset is\\ndesigned to assess the geolocation capability of visual rep-\\nresentations. We filtered the YFCC100m dataset (Thomee\\net al., 2016) to find 211 countries (defined as having an\\nISO-3166 country code) that have at least 300 photos with\\nGPS coordinates, and we built a balanced dataset with 211\\ncategories, by sampling 200 photos for training and 100\\nphotos for testing, for each country.\\n\\nThe Rendered SST2 dataset is designed to measure the opti-\\ncal character recognition capability of visual representations.\\nTo do so, we used the sentences from the Stanford Sentiment\\nTreebank dataset (Socher et al., 2013) and rendered them\\ninto images, with black texts on a white background, in a\\n448×448 resolution. Two example images from this dataset\\nare shown in Figure 19.\\n\\nA.2. Models\\n\\nIn combination with the datasets listed above, we evaluate\\nthe following series of models using linear probes.\\n\\nLM RN50 This is a multimodal model that uses an au-\\ntoregressive loss instead of a contrastive loss, while using\\n\\nthe ResNet-50 architecture as in the smallest contrastive\\nmodel. To do so, the output from the CNN is projected into\\nfour tokens, which are then fed as a prefix to a language\\nmodel autoregressively predicting the text tokens. Apart\\nfrom the training objective, the model was trained on the\\nsame dataset for the same number of epochs as other CLIP\\nmodels.\\n\\nCLIP-RN Five ResNet-based contrastive CLIP models\\nare included. As discussed in the paper, the first two models\\nfollow ResNet-50 and ResNet-101, and we use EfficientNet-\\nstyle (Tan & Le, 2019) scaling for the next three models\\nwhich simultaneously scale the model width, the number\\nof layers, and the input resolution to obtain models with\\nroughly 4x, 16x, and 64x computation.\\n\\nCLIP-ViT We include four CLIP models that use the Vi-\\nsion Transformer (Dosovitskiy et al., 2020) architecture as\\nthe image encoder. We include three models trained on 224-\\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\\nthe ViT-L/14 model fine-tuned on 336-by-336 pixel input\\nimages.\\n\\nEfficietNet We use the nine models (B0-B8) from the\\noriginal EfficientNet paper (Tan & Le, 2019), as well as\\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\\ntake the input resolutions of 475x475 and 800x800 pixels,\\nrespectively.\\n\\nInstagram-pretrained ResNeXt We use the four models\\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\\net al., 2018), as well as their two FixRes variants which use\\nhigher input resolutions (Touvron et al., 2019).\\n\\nBig Transfer (BiT) We use BiT-S and BiT-M models\\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\\nImageNet-21k datasets. The model weights for BiT-L is not\\npublicly available.\\n\\nVision Transformer (ViT) We also include four ViT\\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\\nL/16, and ViT-H/14. We note that their best-performing\\nmodels, trained on the JFT-300M dataset, are not available\\npublicly.\\n\\nSimCLRv2 The SimCLRv2 (Chen et al., 2020c) project\\nreleased pre-trained and fine-tuned models in various set-\\ntings. We use the seven pretrain-only checkpoints with\\nselective kernels.\\n\\nBYOL We use the recently released model weights of\\nBYOL (Grill et al., 2020), specifically their 50x1 and 200x2\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 38\\n\\nFigure 19. Two example images from the Rendered SST2 dataset\\n\\ncheckpoints.\\n\\nMomentum Contrast (MoCo) We include the MoCo-v1\\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\\ncheckpoints.\\n\\nVirTex We use the pretrained model of VirTex (Desai &\\nJohnson, 2020). We note that VirTex has a similar model\\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\\nof high-quality captions from MSCOCO.\\n\\nResNet We add the original ResNet checkpoints released\\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\\nResNet152.\\n\\nA.3. Evaluation\\n\\nWe use image features taken from the penultimate layer of\\neach model, ignoring any classification layer provided. For\\nCLIP-ViT models, we used the features before the linear\\nprojection to the embedding space, which corresponds to\\nI f in Figure 3. We train a logistic regression classifier\\nusing scikit-learn’s L-BFGS implementation, with maxi-\\nmum 1,000 iterations, and report the corresponding met-\\nric for each dataset. We determine the L2 regularization\\nstrength λ using a hyperparameter sweep on the validation\\nsets over the range between 10−6 and 106, with 96 log-\\narithmically spaced steps. To save compute required for\\nthe sweeps, we perform a parametric binary search that\\nstarts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\\neratively halves the interval around the peak until it reaches\\na resolution of 8 steps per decade. The hyperparameter\\nsweeps are performed on a validation split of each dataset.\\nFor the datasets that contain a validation split in addition to\\n\\na test split, we use the provided validation set to perform\\nthe hyperparameter search, and for the datasets that do not\\nprovide a validation split or have not published labels for\\nthe test data, we split the training dataset to perform the\\nhyperparameter search. For the final result, we combine the\\nvalidation split back with the training split and report the\\nperformance on the unused split.\\n\\nA.4. Results\\n\\nThe individual linear probe scores are provided in Table 10\\nand plotted in Figure 20. The best-performing CLIP model,\\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\\nachieved the state of the art in 21 of the 27 datasets, i.e.\\nincluded in the Clopper-Pearson 99.5% confidence interval\\naround each dataset’s top score. For many datasets, CLIP\\nperforms significantly better than other models, demonstrat-\\ning the advantage of natural language supervision over tradi-\\ntional pre-training approaches based on image classification.\\nSee Section 3.2 for more discussions on the linear probe\\nresults.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 39\\n\\nDataset Classes Train size Test size Evaluation metric\\n\\nFood-101 102 75,750 25,250 accuracy\\nCIFAR-10 10 50,000 10,000 accuracy\\nCIFAR-100 100 50,000 10,000 accuracy\\nBirdsnap 500 42,283 2,149 accuracy\\nSUN397 397 19,850 19,850 accuracy\\nStanford Cars 196 8,144 8,041 accuracy\\nFGVC Aircraft 100 6,667 3,333 mean per class\\nPascal VOC 2007 Classification 20 5,011 4,952 11-point mAP\\nDescribable Textures 47 3,760 1,880 accuracy\\nOxford-IIIT Pets 37 3,680 3,669 mean per class\\nCaltech-101 102 3,060 6,085 mean-per-class\\nOxford Flowers 102 102 2,040 6,149 mean per class\\n\\nMNIST 10 60,000 10,000 accuracy\\nFacial Emotion Recognition 2013 8 32,140 3,574 accuracy\\nSTL-10 10 1000 8000 accuracy\\nEuroSAT 10 10,000 5,000 accuracy\\nRESISC45 45 3,150 25,200 accuracy\\nGTSRB 43 26,640 12,630 accuracy\\nKITTI 4 6,770 711 accuracy\\nCountry211 211 43,200 21,100 accuracy\\nPatchCamelyon 2 294,912 32,768 accuracy\\nUCF101 101 9,537 1,794 accuracy\\nKinetics700 700 494,801 31,669 mean(top1, top5)\\nCLEVR Counts 8 2,000 500 accuracy\\nHateful Memes 2 8,500 500 ROC AUC\\nRendered SST2 2 7,792 1,821 accuracy\\nImageNet 1000 1,281,167 50,000 accuracy\\n\\nTable 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\\navailable online at the time of this writing.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 40\\n\\nFo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nC\\nar\\n\\ns\\n\\nA\\nir\\n\\ncr\\naf\\n\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n?\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nA\\n\\nM\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nSS\\nT\\n\\nIm\\nag\\n\\neN\\net\\n\\nLM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nN\\n\\n50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\\n101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\\n50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\\n\\n50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\\n50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6\\n\\nC\\nL\\n\\nIP\\n-V\\n\\niT B/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\\nB/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\\nL/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\\n\\nL/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4\\n\\nE\\nffi\\n\\nci\\nen\\n\\ntN\\net\\n\\nB0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\\nB1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\\nB2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\\nB3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\\nB4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\\nB5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\\nB6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\\nB7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\\nB8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3\\n\\nE\\nffi\\n\\nci\\nen\\n\\ntN\\net\\n\\nN\\noi\\n\\nsy\\nSt\\n\\nud\\nen\\n\\nt B0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\\nB1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\\nB2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\\nB3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\\nB4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\\nB5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\\nB6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\\nB7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\\n\\nL2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\\nL2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4\\n\\nIn\\nst\\n\\nag\\nra\\n\\nm\\n\\n32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\\n32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\\n32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\\n32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\\n\\nFixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\\nFixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0\\n\\nB\\niT\\n\\n-S\\n\\nR50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\\nR50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\\n\\nR101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\\nR101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\\nR152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\\nR152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5\\n\\nB\\niT\\n\\n-M\\n\\nR50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\\nR50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\\n\\nR101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\\nR101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\\nR152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\\nR152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5\\n\\nV\\niT\\n\\nB/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\\nB/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\\nL/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\\nH/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9\\n\\nSi\\nm\\n\\nC\\nL\\n\\nR\\nv2\\n\\nR50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\\nR50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\\n\\nR101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\\nR101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\\nR152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\\nR152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\\nR152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0\\n\\nB\\nY\\n\\nO\\nL 50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\\n\\n200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2\\n\\nM\\noC\\n\\no v1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\\nv2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1\\n\\nVirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7\\n\\nR\\nes\\n\\nN\\net 50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\\n\\n101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\\n152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1\\n\\nTable 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson confidence\\ninterval of each dataset’s top score are shown in bold.\\n\\n?We updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 41\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\nac\\n\\ncu\\nra\\n\\ncy\\nFood101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n100 101 102\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n100 101 102\\n\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n100 101 102\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n100 101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n100 101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n100 101 102\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\n\\n88\\n\\n89\\n\\n90\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes\\n\\nPascalVOC2007\\n\\n100 101 102\\n\\n72\\n\\n74\\n\\n76\\n\\n78\\n\\n80\\n\\n82\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n100 101 102\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n100 101 102\\n\\n90\\n\\n91\\n\\n92\\n\\n93\\n\\n94\\n\\n95\\n\\n96\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\n100\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n100 101 102\\n\\n97.25\\n\\n97.50\\n\\n97.75\\n\\n98.00\\n\\n98.25\\n\\n98.50\\n\\n98.75\\n\\n99.00\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n100 101 102\\n\\n55.0\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n100 101 102\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\n98.5\\n\\n99.0\\n\\n99.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n100 101 102\\n\\n95.5\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n100 101 102\\n82\\n\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n100 101 102\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n100 101 102\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n100 101 102\\n\\n81\\n\\n82\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\nac\\n\\ncu\\nra\\n\\ncy\\nPatchCamelyon\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n100 101 102\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n100 101 102\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\n77.5\\n\\n80.0\\n\\n82.5\\n\\n85.0\\n\\n87.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 42\\n\\ncorrect label: red and white triangle with exclamation mark warning\\n\\n0 20 40 60 80 100\\n\\na zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.\\n\\na zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.\\n\\ncorrect rank: 1/43    correct probability: 45.75%\\nGerman Traffic Sign Recognition Benchmark (GTSRB)\\n\\ncorrect label: positive\\n\\n0 20 40 60 80 100\\n\\na positive review of a movie.\\n\\na negative review of a movie.\\n\\ncorrect rank: 1/2    correct probability: 78.21%\\nStanford Sentiment Treebank\\n\\ncorrect label: meme\\n\\n0 20 40 60 80 100\\n\\na meme.\\n\\na hatespeech meme.\\n\\ncorrect rank: 1/2    correct probability: 99.20%\\nHateful Memes\\n\\ncorrect label: barn\\n\\n0 20 40 60 80 100\\n\\na photo of a barn.\\n\\na photo of a church.\\n\\na photo of a threshing machine.\\n\\na photo of a sawmill.\\n\\na photo of a prison.\\n\\ncorrect rank: 1/1000    correct probability: 79.56%\\nImageNet Sketch\\n\\ncorrect label(s): antelope\\n\\n0 20 40 60 80 100\\n\\na photo of a antelope.\\n\\na photo of a zebra.\\n\\na photo of a car.\\n\\na photo of a cattle.\\n\\na photo of a elephant.\\n\\ncorrect rank: 1/30    correct probability: 99.77%\\nImageNet Vid\\n\\ncorrect label: 158\\n\\n0 20 40 60 80 100\\n\\na street sign of the number: \"1157\".\\n\\na street sign of the number: \"1165\".\\n\\na street sign of the number: \"1164\".\\n\\na street sign of the number: \"1155\".\\n\\na street sign of the number: \"1364\".\\n\\ncorrect rank: 83/2000    correct probability: 0.27%\\nStreet View House Numbers (SVHN)\\n\\ncorrect label: 7\\n\\n0 20 40 60 80 100\\n\\na photo of the number: \"7\".\\n\\na photo of the number: \"2\".\\n\\na photo of the number: \"1\".\\n\\na photo of the number: \"6\".\\n\\na photo of the number: \"4\".\\n\\ncorrect rank: 1/10    correct probability: 85.32%\\nMNIST\\n\\ncorrect label(s): motorcycle\\n\\n0 20 40 60 80 100\\n\\na photo of a motorcycle.\\n\\na photo of a bicycle.\\n\\na photo of a car.\\n\\na photo of a horse.\\n\\na photo of a dining table.\\n\\ncorrect rank: 1/20    correct probability: 99.69%\\nPASCAL VOC 2007\\n\\ncorrect label: perforated\\n\\n0 20 40 60 80 100\\n\\na photo of a polka-dotted texture.\\n\\na photo of a perforated texture.\\n\\na photo of a dotted texture.\\n\\na photo of a studded texture.\\n\\na photo of a freckled texture.\\n\\ncorrect rank: 2/47    correct probability: 20.50%\\nDescribable Textures Dataset (DTD)\\n\\ncorrect label: marimba\\n\\n0 20 40 60 80 100\\n\\na photo of a marimba.\\n\\na photo of a abacus.\\n\\na photo of a steel drum.\\n\\na photo of a computer keyboard.\\n\\na photo of a pool table.\\n\\ncorrect rank: 1/1000    correct probability: 79.54%\\nImageNet Blurry\\n\\ncorrect label: Pill bottle\\n\\n0 20 40 60 80 100\\n\\na photo of a pill bottle.\\n\\na photo of a bottle cap.\\n\\na photo of a beer bottle.\\n\\na photo of a pillow.\\n\\na photo of a wine bottle.\\n\\ncorrect rank: 1/113    correct probability: 98.34%\\nObjectNet ImageNet Overlap\\n\\ncorrect label: building\\n\\n0 20 40 60 80 100\\n\\na photo of a building.\\n\\na photo of a carriage.\\n\\na photo of a statue.\\n\\na photo of a bag.\\n\\na photo of a mug.\\n\\ncorrect rank: 1/12    correct probability: 97.69%\\naYahoo\\n\\ncorrect label: Black chinned Hummingbird\\n\\n0 20 40 60 80 100\\n\\na photo of a broad tailed hummingbird, a type of bird.\\n\\na photo of a calliope hummingbird, a type of bird.\\n\\na photo of a costas hummingbird, a type of bird.\\n\\na photo of a black chinned hummingbird, a type of bird.\\n\\na photo of a annas hummingbird, a type of bird.\\n\\ncorrect rank: 4/500    correct probability: 12.00%\\nBirdsnap\\n\\ncorrect label: King Charles Spaniel\\n\\n0 20 40 60 80 100\\n\\na photo of a king charles spaniel.\\n\\na photo of a brittany dog.\\n\\na photo of a cocker spaniel.\\n\\na photo of a papillon.\\n\\na photo of a sussex spaniel.\\n\\ncorrect rank: 1/1000    correct probability: 91.61%\\nImageNet\\n\\ncorrect label: great masterwort\\n\\n0 20 40 60 80 100\\n\\na photo of a great masterwort, a type of flower.\\n\\na photo of a bishop of llandaff, a type of flower.\\n\\na photo of a pincushion flower, a type of flower.\\n\\na photo of a globe flower, a type of flower.\\n\\na photo of a prince of wales feathers, a type of flower.\\n\\ncorrect rank: 1/102    correct probability: 74.25%\\nFlowers-102\\n\\ncorrect label: country line dancing\\n\\n0 20 40 60 80 100\\n\\na photo of country line dancing.\\n\\na photo of square dancing.\\n\\na photo of swing dancing.\\n\\na photo of dancing charleston.\\n\\na photo of salsa dancing.\\n\\ncorrect rank: 1/700    correct probability: 98.98%\\nKinetics-700\\n\\ncorrect label: kennel indoor\\n\\n0 20 40 60 80 100\\n\\na photo of a kennel indoor.\\n\\na photo of a kennel outdoor.\\n\\na photo of a jail cell.\\n\\na photo of a jail indoor.\\n\\na photo of a veterinarians office.\\n\\ncorrect rank: 1/723    correct probability: 98.63%\\nSUN\\n\\ncorrect label: 2012 Honda Accord Coupe\\n\\n0 20 40 60 80 100\\n\\na photo of a 2012 honda accord coupe.\\n\\na photo of a 2012 honda accord sedan.\\n\\na photo of a 2012 acura tl sedan.\\n\\na photo of a 2012 acura tsx sedan.\\n\\na photo of a 2008 acura tl type-s.\\n\\ncorrect rank: 1/196    correct probability: 63.30%\\nStanford Cars\\n\\ncorrect label: roundabout\\n\\n0 20 40 60 80 100\\n\\nsatellite imagery of roundabout.\\n\\nsatellite imagery of intersection.\\n\\nsatellite imagery of church.\\n\\nsatellite imagery of medium residential.\\n\\nsatellite imagery of chaparral.\\n\\ncorrect rank: 1/45    correct probability: 96.39%\\nRESISC45\\n\\ncorrect label: Belize\\n\\n0 20 40 60 80 100\\n\\na photo i took in french guiana.\\n\\na photo i took in gabon.\\n\\na photo i took in cambodia.\\n\\na photo i took in guyana.\\n\\na photo i took in belize.\\n\\ncorrect rank: 5/211    correct probability: 3.92%\\nCountry211\\n\\ncorrect label: Boeing 717\\n\\n0 20 40 60 80 100\\n\\na photo of a mcdonnell douglas md-90, a type of aircraft.\\n\\na photo of a boeing 717, a type of aircraft.\\n\\na photo of a fokker 100, a type of aircraft.\\n\\na photo of a mcdonnell douglas dc-9-30, a type of aircraft.\\n\\na photo of a boeing 727-200, a type of aircraft.\\n\\ncorrect rank: 2/100    correct probability: 9.91%\\nFGVC Aircraft\\n\\ncorrect label: beer bottle\\n\\n0 20 40 60 80 100\\n\\na photo of a beer bottle.\\n\\na photo of a pirate ship.\\n\\na photo of a chocolate syrup.\\n\\na photo of a product packet / packaging.\\n\\na photo of a wine bottle.\\n\\ncorrect rank: 1/1000    correct probability: 88.27%\\nImageNetV2 Matched Frequency\\n\\ncorrect label: snake\\n\\n0 20 40 60 80 100\\n\\na photo of a snake.\\n\\na photo of a sweet pepper.\\n\\na photo of a flatfish.\\n\\na photo of a turtle.\\n\\na photo of a lizard.\\n\\ncorrect rank: 1/100    correct probability: 38.02%\\nCIFAR-100\\n\\ncorrect label: Maine Coon\\n\\n0 20 40 60 80 100\\n\\na photo of a maine coon, a type of pet.\\n\\na photo of a persian, a type of pet.\\n\\na photo of a ragdoll, a type of pet.\\n\\na photo of a birman, a type of pet.\\n\\na photo of a siamese, a type of pet.\\n\\ncorrect rank: 1/37    correct probability: 99.99%\\nOxford-IIIT Pets\\n\\ncorrect label: Siberian Husky\\n\\n0 20 40 60 80 100\\n\\na photo of a siberian husky.\\n\\na photo of a german shepherd dog.\\n\\na photo of a collie.\\n\\na photo of a border collie.\\n\\na photo of a rottweiler.\\n\\ncorrect rank: 1/200    correct probability: 76.02%\\nImageNet-R (Rendition)\\n\\ncorrect label: kangaroo\\n\\n0 20 40 60 80 100\\n\\na photo of a kangaroo.\\n\\na photo of a gerenuk.\\n\\na photo of a emu.\\n\\na photo of a wild cat.\\n\\na photo of a scorpion.\\n\\ncorrect rank: 1/102    correct probability: 99.81%\\nCaltech-101\\n\\ncorrect label: Volleyball Spiking\\n\\n0 20 40 60 80 100\\n\\na photo of a person volleyball spiking.\\n\\na photo of a person jump rope.\\n\\na photo of a person long jump.\\n\\na photo of a person soccer penalty.\\n\\na photo of a person table tennis shot.\\n\\ncorrect rank: 1/101    correct probability: 99.30%\\nUCF101\\n\\ncorrect label: angry\\n\\n0 20 40 60 80 100\\n\\na photo of a happy looking face.\\n\\na photo of a neutral looking face.\\n\\na photo of a surprised looking face.\\n\\na photo of a fearful looking face.\\n\\na photo of a angry looking face.\\n\\ncorrect rank: 5/7    correct probability: 8.16%\\nFacial Emotion Recognition 2013 (FER2013)\\n\\ncorrect label: 4\\n\\n0 20 40 60 80 100\\n\\na photo of 3 objects.\\n\\na photo of 4 objects.\\n\\na photo of 5 objects.\\n\\na photo of 6 objects.\\n\\na photo of 10 objects.\\n\\ncorrect rank: 2/8    correct probability: 17.11%\\nCLEVR Count\\n\\ncorrect label: bird\\n\\n0 20 40 60 80 100\\n\\na photo of a bird.\\n\\na photo of a cat.\\n\\na photo of a deer.\\n\\na photo of a frog.\\n\\na photo of a dog.\\n\\ncorrect rank: 1/10    correct probability: 40.86%\\nCIFAR-10\\n\\ncorrect label: lynx\\n\\n0 20 40 60 80 100\\n\\na photo of a fox squirrel.\\n\\na photo of a mongoose.\\n\\na photo of a skunk.\\n\\na photo of a red fox.\\n\\na photo of a lynx.\\n\\ncorrect rank: 5/200    correct probability: 4.18%\\nImageNet-A (Adversarial)\\n\\ncorrect label: healthy lymph node tissue\\n\\n0 20 40 60 80 100\\n\\nthis is a photo of lymph node tumor tissue\\n\\nthis is a photo of healthy lymph node tissue\\n\\ncorrect rank: 2/2    correct probability: 22.81%\\nPatchCamelyon (PCam)\\n\\ncorrect label: annual crop land\\n\\n0 20 40 60 80 100\\n\\na centered satellite photo of permanent crop land.\\n\\na centered satellite photo of pasture land.\\n\\na centered satellite photo of highway or road.\\n\\na centered satellite photo of annual crop land.\\n\\na centered satellite photo of brushland or shrubland.\\n\\ncorrect rank: 4/10    correct probability: 12.90%\\nEuroSAT\\n\\ncorrect label(s): airplane,person\\n\\n0 20 40 60 80 100\\n\\na photo of a airplane.\\n\\na photo of a bird.\\n\\na photo of a bear.\\n\\na photo of a giraffe.\\n\\na photo of a car.\\n\\ncorrect rank: 1/23    correct probability: 88.98%\\nYoutube-BB\\n\\ncorrect label: television studio\\n\\n0 20 40 60 80 100\\n\\na photo of a television studio.\\n\\na photo of a podium indoor.\\n\\na photo of a conference room.\\n\\na photo of a lecture room.\\n\\na photo of a control room.\\n\\ncorrect rank: 1/397    correct probability: 90.22%\\nSUN397\\n\\ncorrect label: guacamole\\n\\n0 20 40 60 80 100\\n\\na photo of guacamole, a type of food.\\n\\na photo of ceviche, a type of food.\\n\\na photo of edamame, a type of food.\\n\\na photo of tuna tartare, a type of food.\\n\\na photo of hummus, a type of food.\\n\\ncorrect rank: 1/101    correct probability: 90.15%\\nFood101\\n\\nFigure 21. Visualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting\\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\\nthe class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect\\nprediction is colored orange.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 43\\n\\nFo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nSt\\nan\\n\\nfo\\nrd\\n\\nC\\nar\\n\\ns\\n\\nFG\\nV\\n\\nC\\nA\\n\\nir\\ncr\\n\\naf\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nO\\nxf\\n\\nor\\nd\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns1\\n\\n02\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nam\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nR\\nen\\n\\nde\\nre\\n\\nd\\nSS\\n\\nT\\n2\\n\\nIm\\nag\\n\\neN\\net\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nes\\nN\\n\\net RN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\\nRN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\\n\\nRN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\\nRN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\\nRN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6\\n\\nC\\nL\\n\\nIP\\n-V\\n\\niT B/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\\nB/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\\nL/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\\n\\nL/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\\n\\nTable 11. Zero-shot performance of CLIP models over 27 datasets.\\n\\n101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFood101\\n\\n101 102\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n101 102\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n101 102\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n101 102\\n\\n60\\n\\n62\\n\\n64\\n\\n66\\n\\n68\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n101 102\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n101 102\\n\\n20\\n\\n30\\n\\n40\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n101 102\\n82.0\\n\\n82.5\\n\\n83.0\\n\\n83.5\\n\\n84.0\\n\\n84.5\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes PascalVOC2007\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n101 102\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n101 102\\n82\\n84\\n86\\n88\\n90\\n92\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n101 102\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n101 102\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n101 102\\n\\n95\\n\\n96\\n\\n97\\n\\n98\\n\\n99\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n101 102\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n101 102\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n101 102\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n101 102\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nPatchCamelyon\\n\\n101 102\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n101 102\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n101 102\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n101 102\\n\\nGFLOPs/image\\n\\n54\\n\\n56\\n\\n58\\n\\n60\\n\\n62\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n101 102\\n\\nGFLOPs/image\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nResNet\\n\\nFigure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 44\\n\\nB. Zero-Shot Prediction\\nTo provide a qualitative summary / overview of CLIP’s zero-\\nshot performance we visualize a randomly selected predic-\\ntion for 36 different zero-shot CLIP classifiers in Figure\\n21. In addition, Table 11 and Figure 22 show the individual\\nzero-shot performance scores for each dataset.\\n\\nC. Duplicate Detector\\nOur early attempts at duplicate detection and analysis used\\nnearest neighbors in the model’s learned embedding space.\\nWhile it is intuitive to use a model’s own notion of similar-\\nity, we encountered issues. We found the model’s feature\\nspace is weighted very heavily towards semantic similar-\\nity. Many false positives occurred due to distinct objects\\nthat would be described similarly (soccer balls, flowers of\\nthe same species, etc...) having almost perfect similarity.\\nWe also observed the model was quite poor at assigning\\ncertain kinds of near-duplicates high similarity scores. We\\nnoticed repeatedly that images with high-frequency textures\\n(such as fur or stripe patterns) pre-processed by different\\nresizing algorithms (nearest neighbor vs bi-linear) could\\nhave surprisingly low similarity. This resulted in many false\\nnegatives.\\n\\nWe built our own near-duplicate detector to fix this issue.\\nWe created a synthetic data augmentation pipeline that com-\\nbined a variety of common image manipulations. The aug-\\nmentation pipeline combines random cropping and zooming,\\naspect ratio distortion, downsizing and upscaling to different\\nresolutions, minor rotations, jpeg compression, and HSV\\ncolor jitter. The pipeline also randomly selects from differ-\\nent interpolation algorithms for all relevant steps. We then\\ntrained a model to maximize the similarity of an image and\\nits transformed variant while minimizing similarity to all\\nother images in a training batch. We used the same n-pair /\\nInfoNCE loss as CLIP but with a fixed temperature of 0.07.\\n\\nWe selected a ResNet-50 as the model architecture. We\\nmodified the base ResNet-50 with the anti-alias improve-\\nments from (Zhang, 2019) and used weight norm (Sali-\\nmans & Kingma, 2016) instead of batch norm (Ioffe &\\nSzegedy, 2015) to avoid leaking information about dupli-\\ncates via batch statistics - a problem previously noted in\\n(Henaff, 2020). We also found the GELU activation func-\\ntion (Hendrycks & Gimpel, 2016) to perform better for this\\ntask. We trained the model with a total batch size of 1,712\\nfor approximately 30 million images sampled from our pre-\\ntraining dataset. At the end of training it achieves nearly\\n100% accuracy on its proxy training task.\\n\\nLinear Classifier Zero Shot\\nDataset YFCC WIT ∆ YFCC WIT ∆\\n\\nBirdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\\nCountry211 23.1 17.3 +5.8 5.2 5.3 +0.1\\nFlowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\\nGTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1\\nUCF101 69.2 74.9 −5.7 22.9 32.0 −9.1\\nStanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1\\n\\nImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7\\nDataset Average 65.5 66.6 −1.1 29.6 30.0 −0.4\\nDataset “Wins” 10 15 −5 19 18 +1\\n\\nTable 12. CLIP performs similarly when trained on only\\nYFCC100M. Comparing a ResNet-50 trained on only\\nYFCC100M with a same sized subset of WIT shows simi-\\nlar average performance and number of wins on zero shot and\\nlinear classifier evals. However, large differences in dataset\\nspecific performance occur. We include performance on the 3\\ndatasets where YFCC does best and worst compared to WIT\\naccording to a linear probe in order to highlight this as well as\\naggregate performance across all linear and zero-shot evals and\\nthe canonical ImageNet dataset.\\n\\nD. Dataset Ablation on YFCC100M\\nTo study whether our custom dataset is critical to the perfor-\\nmance of CLIP, we trained a model on a filtered subset of\\nthe YFCC100M dataset (details described in Section 2.2)\\nand compared its performance to the same model trained\\non an equally sized subset of WIT. We train each model for\\n32 epochs at which point transfer performance begins to\\nplateau due to overfitting. Results are shown in Table 12.\\nAcross our whole eval suite, YFCC and WIT perform simi-\\nlarly on average for both zero-shot and linear probe settings.\\nHowever, performance on specific fine-grained classifica-\\ntion datasets can vary widely - sometimes by over 10%.\\nOur speculation is that these differences in performance re-\\nflect the relative density of relevant data in each pre-training\\ndataset. For instance, pre-training on YFCC100M, which\\nmight contain many photos of birds and flowers (common\\nsubjects for photographers), results in better performance on\\nBirdsnap and Flowers102, while pre-training on WIT results\\nin better car and pet classifiers (which appear common in\\nour dataset).\\n\\nOverall, these results are encouraging as they suggest our\\napproach can use any reasonably filtered collection of paired\\n(text, image) data. This mirrors recent work which reported\\npositive results using the same contrastive pre-training ob-\\njective on the relatively different domain of medical imaging\\n(Zhang et al., 2020). It also is similar to the findings of noisy\\nstudent self-training which reported only slight improve-\\nments when using their JFT300M dataset over YFCC100M\\n(Xie et al., 2020). We suspect the major advantage of our\\ndataset over the already existing YFCC100M is its much\\nlarger size.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 45\\n\\nFinally, we caution that WIT includes this filtered subset\\nof YFCC100M. This could result in our ablation under-\\nestimating the size of performance differences between\\nYFCC100M and the rest of WIT. We do not think this is\\nlikely as YFCC100M is only 3.7% of the overall WIT data\\nblend and it did not noticeably change the performance of\\nmodels when it was added to the existing data blend during\\nthe creation of WIT.\\n\\nE. Selected Task and Dataset Results\\nDue to the large variety of datasets and experiments consid-\\nered in this work, the main body focuses on summarizing\\nand analyzing overall results. In the following subsections\\nwe report details of performance for specific groups of tasks,\\ndatasets, and evaluation settings.\\n\\nE.1. Image and Text Retrieval\\n\\nCLIP pre-trains for the task of image-text retrieval on our\\nnoisy web-scale dataset. Although the focus of this paper\\nis on representation learning and task learning for the pur-\\npose of transfer to a wide variety of downstream datasets,\\nvalidating that CLIP is able to achieve high transfer perfor-\\nmance transfer on exactly what it is pre-trained for is an\\nimportant sanity check / proof of concept. In Table 13 we\\ncheck the zero-shot transfer performance of CLIP for both\\ntext and image retrieval on the Flickr30k and MSCOCO\\ndatsets. Zero-shot CLIP matches or outperforms all prior\\nzero-shot results on these two datasets. Zero-shot CLIP is\\nalso competitive with the current overall SOTA for the task\\nof text retrieval on Flickr30k. On image retrieval, CLIP’s\\nperformance relative to the overall state of the art is notice-\\nably lower. However, zero-shot CLIP is still competitive\\nwith a fine-tuned Unicoder-VL. On the larger MS-COCO\\ndataset fine-tuning improves performance significantly and\\nzero-shot CLIP is not competitive with the most recent work.\\nFor both these datasets we prepend the prompt “a photo\\nof” to the description of each image which we found boosts\\nCLIP’s zero-shot R@1 performance between 1 and 2 points.\\n\\nE.2. Optical Character Recognition\\n\\nAlthough visualizations have shown that ImageNet models\\ncontain features that respond to the presence of text in an\\nimage (Zeiler & Fergus, 2014), these representations are\\nnot sufficiently fine-grained to use for the task of optical\\ncharacter recognition (OCR). To compensate, models are\\naugmented with the outputs of custom OCR engines and\\nfeatures to boost performance on tasks where this capability\\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\\ning the development of CLIP, we noticed that CLIP began to\\nlearn primitive OCR capabilities which appeared to steadily\\nimprove over the course of the project. To evaluate this\\nqualitatively noticed behavior, we measured performance\\n\\non 5 datasets requiring the direct and indirect use of OCR.\\nThree of these datasets MNIST (LeCun), SVHN (Netzer\\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\\nthe ability of a model to perform low-level character and\\nword recognition, while Hateful Memes (Kiela et al., 2020)\\nand SST-2 (Socher et al., 2013) check the ability of a model\\nto use OCR to perform a semantic task. Results are reported\\nin Table 14.\\n\\nCLIP’s performance is still highly variable and appears to\\nbe sensitive to some combination of the domain (rendered or\\nnatural images) and the type of text to be recognized (num-\\nbers or words). CLIP’s OCR performance is strongest Hate-\\nful Memes and SST-2 - datasets where the text is digitally\\nrendered and consists mostly of words. On IIIT5K, which\\nis natural images of individually cropped words, zero-shot\\nCLIP performs a bit more respectively and its performance\\nis similar to Jaderberg et al. (2014) early work combining\\ndeep learning and structured prediction to perform open-\\nvocabulary OCR. However, performance is noticeably lower\\non two datasets involving recognition of hand written and\\nstreet view numbers. CLIP’s 51% accuracy on full number\\nSVHN is well below any published results. Inspection sug-\\ngests CLIP struggles with repeated characters as well as the\\nlow resolution and blurry images of SVHN. CLIP’s zero-\\nshot MNIST performance is also poor and is outperformed\\nby supervised logistic regression on raw pixels, one of the\\nsimplest possible machine learning baselines.\\n\\nSST-2 is a sentence level NLP dataset which we render into\\nimages. We include SST-2 in order to check whether CLIP\\nis able to convert low level OCR capability into a higher\\nlevel representation. Fitting a linear classifier on CLIP’s rep-\\nresentation of rendered sentences achives 80.5% accuracy.\\nThis is on par with the 80% accuracy of a continuous bag\\nof words baseline using GloVe word vectors pre-trained on\\n840 billion tokens (Pennington et al., 2014). While this is a\\nsimple NLP baseline by today’s standard, and well below\\nthe 97.5% of the current SOTA, it is encouraging to see\\nthat CLIP is able to turn an image of rendered text into a\\nnon-trivial sentence level representation. Fully supervised\\nCLIP is also surprisingly strong on Hateful Meme detec-\\ntion, where CLIP is only 0.7 points behind the current single\\nmodel SOTA and several points above the best baseline from\\nthe original paper. Similar to SST-2, these other results on\\nHateful Memes use the ground truth text which CLIP does\\nnot have access to. Finally, we note that zero-shot CLIP\\noutperforms the best results using fully supervised linear\\nprobes across all other 56 models included in our evaluation\\nsuite. This suggests CLIP’s OCR capability is at least some-\\nwhat unique compared to existing work on self-supervised\\nand supervised representation learning.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 46\\n\\nText Retrieval Image Retrieval\\nFlickr30k MSCOCO Flickr30k MSCOCO\\n\\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\\n\\nFi\\nne\\n\\ntu\\nne\\n\\nUnicoder-VLa 86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\\nUniterb 87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\\nVILLAc 87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\\nOscard - - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\\nERNIE-ViLe 88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -\\n\\nZ\\ner\\n\\no-\\nSh\\n\\not Visual N-Gramsf 15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\\nImageBERTg - - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\\nUnicoder-VLa 64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - -\\nUniterb 83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\\nCLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2\\n\\nTable 13. CLIP improves zero-shot retrieval and is competitive with the best fine-tuned result on Flickr30k text retrieval. Bold\\nindicates best overall performance while an underline indicates best in category performance (zero-shot or fine-tuned). For all other\\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\\na(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f (Li et al., 2017) g(Qi et al., 2020)\\n\\nIIIT5K Hateful\\nMNIST SVHN 1k Memes SST-2\\n\\nFi\\nne\\n\\ntu\\nne SOTA 99.8a 96.4b 98.9c 78.0d 97.5e\\n\\nJOINTf - - 89.6 - -\\nCBoWg - - - - 80.0\\n\\nL\\nin\\n\\nea\\nr Raw Pixels 92.5 - - - -\\n\\nES Best 98.9h - - 58.6h 59.0i\\n\\nCLIP 99.2 - - 77.3 80.5\\n\\nZ\\nS\\n\\nCLIP 88.4 51.0 90.0 63.3 67.9\\n\\nTable 14. OCR performance on 5 datasets. All metrics are accuracy\\non the test set except for Hateful Memes which reports ROC AUC\\non the dev set. Single model SOTA reported to best of knowledge.\\nES Best reports the best performance across the 56 non-CLIP\\nmodels in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al.,\\n2015) c(Wang et al., 2020) d(Lippe et al., 2020) f (Jaderberg et al.,\\n2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\\n2018)\\n\\nE.3. Action Recognition in Videos\\n\\nFor the purpose of learning, a potentially important aspect\\nof natural language is its ability to express, and therefore su-\\npervise, an extremely wide set of concepts. A CLIP model,\\nsince it is trained to pair semi-arbitrary text with images, is\\nlikely to receive supervision for a wide range of visual con-\\ncepts involving both common and proper nouns, verbs, and\\nadjectives. ImageNet-1K, by contrast, only labels common\\nnouns. Does the lack of broader supervision in ImageNet\\nresult in weaker transfer of ImageNet models to tasks involv-\\ning the recognition of visual concepts that are not nouns?\\n\\nTo investigate this, we measure and compare the perfor-\\nmance of CLIP and ImageNet models on several video\\n\\nUCF101 K700 RareAct\\nTop-1 AVG mWAP mWSAP\\n\\nFi\\nne\\n\\ntu\\nne R(2+1)D-BERTa 98.7 - - -\\n\\nNS ENet-L2b - 84.8 - -\\nHT100M S3Dd 91.3 - - -\\nBaseline I3De - 70.2 - -\\n\\nL\\nin\\n\\nea\\nr MMV FACf 91.8 - - -\\n\\nNS ENet-L2c 89.4c 68.2c - -\\nCLIP 92.0 73.0 - -\\n\\nZ\\nS HT100M S3Dd - - 30.5 34.8\\n\\nCLIP 80.3 69.6 40.7 44.8\\n\\nTable 15. Action recognition performance on 3 video datasets. Sin-\\ngle model SOTA reported to best of knowledge. Note that linear\\nCLIP and linear NS ENet-L2 are trained and evaluated on a single\\nframe subsampled version of each dataset and not directly compa-\\nrable to prior work. On Kinetics-700, we report the ActivityNet\\ncompetition metric which is the average of top-1 and top-5 per-\\nformance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al.,\\n2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f (Alayrac\\net al., 2020)\\n\\naction classification datasets which measure the ability of a\\nmodel to recognize verbs. In Table 15 we report results on\\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\\net al., 2019), two common datasets for the task. Unfortu-\\nnately, our CPU based linear classifier takes a prohibitively\\nlong time to evaluate on a video dataset due to the very large\\nnumber of training frames. To deal with this, we aggres-\\nsively sub-sample each video to only a single center frame,\\neffectively turning it into an image classification dataset.\\nAs a result, our reported performance in a linear evaluation\\nsetting likely under estimates performance by a moderate\\namount.\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 47\\n\\nIN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\\nTop-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10\\n\\nNS EfficientNet-L2a 88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\\nFixResNeXt101-32x48d V2b 86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\\nLinear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\\nZero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5\\n\\nTable 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)\\n\\nDespite this handicap, CLIP features transfer surprisingly\\nwell to this task. CLIP matches the best prior result on UCF-\\n101 in a linear probe evaluation setting and also outperforms\\nall other models in our evaluation suite. On Kinetics-700,\\nCLIP also outperforms the fine-tuned I3D baseline from the\\noriginal paper. Since it does not require a training stage,\\nwe report CLIP’s zero-shot performance when averaging\\npredictions across all frames. CLIP also performs well in\\nthis setting and on Kinetics-700 its performance is within\\n1% of the fully supervised I3D baseline which is trained\\non 545000 labeled videos. Encouraged by these results, we\\nalso measure CLIP’s performance on the recently introduced\\nRareAct dataset (Miech et al., 2020a) which was designed\\nto measure zero-shot recognition of unusual actions like\\n“hammering a phone” and “drilling an egg”. CLIP improves\\nover the prior state of the art, a S3D model trained on auto-\\nmatically extracted captions from 100 million instructional\\nvideos, by 10 points.\\n\\nWhile CLIP has encouragingly strong performance on the\\ntask of action recognition, we note that there are many differ-\\nences between the models being compared beyond just their\\nform of supervision such as model architecture, training\\ndata distribution, dataset size, and compute used. Further\\nwork is needed to more precisely determine what specific\\ndesign decisions contribute to achieving high performance\\non this task.\\n\\n1km 25km 200km 750km 2500km\\n\\nISNsa 16.9 43.0 51.9 66.7 80.2\\nCPlaNetb 16.5 37.1 46.4 62.0 78.5\\nCLIP 13.9 32.9 43.0 62.0 79.3\\nDeep-Ret+c 14.4 33.3 47.7 61.6 73.4\\nPlaNetd 8.4 24.5 37.6 53.6 71.3\\n\\nTable 17. Geolocalization performance on the IM2GPS test set.\\nMetric is percent of images localized within a given radius. Models\\nare ordered by average performance. a(Muller-Budack et al., 2018)\\nb(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al.,\\n2016)\\n\\nE.4. Geolocalization\\n\\nAnother behavior we noticed during the development of\\nCLIP was its ability to recognize many places and locations.\\nTo quantify this we created the Country211 dataset as de-\\nscribed in Appendix A and report results on it throughout\\nthe paper. However it is a new benchmark so to compare\\nwith prior work on geolocalization we also report results\\non the IM2GPS test set from Hays & Efros (2008) in Table\\n17. Since IM2GPS is a regression benchmark, we guess the\\nGPS coordinates of the nearest image in a set of reference\\nimages using CLIP’s embedding space. This is not a zero-\\nshot result since it uses nearest-neighbor regression. Despite\\nquerying only 1 million images, which is much less than\\nprior work, CLIP performs similarly to several task specific\\nmodels. It is not, however, competitive with the current state\\nof the art.\\n\\nE.5. Robustness to Distribution Shift\\n\\nSection 3.3 provides a high level summary and analysis of\\nImageNet-related robustness results. We briefly provide\\nsome additional numerical details in this appendix. Per-\\nformance results per dataset are provided in Table 16 and\\ncompared with the current state of the art results reported\\nin Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\\nBB. CLIP’s improvements are largest on ImageNet-Vid and\\nYoutube-BB due to its flexible zero-shot capability and on\\nImageNet-R, which likely reflects CLIP’s pre-training dis-\\ntribution including significant amounts of creative content.\\nA similar behavior has been documented for the Instagram\\npre-trained ResNeXt models as discussed in Taori et al.\\n(2020).\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 48\\n\\nF. Model Hyperparameters\\n\\nHyperparameter Value\\n\\nBatch size 32768\\nVocabulary size 49408\\nTraining epochs 32\\nMaximum temperature 100.0\\nWeight decay 0.2\\nWarm-up iterations 2000\\nAdam β1 0.9\\nAdam β2 0.999 (ResNet), 0.98 (ViT)\\nAdam ε 10−8 (ResNet), 10−6 (ViT)\\n\\nTable 18. Common CLIP hyperparameters\\n\\nLearning Embedding Input ResNet Text Transformer\\nModel rate dimension resolution blocks width layers width heads\\n\\nRN50 5× 10−4 1024 224 (3, 4, 6, 3) 2048 12 512 8\\nRN101 5× 10−4 512 224 (3, 4, 23, 3) 2048 12 512 8\\nRN50x4 5× 10−4 640 288 (4, 6, 10, 6) 2560 12 640 10\\nRN50x16 4× 10−4 768 384 (6, 8, 18, 8) 3072 12 768 12\\nRN50x64 3.6× 10−4 1024 448 (3, 15, 36, 10) 4096 12 1024 16\\n\\nTable 19. CLIP-ResNet hyperparameters\\n\\nLearning Embedding Input Vision Transformer Text Transformer\\nModel rate dimension resolution layers width heads layers width heads\\n\\nViT-B/32 5× 10−4 512 224 12 768 12 12 512 8\\nViT-B/16 5× 10−4 512 224 12 768 12 12 512 8\\nViT-L/14 4× 10−4 768 224 24 1024 16 12 768 12\\nViT-L/14-336px 2× 10−5 768 336 24 1024 16 12 768 12\\n\\nTable 20. CLIP-ViT hyperparameters\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.\\n\\n\\n\\n“Beware the Jabberwock, my son!\\n      \\n\\nThe jaws that bite, the claws that catch!\\nBeware the Jubjub bird, and shun\\n      The frumious Bandersnatch!”\\n\\n\\n\\nHe took his vorpal sword in hand;\\n      Long time the manxome foe he sought—\\nSo rested he by the Tumtum tree\\n      And stood awhile in thought.\\n\\n\\n\\nAnd, as in uffish thought he stood,\\n      The Jabberwock, with eyes of flame,\\nCame whiffling through the tulgey wood,\\n      And burbled as it came!\\n\\n\\n\\nOne, two!\\n\\nOne, two!\\n\\nAnd through and through\\n      The vorpal blade went snicker-snack!\\n\\n\\nHe left it dead, and with its head\\n      He went galumphing back.\\n\\n\\n\\n“And hast thou slain the Jabberwock?\\n      Come to my arms, my beamish boy!\\n\\n\\nO frabjous day!\\n\\nCallooh! Callay!”\\n      \\n\\nHe chortled in his joy.\\n\\n’Twas brillig, and the slithy toves\\n      Did gyre and gimble in the wabe:\\n\\n\\nAll mimsy were the borogoves,\\n      And the mome raths outgrabe.']\n",
      "['It was the best of times, it was the worst of times, \\nit was the age of wisdom, it was the age of foolishness, \\nit was the epoch of belief, it was the epoch of incredulity, \\nit was the season of light, it was the season of darkness, \\nit was the spring of hope, it was the winter of despair, \\nwe had everything before us, we had nothing before us, \\nwe were all going direct to heaven, \\nwe were all going direct the other way–in short, \\nthe period was so far like the present period, \\nthat some of its noisiest authorities insisted on its being received, \\nfor good or for evil, in the superlative degree of comparison only.']\n",
      "['I wish you to know that you have been the last dream of my soul. \\n\\n\\nIn my degradation I have not been so degraded but that the sight \\nof you with your father, and of this home made such a home by you, \\nhas stirred old shadows that I thought had died out of me. \\n\\n\\nSince I knew you, I have been troubled by a remorse that I \\nthought would never reproach me again, and have heard whispers \\nfrom old voices impelling me upward, that I thought were silent \\nfor ever.\\n\\nI have had unformed ideas of striving afresh, beginning anew, \\nshaking off sloth and sensuality, and fighting out the abandoned fight. \\n\\n\\nA dream, all a dream, that ends in nothing, and leaves the sleeper \\nwhere he lay down, but I wish you to know that you inspired it.']\n",
      "['A wonderful fact to reflect upon, that every human creature is \\nconstituted to be that profound secret and mystery to every other.']\n",
      "[\"The dog is a gentleman; I hope to go to his heaven not man's.\"]\n",
      "['If a man aspires towards a righteous life, his first act of abstinence is from injury to animals.']\n",
      "['Tweedledum and Tweedledee: She then meets the fat twin brothers \\nTweedledum and Tweedledee, whom she knows from the nursery rhyme. \\n\\n\\nAfter reciting the long poem \"The Walrus and the Carpenter\", \\nthey draw Alice\\'s attention to the Red King—loudly snoring away \\nunder a nearby tree—and maliciously provoke her with idle philosophical \\nbanter that she exists only as an imaginary figure in the Red King\\'s dreams. \\n\\n\\nFinally, the brothers begin suiting up for battle, only to be frightened \\naway by an enormous crow, as the nursery rhyme about them predicts.']\n",
      "['Golden retrievers are not bred to be guard dogs, and considering the size of their hearts and their irrepressible joy and life, they are less likely to bite than to bark, less likely to bark than to lick a hand in greeting.\\n\\nIn spite of their size, they think they are lap dogs, and in spite of being dogs, they think they’re also human, and nearly every human they meet is judged to have the potential to be a boon companion who might at any moment, cry, “Let’s go!” and lead them on a great adventure.']\n",
      "['If you’re lucky, a golden retriever will come into your life, steal your heart, and change everything']\n",
      "['My friend Phil has a theory that the Lord, having made teenagers, felt constrained to make amends and so created the golden retriever.']\n",
      "['If you don’t believe that dogs have souls, you haven’t looked into their eyes long enough.']\n",
      "[\"A thing of beauty is a joy for ever:\\nIts loveliness increases; it will never\\nPass into nothingness; but still will keep\\nA bower quiet for us, and a sleep\\nFull of sweet dreams, and health, and quiet breathing.\\n\\n\\nTherefore, on every morrow, are we wreathing\\nA flowery band to bind us to the earth,\\nSpite of despondence, of the inhuman dearth\\nOf noble natures, of the gloomy days,\\nOf all the unhealthy and o'er-darkn'd ways\\nMade for our searching: yes, in spite of all,\\nSome shape of beauty moves away the pall\\nFrom our dark spirits.\\n\\nSuch the sun, the moon,\\nTrees old and young, sprouting a shady boon\\nFor simple sheep; and such are daffodils\\nWith the green world they live in; and clear rills\\nThat for themselves a cooling covert make\\n'Gainst the hot season; the mid-forest brake,\\nRich with a sprinkling of fair musk-rose blooms:\\nAnd such too is the grandeur of the dooms\\nWe have imagined for the mighty dead;\\nAn endless fountain of immortal drink,\\nPouring unto us from the heaven's brink\"]\n",
      "['The dominant sequence transduction models are based on \\ncomplex recurrent or convolutional neural networks in an encoder-decoder configuration. \\n\\n\\nThe best performing models also connect the encoder and decoder through \\nan attention mechanism.\\n\\nWe propose a new simple network architecture, \\nthe Transformer, based solely on attention mechanisms, \\ndispensing with recurrence and convolutions entirely. \\n\\n\\nExperiments on two machine translation tasks show these models \\nto be superior in quality while being more parallelizable \\nand requiring significantly less time to train. \\n\\n\\nOur model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, \\nimproving over the existing best results, including ensembles by over 2 BLEU. \\n\\n\\nOn the WMT 2014 English-to-French translation task, our model establishes \\na new single-model state-of-the-art BLEU score of 41.8 after training for \\n3.5 days on eight GPUs, a small fraction of the training costs of the \\nbest models from the literature.\\n\\nWe show that the Transformer \\ngeneralizes well to other tasks by applying it successfully to \\nEnglish constituency parsing both with large and limited training data.']\n",
      "['In machine learning, backpropagation\\n(backprop,[1] BP) is a widely used\\nalgorithm for training feedforward\\nartificial neural networks.\\n\\n\\nGeneralizations of backpropagation\\nexist for other artificial neural\\nnetworks (ANNs), and for functions\\ngenerally.\\n\\nThese classes of algorithms\\nare all referred to generically as\\n\"backpropagation\".[2]\\n\\nIn fitting a\\nneural network, backpropagation\\ncomputes the gradient of the loss\\nfunction with respect to the weights of\\nthe network for a single input–output\\nexample, and does so efficiently,\\nunlike a naive direct computation of\\nthe gradient with respect to each\\nweight individually.\\n\\nThis efficiency\\nmakes it feasible to use gradient\\nmethods for training multilayer\\nnetworks, updating weights to minimize\\nloss; gradient descent, or variants\\nsuch as stochastic gradient descent,\\nare commonly used.\\n\\nThe backpropagation\\nalgorithm works by computing the\\ngradient of the loss function with\\nrespect to each weight by the chain\\nrule, computing the gradient one layer\\nat a time, iterating backward from the\\nlast layer to avoid redundant\\ncalculations of intermediate terms in\\nthe chain rule; this is an example of\\ndynamic programming.[3]']\n",
      "['She dwelt among the untrodden ways\\nBeside the springs of Dove,\\nA Maid whom there were none to praise\\nAnd very few to love:\\n\\nA violet by a mossy stone\\nHalf hidden from the eye!\\n—Fair as a star, when only one\\nIs shining in the sky.\\n\\n\\n\\nShe lived unknown, and few could know\\nWhen Lucy ceased to be;\\n\\n\\nBut she is in her grave, and, oh,\\n\\n\\nThe difference to me!']\n",
      "[\"What is this life if, full of care,\\nWe have no time to stand and stare.\\n\\n\\n\\nNo time to stand beneath the boughs\\nAnd stare as long as sheep or cows.\\n\\n\\n\\nNo time to see, when woods we pass,\\nWhere squirrels hide their nuts in grass.\\n\\n\\n\\nNo time to see, in broad daylight,\\nStreams full of stars, like skies at night.\\n\\n\\n\\nNo time to turn at Beauty's glance,\\nAnd watch her feet, how they can dance.\\n\\n\\n\\nNo time to wait till her mouth can\\nEnrich that smile her eyes began.\\n\\n\\n\\nA poor life this if, full of care,\\nWe have no time to stand and stare.\"]\n",
      "['Kapil Dev Ramlal Nikhanj (Pronunciation: [kəpiːl deːʋ] born 6 January 1959) is an Indian former cricketer.\\n\\nOne of the greatest all-rounders in the history of cricket, he was a fast-medium bowler and a hard-hitting middle-order batsman.\\n\\nDev is the only player in the history of cricket to have taken more than 400 wickets (434 wickets) and scored more than 5,000 runs in Test.[4]\\n\\n\\n\\nDev captained the Indian cricket team that won the 1983 Cricket World Cup,[5] becoming the first Indian captain to win the Cricket World Cup.\\n\\nHe is still the youngest captain (at the age of 24) to win the World Cup for any team.[6]\\n\\nHe retired in 1994, as the first player to take 200 ODI wickets,[7] and holding the world record for the highest number of wickets taken in Test cricket, a record subsequently broken by Courtney Walsh in 2000.[8]\\n\\nKapil Dev still holds the record for the highest individual score i.e. 175* scored by a batsman batting at number 5 or lower in ODIs.[9]\\n\\nAfter retiring, he coached the Indian national team between September 1999 and September 2000.[10][11]\\n\\nIn 1982, Dev was awarded the Padma Shri, and in 1991 the Padma Bhushan.\\n\\nIn 2002, he was named by Wisden as the Indian Cricketer of the Century.\\n\\nOn 11 March 2010, Dev was inducted into the ICC Cricket Hall of Fame.[12]\\n\\nIn 2013, he received the C. K. Nayudu Lifetime Achievement Award, the highest honour conferred by BCCI on a former player.[13]\\n\\nEarly life\\nKapildev Ramlal Nikhanj[14] was born in a[15] family from Punjab to his father Ram Lal Nikhanj, a teak merchant and his wife, Rajkumari in Chandigarh on 6 January 1959.\\n\\nHis family moved to Fazilka after the Partition before eventually moving to Chandigarh.\\n\\nHis paternal family is from Montgomery (now known as Sahiwal) and his mother was born in Pakpattan, Okara.[16][17][18]\\n\\nDev was a student at D.A.V.College.[19][20]']\n",
      "['Sachin Ramesh Tendulkar, (/ˌsʌtʃɪn tɛnˈduːlkər/ i; pronounced [sətɕin teːɳɖulkəɾ]; born 24 April 1973) is an Indian former international cricketer who captained the Indian national team.\\n\\nHe is widely regarded as one of the greatest batsmen in the history of cricket.[4]\\n\\nHe is the all-time highest run-scorer in both ODI and Test cricket with more than 18,000 runs and 15,000 runs, respectively.[5]\\n\\nHe also holds the record for receiving the most man-of-the-match awards in international cricket.[6] Tendulkar was a Member of Parliament, Rajya Sabha by nomination from 2012 to 2018.[7][8]\\n\\nTendulkar took up cricket at the age of eleven, made his Test match debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for over 24 years.[9]\\n\\nIn 2002, halfway through his career, Wisden ranked him the second-greatest Test batsman of all time, behind Don Bradman, and the second-greatest ODI batsman of all time, behind Viv Richards.[10] The same year, Tendulkar was a part of the team that was one of the joint-winners of the 2002 ICC Champions Trophy.\\n\\nLater in his career, Tendulkar was part of the Indian team that won the 2011 Cricket World Cup, his first win in six World Cup appearances for India.[11]\\n\\nHe had previously been named \"Player of the Tournament\" at the 2003 World Cup.\\n\\n\\n\\nTendulkar has received several awards from the government of India: the Arjuna Award (1994), the Khel Ratna Award (1997), the Padma Shri (1998), and the Padma Vibhushan (2008).[12][13] After Tendulkar played his last match in November 2013, the Prime Minister\\'s Office announced the decision to award him the Bharat Ratna, India\\'s highest civilian award.[14][15]\\n\\nHe was the first sportsperson to receive the reward and, as of 2023, is the youngest recipient.[16][17][18]\\n\\nIn 2010, Time included Tendulkar in its annual list of the most influential people in the world.[19] Tendulkar was awarded the Sir Garfield Sobers Trophy for cricketer of the year at the 2010 International Cricket Council (ICC) Awards.[20]\\n\\n\\n\\nHaving retired from ODI cricket in 2012,[21][22] he retired from all forms of cricket in November 2013 after playing his 200th Test match.[23] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[24] In 2013, Tendulkar was included in an all-time Test World XI to mark the 150th anniversary of Wisden Cricketers\\' Almanack, and he was the only specialist batsman of the post–World War II era, along with Viv Richards, to get featured in the team.[25] In 2019, he was inducted into the ICC Cricket Hall of Fame.[26]\\n\\nOn 24 April 2023, the Sydney Cricket Ground unveiled a set of gates named after Tendulkar and Brian Lara on the occasion of Tendulkar\\'s 50th birthday and the 30th anniversary of Lara\\'s inning of 277 at the ground.[27][28][29]']\n",
      "['Shubman Gill (born 8 September 1999) is an Indian cricketer.\\n\\nRepresenting Indian cricket team at the international level, he also plays for Gujarat Titans in the Indian Premier League and Punjab in domestic cricket.\\n\\nGill served as the vice-captain of the Indian Under-19 cricket team in the 2018 Under-19 Cricket World Cup and won Player of the Tournament award.\\n\\nA right-handed opening batsman, he is considered one of the best young cricketers in the world.\\n\\nHe is nicknamed \"The Prince\" for the success he achieved in his career still now.[1] Gill holds the record for youngest cricketer to score a double century in One Day International cricket.[2] and for the highest T20 score by an individual for the Indian team.\\n\\n\\n\\nHe made his List-A debut against Vidharbha[3] in 2017 and first-class debut for Punjab against Bengal in the 2017–18 Ranji Trophy, in late 2017, with a half-century in the game,[4] and 129 runs in the LAST match against Services.[5]\\n\\nHe made his international debut for the Indian cricket team in January 2019.[6]\\n\\nHe was drafted into India\\'s Under-19 side as the vice-captain for the 2018 Under-19 Cricket World Cup.\\n\\nShubman scored 372 runs at an average of 124.00 at the tournament, where he batted at number three to play a crucial role in India\\'s record fourth world title and was adjudged the edition\\'s Player of the Tournament.[7] His match-winning 102 not out in the semi-final against arch-rivals Pakistan U-19 drew praises from batting greats such as Rahul Dravid, Sachin Tendulkar, VVS Laxman, and Sourav Ganguly.[8][9]\\n\\nIn 2022, Gill was part of the IPL championship winning Gujarat Titans team.\\n\\nGill would go on to win the 2023 IPL Orange Cap, scoring 890 runs, the second highest total in IPL history, including three centuries.\\n\\nGill scored 129 in the IPL Qualifies 2, the highest individual score in IPL Playoffs history.\\n\\nIn 2023, Gill made his acting debut as the voice of Pavitr \"Pav\" Prabhakar / Spider-Man India in the Hindi and Punjabi dubs of the Sony Pictures Animation film Spider-Man: Across the Spider-Verse.']\n",
      "['Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] i; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays for Royal Challengers Bangalore in the IPL and Delhi in domestic cricket.\\n\\nConsidered to be one of the best cricketers in the world, he is widely regarded as one of the greatest batsmen in the history of the sport.[4] Nicknamed \"The King\", due to his dominant style of play and popularity, Kohli holds numerous records in his career across all formats.\\n\\nIn 2020, the International Cricket Council named him the male cricketer of the decade.\\n\\nKohli has also contributed to India\\'s successes, captaining the team from 2014 to 2022, and winning the 2011 World Cup and the 2013 Champions trophy.\\n\\nHe is among the only four Indian cricketers who have played over 500 matches for India.[5]\\n\\nBorn and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team.\\n\\nHe made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011.\\n\\nIn 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time.\\n\\nDuring 2014 T20 World Cup, he set a record for the most runs scored in the tournament.\\n\\nIn 2018, he achieved yet another milestone, becoming the world\\'s top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game.\\n\\nHis form continued in 2019, when he became the first player to score 20,000 international runs in a single decade.\\n\\nIn 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\\n\\n\\n\\nHe has received many accolades for his performances on the cricket field.\\n\\nHe was recognized as the ICC ODI Player of the Year in 2012 and has won the Sir Garfield Sobers Trophy, given to the ICC Cricketer of the Year, on two occasions, in 2017 and 2018 respectively.\\n\\nSubsequently, Kohli also won ICC Test Player of the Year and ICC ODI Player of the Year awards in 2018, becoming the first player to win both awards in the same year.\\n\\nAdditionally, he was named the Wisden Leading Cricketer in the World for three consecutive years, from 2016 to 2018.\\n\\nAt the national level, Kohli was honoured with the Arjuna Award in 2013, the Padma Shri under the sports category in 2017 and the Khel Ratna award, India\\'s highest sporting honour, in 2018.\\n\\n\\n\\nIn 2016, he was ranked as one of the world\\'s most famous athletes by ESPN, and one of the most valuable athlete brands by Forbes.\\n\\nIn 2018, Time magazine included him on its list of the 100 most influential people in the world.\\n\\nIn 2020, he was ranked 66th in Forbes list of the top 100 highest-paid athletes in the world for the year 2020 with estimated earnings of over $26 million.\\n\\nKohli has been deemed one of the most commercially viable cricketers, with estimated earnings of ₹165 crore (US$21 million) in the year 2022.']\n",
      "['Learning Transferable Visual Models From Natural Language Supervision\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision\\n\\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1\\n\\nAditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\\n\\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1\\n\\nAbstract\\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories.\\n\\nThis restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept.\\n\\nLearning directly from raw\\ntext about images is a promising alternative which\\nleverages a much broader source of supervision.\\n\\n\\nWe demonstrate that the simple pre-training task\\nof predicting which caption goes with which im-\\nage is an efficient and scalable way to learn SOTA\\nimage representations from scratch on a dataset\\nof 400 million (image, text) pairs collected from\\nthe internet.\\n\\nAfter pre-training, natural language\\nis used to reference learned visual concepts (or\\ndescribe new ones) enabling zero-shot transfer\\nof the model to downstream tasks.\\n\\nWe study\\nthe performance of this approach by benchmark-\\ning on over 30 different existing computer vi-\\nsion datasets, spanning tasks such as OCR, ac-\\ntion recognition in videos, geo-localization, and\\nmany types of fine-grained object classification.\\n\\n\\nThe model transfers non-trivially to most tasks\\nand is often competitive with a fully supervised\\nbaseline without the need for any dataset spe-\\ncific training.\\n\\nFor instance, we match the ac-\\ncuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28\\nmillion training examples it was trained on.\\n\\nWe\\nrelease our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.\\n\\n1.\\n\\nIntroduction and Motivating Work\\nPre-training methods which learn directly from raw text\\nhave revolutionized NLP over the last few years (Dai &\\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\\n\\n\\n\\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\\n\\n\\nCorrespondence to: <{alec, jongwook}@openai.com>.\\n\\n\\n\\nTask-agnostic objectives such as autoregressive and masked\\nlanguage modeling have scaled across many orders of mag-\\nnitude in compute, model capacity, and data, steadily im-\\nproving capabilities.\\n\\nThe development of “text-to-text” as\\na standardized input-output interface (McCann et al., 2018;\\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\\nagnostic architectures to zero-shot transfer to downstream\\ndatasets removing the need for specialized output heads or\\ndataset specific customization.\\n\\nFlagship systems like GPT-3\\n(Brown et al., 2020) are now competitive across many tasks\\nwith bespoke models while requiring little to no dataset\\nspecific training data.\\n\\n\\n\\nThese results suggest that the aggregate supervision acces-\\nsible to modern pre-training methods within web-scale col-\\nlections of text surpasses that of high-quality crowd-labeled\\nNLP datasets.\\n\\nHowever, in other fields such as computer\\nvision it is still standard practice to pre-train models on\\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\\n\\n\\nCould scalable pre-training methods which learn directly\\nfrom web text result in a similar breakthrough in computer\\nvision?\\n\\nPrior work is encouraging.\\n\\n\\n\\nOver 20 years ago Mori et al.\\n\\n(1999) explored improving\\ncontent based image retrieval by training a model to pre-\\ndict the nouns and adjectives in text documents paired with\\nimages.\\n\\nQuattoni et al.\\n\\n(2007) demonstrated it was possi-\\nble to learn more data efficient image representations via\\nmanifold learning in the weight space of classifiers trained\\nto predict words in captions associated with images.', 'Sri-\\nvastava & Salakhutdinov (2012) explored deep represen-\\ntation learning by training multimodal Deep Boltzmann\\nMachines on top of low-level image and text tag features.\\n\\n\\nJoulin et al. (2016) modernized this line of work and demon-\\nstrated that CNNs trained to predict words in image cap-\\ntions learn useful image representations.\\n\\nThey converted\\nthe title, description, and hashtag metadata of images in the\\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\\nwords multi-label classification task and showed that pre-\\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\\nlabels learned representations which preformed similarly\\nto ImageNet-based pre-training on transfer tasks.\\n\\nLi et al.\\n(2017) then extended this approach to predicting phrase n-\\ngrams in addition to individual words and demonstrated the\\nability of their system to zero-shot transfer to other image\\n\\nar\\nX\\n\\niv\\n:2\\n\\n10\\n3.\\n\\n00\\n02\\n\\n0v\\n1 \\n\\n [\\ncs\\n\\n.C\\nV\\n\\n] \\n 2\\n\\n6 \\nFe\\n\\nb \\n20\\n\\n21\\n\\nhttps://github.com/OpenAI/CLIP\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 2\\n\\nI1·T2 I1·T3 …\\n\\nI2·T1 I2·T3 …\\n\\nI3·T1 I3·T2 …\\n\\n⋮ ⋮ ⋮\\n\\nI1·T1\\n\\nI2·T2\\n\\nI3·T3\\n\\n(1) Contrastive pre-training\\n\\nImage\\nEncoder\\n\\nText\\nEncoderPepper\\tthe\\n\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nPepper\\tthe\\naussie\\tpup\\n\\nT1 T2 T3 …\\n\\nI1\\n\\nI2\\n\\nI3\\n\\n⋮\\n\\n(2) Create dataset classifier from label text\\n\\nplane\\n\\ncar\\n\\ndog\\n\\n⋮\\n\\nbird\\n\\nA\\tphoto\\tof\\na\\t{object}.\\n\\n\\n\\n⋮\\n\\nText\\nEncoder\\n\\nT1 T2 T3 TN\\n\\n…\\n\\n(3) Use for zero-shot prediction\\n\\nImage\\nEncoder\\n\\nI1 I1·T2 I1·TNI1·T1\\n\\n…\\n\\n…\\n\\nA\\tphoto\\tof\\n\\ta\\tdog.\\n\\n\\n\\nTN\\n\\nIN·T1 IN·T2 IN·T3\\n\\nI1·TN\\n\\nI2·TN\\n\\nI3·TN\\n\\n⋮\\n\\n…IN\\n\\n…\\n\\n⋮ ⋱\\n\\nIN·TN\\n\\nI1·T3\\n\\nFigure 1.\\n\\nSummary of our approach.\\n\\nWhile standard image models jointly train an image feature extractor and a linear classifier to predict\\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\\nexamples.\\n\\nAt test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\\ntarget dataset’s classes.\\n\\n\\n\\nclassification datasets by scoring target classes based on\\ntheir dictionary of learned visual n-grams and predicting the\\none with the highest score.\\n\\nAdopting more recent architec-\\ntures and pre-training approaches, VirTex (Desai & Johnson,\\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\\nVIRT (Zhang et al., 2020) have recently demonstrated the\\npotential of transformer-based language modeling, masked\\nlanguage modeling, and contrastive objectives to learn im-\\nage representations from text.\\n\\n\\n\\nWhile exciting as proofs of concept, using natural language\\nsupervision for image representation learning is still rare.\\n\\n\\nThis is likely because demonstrated performance on com-\\nmon benchmarks is much lower than alternative approaches.\\n\\n\\nFor example, Li et al. (2017) reach only 11.5% accuracy\\non ImageNet in a zero-shot setting.\\n\\nThis is well below the\\n88.4% accuracy of the current state of the art (Xie et al.,\\n2020).\\n\\nIt is even below the 50% accuracy of classic com-\\nputer vision approaches (Deng et al., 2012).\\n\\nInstead, more\\nnarrowly scoped but well-targeted uses of weak supervision\\nhave improved performance.\\n\\nMahajan et al. (2018) showed\\nthat predicting ImageNet-related hashtags on Instagram im-\\nages is an effective pre-training task.\\n\\nWhen fine-tuned to\\nImageNet these pre-trained models increased accuracy by\\nover 5% and improved the overall state of the art at the time.\\n\\n\\nKolesnikov et al. (2019) and Dosovitskiy et al.\\n\\n(2020) have\\nalso demonstrated large gains on a broader set of transfer\\nbenchmarks by pre-training models to predict the classes of\\nthe noisily labeled JFT-300M dataset.\\n\\n\\n\\nThis line of work represents the current pragmatic middle\\nground between learning from a limited amount of super-\\nvised “gold-labels” and learning from practically unlimited\\namounts of raw text.\\n\\nHowever, it is not without compro-\\n\\nmises.', 'However, it is not without compro-\\n\\nmises.\\n\\nBoth works carefully design, and in the process limit,\\ntheir supervision to 1000 and 18291 classes respectively.\\n\\n\\nNatural language is able to express, and therefore supervise,\\na much wider set of visual concepts through its general-\\nity.\\n\\nBoth approaches also use static softmax classifiers to\\nperform prediction and lack a mechanism for dynamic out-\\nputs.\\n\\nThis severely curtails their flexibility and limits their\\n“zero-shot” capabilities.\\n\\n\\n\\nA crucial difference between these weakly supervised mod-\\nels and recent explorations of learning image representations\\ndirectly from natural language is scale.\\n\\nWhile Mahajan et al.\\n(2018) and Kolesnikov et al.\\n\\n(2019) trained their models for\\naccelerator years on millions to billions of images, VirTex,\\nICMLM, and ConVIRT trained for accelerator days on one\\nto two hundred thousand images.\\n\\nIn this work, we close\\nthis gap and study the behaviors of image classifiers trained\\nwith natural language supervision at large scale.\\n\\nEnabled\\nby the large amounts of publicly available data of this form\\non the internet, we create a new dataset of 400 million (im-\\nage, text) pairs and demonstrate that a simplified version of\\nConVIRT trained from scratch, which we call CLIP, for Con-\\ntrastive Language-Image Pre-training, is an efficient method\\nof learning from natural language supervision.\\n\\nWe study\\nthe scalability of CLIP by training a series of eight models\\nspanning almost 2 orders of magnitude of compute and ob-\\nserve that transfer performance is a smoothly predictable\\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\\n2020).\\n\\nWe find that CLIP, similar to the GPT family, learns\\nto perform a wide set of tasks during pre-training including\\nOCR, geo-localization, action recognition, and many others.\\n\\n\\nWe measure this by benchmarking the zero-shot transfer\\nperformance of CLIP on over 30 existing datasets and find\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 3\\n\\n2M 33M 67M 134M 268M 400M\\n# of images processed\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt\\n\\nI\\nm\\n\\nag\\neN\\n\\net\\n A\\n\\ncc\\nur\\n\\nac\\ny\\n\\n3X efficiency4X efficiency\\n\\nBag of Words Contrastive (CLIP)\\n\\n\\nBag of Words Prediction\\nTransformer Language Model\\n\\nFigure 2.\\n\\nCLIP is much more efficient at zero-shot transfer\\nthan our image caption baseline.\\n\\nAlthough highly expressive,\\nwe found that transformer-based language models are relatively\\nweak at zero-shot ImageNet classification.\\n\\nHere, we see that it\\nlearns 3x slower than a baseline which predicts a bag-of-words\\n(BoW) encoding of the text (Joulin et al., 2016).\\n\\nSwapping the\\nprediction objective for the contrastive objective of CLIP further\\nimproves efficiency another 4x.\\n\\n\\n\\nit can be competitive with prior task-specific supervised\\nmodels.\\n\\nWe also confirm these findings with linear-probe\\nrepresentation learning analysis and show that CLIP out-\\nperforms the best publicly available ImageNet model while\\nalso being more computationally efficient.\\n\\nWe additionally\\nfind that zero-shot CLIP models are much more robust than\\nequivalent accuracy supervised ImageNet models which\\nsuggests that zero-shot evaluation of task-agnostic models is\\nmuch more representative of a model’s capability.\\n\\nThese re-\\nsults have significant policy and ethical implications, which\\nwe consider in Section 7.\\n\\n2.\\n\\nApproach\\n2.1.\\n\\nNatural Language Supervision\\n\\nAt the core of our approach is the idea of learning percep-\\ntion from supervision contained in natural language.\\n\\nAs\\ndiscussed in the introduction, this is not at all a new idea,\\nhowever terminology used to describe work in this space\\nis varied, even seemingly contradictory, and stated motiva-\\ntions are diverse.\\n\\nZhang et al. (2020), Gomez et al. (2017),\\nJoulin et al.\\n\\n(\\n\\n2016), and Desai & Johnson (2020) all intro-\\nduce methods which learn visual representations from text\\npaired with images but describe their approaches as unsuper-\\nvised\\n\\n, self-supervised, weakly supervised, and supervised\\nrespectively.', ', self-supervised, weakly supervised, and supervised\\nrespectively.\\n\\n\\n\\nWe emphasize that what is common across this line of work\\nis not any of the details of the particular methods used but\\nthe appreciation of natural language as a training signal.\\n\\nAll\\nthese approaches are learning from natural language super-\\n\\nvision.\\n\\nAlthough early work wrestled with the complexity\\nof natural language when using topic model and n-gram\\nrepresentations, improvements in deep contextual represen-\\ntation learning suggest we now have the tools to effectively\\nleverage this abundant source of supervision (McCann et al.,\\n2017).\\n\\n\\n\\nLearning from natural language has several potential\\nstrengths over other training methods.\\n\\nIt’s much easier\\nto scale natural language supervision compared to standard\\ncrowd-sourced labeling for image classification since it does\\nnot require annotations to be in a classic “machine learning\\ncompatible format” such as the canonical 1-of-N majority\\nvote “gold label”.\\n\\nInstead, methods which work on natural\\nlanguage can learn passively from the supervision contained\\nin the vast amount of text on the internet.\\n\\nLearning from\\nnatural language also has an important advantage over most\\nunsupervised or self-supervised learning approaches in that\\nit doesn’t “just” learn a representation but also connects that\\nrepresentation to language which enables flexible zero-shot\\ntransfer.\\n\\nIn the following subsections, we detail the specific\\napproach we settled on.\\n\\n2.2.\\n\\nCreating a Sufficiently Large Dataset\\n\\nExisting work has mainly used three datasets, MS-COCO\\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\\nYFCC100M (Thomee et al., 2016).\\n\\nWhile MS-COCO and\\nVisual Genome are high quality crowd-labeled datasets, they\\nare small by modern standards with approximately 100,000\\ntraining photos each.\\n\\nBy comparison, other computer vision\\nsystems are trained on up to 3.5 billion Instagram photos\\n(Mahajan et al., 2018).\\n\\nYFCC100M, at 100 million photos,\\nis a possible alternative, but the metadata for each image is\\nsparse and of varying quality.\\n\\nMany images use automati-\\ncally generated filenames like 20160716 113957.JPG\\nas “titles” or contain “descriptions” of camera exposure\\nsettings.\\n\\nAfter filtering to keep only images with natural\\nlanguage titles and/or descriptions in English, the dataset\\nshrunk by a factor of 6 to only 15 million photos.\\n\\nThis is\\napproximately the same size as ImageNet.\\n\\n\\n\\nA major motivation for natural language supervision is the\\nlarge quantities of data of this form available publicly on the\\ninternet.\\n\\nSince existing datasets do not adequately reflect\\nthis possibility, considering results only on them would un-\\nderestimate the potential of this line of research.\\n\\nTo address\\nthis, we constructed a new dataset of 400 million (image,\\ntext) pairs collected form a variety of publicly available\\nsources on the Internet.\\n\\nTo attempt to cover as broad a set\\nof visual concepts as possible, we search for (image, text)\\npairs as part of the construction process whose text includes\\none of a set of 500,000 queries.1 We approximately class\\n\\n1The base query list is all words occurring at least 100 times in\\nthe English version of Wikipedia.\\n\\nThis is augmented with bi-grams\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 4\\n\\nbalance the results by including up to 20,000 (image, text)\\npairs per query.\\n\\nThe resulting dataset has a similar total\\nword count as the WebText dataset used to train GPT-2.\\n\\nWe\\nrefer to this dataset as WIT for WebImageText.\\n\\n2.3.\\n\\nSelecting an Efficient Pre-Training Method\\n\\nState-of-the-art computer vision systems use very large\\namounts of compute.\\n\\nMahajan et al. (2018) required 19\\nGPU years to train their ResNeXt101-32x48d and Xie et al.\\n(2020) required 33 TPUv3 core-years to train their Noisy\\nStudent EfficientNet-L2.', 'Mahajan et al. (2018) required 19\\nGPU years to train their ResNeXt101-32x48d and Xie et al.\\n(2020) required 33 TPUv3 core-years to train their Noisy\\nStudent EfficientNet-L2.\\n\\nWhen considering that both these\\nsystems were trained to predict only 1000 ImageNet classes,\\nthe task of learning an open set of visual concepts from\\nnatural language seems daunting.\\n\\nIn the course of our ef-\\nforts, we found training efficiency was key to successfully\\nscaling natural language supervision and we selected our\\nfinal pre-training method based on this metric.\\n\\n\\n\\nOur initial approach, similar to VirTex, jointly trained an\\nimage CNN and text transformer from scratch to predict the\\ncaption of an image.\\n\\nHowever, we encountered difficulties\\nefficiently scaling this method.\\n\\nIn Figure 2 we show that a\\n63 million parameter transformer language model, which\\nalready uses twice the compute of its ResNet-50 image\\nencoder, learns to recognize ImageNet classes three times\\nslower than a much simpler baseline that predicts a bag-of-\\nwords encoding of the same text.\\n\\n\\n\\nBoth these approaches share a key similarity.\\n\\nThey try to pre-\\ndict the exact words of the text accompanying each image.\\n\\n\\nThis is a difficult task due to the wide variety of descriptions,\\ncomments, and related text that co-occur with images.\\n\\nRe-\\ncent work in contrastive representation learning for images\\nhas found that contrastive objectives can learn better repre-\\nsentations than their equivalent predictive objective (Tian\\net al., 2019).\\n\\nOther work has found that although generative\\nmodels of images can learn high quality image representa-\\ntions, they require over an order of magnitude more compute\\nthan contrastive models with the same performance (Chen\\net al., 2020a).\\n\\nNoting these findings, we explored training\\na system to solve the potentially easier proxy task of pre-\\ndicting only which text as a whole is paired with which\\nimage and not the exact words of that text.\\n\\nStarting with\\nthe same bag-of-words encoding baseline, we swapped the\\npredictive objective for a contrastive objective in Figure 2\\nand observed a further 4x efficiency improvement in the rate\\nof zero-shot transfer to ImageNet.\\n\\n\\n\\nGiven a batch of N (image, text) pairs, CLIP is trained to\\npredict which of the N ×N possible (image, text) pairings\\nacross a batch actually occurred.\\n\\nTo do this, CLIP learns a\\n\\nwith high pointwise mutual information as well as the names of\\nall Wikipedia articles above a certain search volume.\\n\\nFinally all\\nWordNet synsets not already in the query list are added.\\n\\n\\n\\nmulti-modal embedding space by jointly training an image\\nencoder and text encoder to maximize the cosine similar-\\nity of the image and text embeddings of the N real pairs\\nin the batch while minimizing the cosine similarity of the\\nembeddings of the N2 − N incorrect pairings.\\n\\nWe opti-\\nmize a symmetric cross entropy loss over these similarity\\nscores.\\n\\nIn Figure 3 we include pseudocode of the core of an\\nimplementation of CLIP.\\n\\nTo our knowledge this batch con-\\nstruction technique and objective was first introduced in the\\narea of deep metric learning as the multi-class N-pair loss\\nSohn (2016), was popularized for contrastive representation\\nlearning by Oord et al.\\n\\n(2018) as the InfoNCE loss, and was\\nrecently adapted for contrastive (text, image) representation\\nlearning in the domain of medical imaging by Zhang et al.\\n(2020).\\n\\n\\n\\nDue to the large size of our pre-training dataset, over-fitting\\nis not a major concern and the details of training CLIP are\\nsimplified compared to the implementation of Zhang et al.\\n(2020).\\n\\nWe train CLIP from scratch without initializing the\\nimage encoder with ImageNet weights or the text encoder\\nwith pre-trained weights.\\n\\nWe do not use the non-linear\\nprojection between the representation and the contrastive\\nembedding space, a change which was introduced by Bach-\\nman et al. (2019) and popularized by Chen et al. (2020b).', 'We instead use only a linear projection to map from each en-\\ncoder’s representation to the multi-modal embedding space.\\n\\n\\nWe did not notice a difference in training efficiency between\\nthe two versions and speculate that non-linear projections\\nmay be co-adapted with details of current image only in\\nself-supervised representation learning methods.\\n\\nWe also\\nremove the text transformation function tu from Zhang et al.\\n(2020) which samples a single sentence at uniform from\\nthe text since many of the (image, text) pairs in CLIP’s pre-\\ntraining dataset are only a single sentence.\\n\\nWe also simplify\\nthe image transformation function tv.\\n\\nA random square\\ncrop from resized images is the only data augmentation\\nused during training.\\n\\nFinally, the temperature parameter\\nwhich controls the range of the logits in the softmax, τ , is\\ndirectly optimized during training as a log-parameterized\\nmultiplicative scalar to avoid turning as a hyper-parameter.\\n\\n2.4.\\n\\nChoosing and Scaling a Model\\n\\nWe consider two different architectures for the image en-\\ncoder.\\n\\nFor the first, we use ResNet-50 (He et al., 2016a)\\nas the base architecture for the image encoder due to its\\nwidespread adoption and proven performance.\\n\\nWe make sev-\\neral modifications to the original version using the ResNet-\\nD improvements from He et al.\\n\\n(2019) and the antialiased\\nrect-2 blur pooling from Zhang (2019).\\n\\nWe also replace\\nthe global average pooling layer with an attention pooling\\nmechanism.\\n\\nThe attention pooling is implemented as a sin-\\ngle layer of “transformer-style” multi-head QKV attention\\nwhere the query is conditioned on the global average-pooled\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 5\\n\\n# image_encoder - ResNet or Vision Transformer\\n# text_encoder  - CBOW or Text Transformer\\n# I[n, h, w, c] - minibatch of aligned images\\n# T[n, l]       - minibatch of aligned texts\\n# W_i[d_i, d_e] - learned proj of image to embed\\n# W_t[d_t, d_e] - learned proj of text to embed\\n# t             - learned temperature parameter\\n\\n# extract feature representations of each modality\\nI_f = image_encoder(I) #[n, d_i]\\nT_f = text_encoder(T)  #[n, d_t]\\n\\n# joint multimodal embedding [n, d_e]\\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\\n\\n# scaled pairwise cosine similarities [n, n]\\nlogits = np.dot(I_e, T_e.T)\\n\\n* np.exp(t)\\n\\n# symmetric loss function\\nlabels = np.arange(n)\\nloss_i = cross_entropy_loss(logits, labels, axis=0)\\nloss_t = cross_entropy_loss(logits, labels, axis=1)\\nloss   = (loss_i + loss_t)/2\\n\\nFigure 3.\\n\\nNumpy-like pseudocode for the core of an implementa-\\ntion of CLIP.\\n\\n\\n\\nrepresentation of the image.\\n\\nFor the second architecture, we\\nexperiment with the recently introduced Vision Transformer\\n(ViT) (Dosovitskiy et al., 2020).\\n\\nWe closely follow their\\nimplementation with only the minor modification of adding\\nan additional layer normalization to the combined patch\\nand position embeddings before the transformer and use a\\nslightly different initialization scheme.\\n\\n\\n\\nThe text encoder is a Transformer (Vaswani et al., 2017)\\nwith the architecture modifications described in Radford\\net al. (2019).\\n\\nAs a base size we use a 63M-parameter 12-\\nlayer 512-wide model with 8 attention heads.\\n\\nThe trans-\\nformer operates on a lower-cased byte pair encoding (BPE)\\nrepresentation of the text with a 49,152 vocab size (Sen-\\nnrich et al., 2015).\\n\\nFor computational efficiency, the max\\nsequence length was capped at 76.\\n\\nThe text sequence is\\nbracketed with [SOS] and [EOS] tokens and the activa-\\ntions of the highest layer of the transformer at the [EOS]\\ntoken are treated as the feature representation of the text\\nwhich is layer normalized and then linearly projected into\\nthe multi-modal embedding space.\\n\\nMasked self-attention\\nwas used in the text encoder to preserve the ability to ini-\\ntialize with a pre-trained language model or add language\\nmodeling as an auxiliary objective, though exploration of\\nthis is left as future work.', 'While previous computer vision research has often scaled\\nmodels by increasing the width (Mahajan et al., 2018) or\\ndepth (He et al., 2016a) in isolation, for the ResNet image\\nencoders we adapt the approach of Tan & Le (2019) which\\nfound that allocating additional compute across all of width,\\ndepth, and resolution outperforms only allocating it to only\\n\\none dimension of the model.\\n\\nWhile Tan & Le (2019) tune\\nthe ratio of compute allocated to each dimension for their\\nEfficientNet architecture, we use a simple baseline of allo-\\ncating additional compute equally to increasing the width,\\ndepth, and resolution of the model.\\n\\nFor the text encoder, we\\nonly scale the width of the model to be proportional to the\\ncalculated increase in width of the ResNet and do not scale\\nthe depth at all, as we found CLIP’s performance to be less\\nsensitive to the capacity of the text encoder.\\n\\n2.5.\\n\\nTraining\\n\\nWe train a series of 5 ResNets and 3 Vision Transformers.\\n\\n\\nFor the ResNets we train a ResNet-50, a ResNet-101, and\\nthen 3 more which follow EfficientNet-style model scaling\\nand use approximately 4x, 16x, and 64x the compute of a\\nResNet-50.\\n\\nThey are denoted as RN50x4, RN50x16, and\\nRN50x64 respectively.\\n\\nFor the Vision Transformers we\\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14.\\n\\nWe train all\\nmodels for 32 epochs.\\n\\nWe use the Adam optimizer (Kingma\\n& Ba, 2014) with decoupled weight decay regularization\\n(Loshchilov & Hutter, 2017) applied to all weights that are\\nnot gains or biases, and decay the learning rate using a\\ncosine schedule (Loshchilov & Hutter, 2016).\\n\\nInitial hyper-\\nparameters were set using a combination of grid searches,\\nrandom search, and manual tuning on the baseline ResNet-\\n50 model when trained for 1 epoch.\\n\\nHyper-parameters were\\nthen adapted heuristically for larger models due to compu-\\ntational constraints.\\n\\nThe learnable temperature parameter\\nτ was initialized to the equivalent of 0.07 from (Wu et al.,\\n2018) and clipped to prevent scaling the logits by more\\nthan 100 which we found necessary to prevent training in-\\nstability.\\n\\nWe use a very large minibatch size of 32,768.\\nMixed-precision (Micikevicius et al., 2017) was used to ac-\\ncelerate training and save memory.\\n\\nTo save additional mem-\\nory, gradient checkpointing (Griewank & Walther, 2000;\\nChen et al., 2016), half-precision Adam statistics (Dhariwal\\net al., 2020), and half-precision stochastically rounded text\\nencoder weights were used.\\n\\nThe calculation of embedding\\nsimilarities was also sharded with individual GPUs comput-\\ning only the subset of the pairwise similarities necessary for\\ntheir local batch of embeddings.\\n\\nThe largest ResNet model,\\nRN50x64, took 18 days to train on 592 V100 GPUs while\\nthe largest Vision Transformer took 12 days on 256 V100\\nGPUs.\\n\\nFor the ViT-L/14 we also pre-train at a higher 336\\npixel resolution for one additional epoch to boost perfor-\\nmance similar to FixRes (Touvron et al., 2019).\\n\\nWe denote\\nthis model as ViT-L/14@336px.\\n\\nUnless otherwise specified,\\nall results reported in this paper as “CLIP” use this model\\nwhich we found to perform best.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 6\\n\\n3.\\n\\nExperiments\\n3.1.\\n\\nZero-Shot Transfer\\n\\n3.1.1.\\n\\nMOTIVATION\\n\\n\\n\\nIn computer vision, zero-shot learning usually refers to the\\nstudy of generalizing to unseen object categories in image\\nclassification (Lampert et al., 2009).\\n\\nWe instead use the\\nterm in a broader sense and study generalization to unseen\\ndatasets.\\n\\nWe motivate this as a proxy for performing un-\\nseen tasks, as aspired to in the zero-data learning paper of\\nLarochelle et al. (2008).\\n\\nWhile much research in the field of\\nunsupervised learning focuses on the representation learn-\\ning capabilities of machine learning systems, we motivate\\nstudying zero-shot transfer as a way of measuring the task-\\nlearning capabilities of machine learning systems.\\n\\nIn this\\nview, a dataset evaluates performance on a task on a spe-\\ncific distribution.', 'In this\\nview, a dataset evaluates performance on a task on a spe-\\ncific distribution.\\n\\nHowever, many popular computer vision\\ndatasets were created by the research community primarily\\nas benchmarks to guide the development of generic image\\nclassification methods rather than measuring performance\\non a specific task.\\n\\nWhile it is reasonable to say that the\\nSVHN dataset measures the task of street number transcrip-\\ntion on the distribution of Google Street View photos, it is\\nunclear what “real” task the CIFAR-10 dataset measures.\\n\\n\\nIt is clear, however, what distribution CIFAR-10 is drawn\\nfrom - TinyImages (Torralba et al., 2008).\\n\\nOn these kinds of\\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\\nrobustness to distribution shift and domain generalization\\nrather than task generalization.\\n\\nPlease see Section 3.3 for\\nanalysis focused on this.\\n\\n\\n\\nTo our knowledge, Visual N-Grams (Li et al., 2017) first\\nstudied zero-shot transfer to existing image classification\\ndatasets in the manner described above.\\n\\nIt is also the only\\nother work we are aware of that has studied zero-shot trans-\\nfer to standard image classification datasets using a gener-\\nically pre-trained model and serves as the best reference\\npoint for contextualizing CLIP.\\n\\nTheir approach learns the\\nparameters of a dictionary of 142,806 visual n-grams (span-\\nning 1- to 5- grams) and optimizes these n-grams using a\\ndifferential version of Jelinek-Mercer smoothing to maxi-\\nmize the probability of all text n-grams for a given image.\\n\\n\\nIn order to perform zero-shot transfer, they first convert the\\ntext of each of the dataset’s class names into its n-gram\\nrepresentation and then compute its probability according\\nto their model, predicting the one with the highest score.\\n\\n\\n\\nOur focus on studying zero-shot transfer as an evaluation of\\ntask learning is inspired by work demonstrating task learn-\\ning in the field of NLP.\\n\\nTo our knowledge Liu et al. (2018)\\nfirst identified task learning as an “unexpected side-effect”\\nwhen a language model trained to generate Wikipedia ar-\\nticles learned to reliably transliterate names between lan-\\nguages.\\n\\nWhile GPT-1 (Radford et al., 2018) focused on pre-\\n\\ntraining as a transfer learning method to improve supervised\\nfine-tuning, it also included an ablation study demonstrat-\\ning that the performance of four heuristic zero-shot transfer\\nmethods improved steadily over the course of pre-training,\\nwithout any supervised adaption.\\n\\nThis analysis served as the\\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\\nsively on studying the task-learning capabilities of language\\nmodels via zero-shot transfer.\\n\\n3.1.2.\\n\\nUSING CLIP FOR ZERO-SHOT TRANSFER\\n\\nCLIP is pre-trained to predict if an image and a text snippet\\nare paired together in its dataset.\\n\\nTo perform zero-shot clas-\\nsification, we reuse this capability.\\n\\nFor each dataset, we use\\nthe names of all the classes in the dataset as the set of poten-\\ntial text pairings and predict the most probable (image, text)\\npair according to CLIP.\\n\\nIn a bit more detail, we first compute\\nthe feature embedding of the image and the feature embed-\\nding of the set of possible texts by their respective encoders.\\n\\n\\nThe cosine similarity of these embeddings is then calculated,\\nscaled by a temperature parameter τ , and normalized into a\\nprobability distribution via a softmax.\\n\\nNote that this predic-\\ntion layer is a multinomial logistic regression classifier with\\nL2-normalized inputs, L2-normalized weights, no bias, and\\ntemperature scaling.\\n\\nWhen interpreted this way, the image\\nencoder is the computer vision backbone which computes a\\nfeature representation for the image and the text encoder is a\\nhypernetwork (Ha et al., 2016) which generates the weights\\nof a linear classifier based on the text specifying the visual\\nconcepts that the classes represent.\\n\\nLei Ba et al.', 'Lei Ba et al.\\n\\n(2015) first\\nintroduced a zero-shot image classifier of this form while\\nthe idea of generating a classifier from natural language\\ndates back to at least Elhoseiny et al. (2013).\\n\\nContinuing\\nwith this interpretation, every step of CLIP pre-training can\\nbe viewed as optimizing the performance of a randomly\\ncreated proxy to a computer vision dataset which contains 1\\nexample per class and has 32,768 total classes defined via\\nnatural language descriptions.\\n\\nFor zero-shot evaluation, we\\ncache the zero-shot classifier once it has been computed by\\nthe text encoder and reuse it for all subsequent predictions.\\n\\n\\nThis allows the cost of generating it to be amortized across\\nall the predictions in a dataset.\\n\\n\\n\\n3.1.3.\\n\\nINITIAL COMPARISON TO VISUAL N-GRAMS\\n\\nIn Table 1\\n\\nwe compare Visual N-Grams to CLIP.\\n\\nThe best\\nCLIP model improves accuracy on ImageNet from a proof\\nof concept 11.5% to 76.2% and matches the performance\\nof the original ResNet-50 despite using none of the 1.28\\nmillion crowd-labeled training examples available for this\\ndataset.\\n\\nAdditionally, the top-5 accuracy of CLIP models\\nare noticeably higher than their top-1, and this model has a\\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\\n2016).\\n\\nThe ability to match the performance of a strong,\\nfully supervised baselines in a zero-shot setting suggests\\n\\n\\n\\nLearning Transferable Visual Models From\\n\\nNatural Language Supervision 7\\n\\naYahoo\\n\\nImageNet SUN\\n\\nVisual N-Grams 72.4 11.5 23.0\\nCLIP 98.4 76.2 58.5\\n\\nTable 1.\\n\\nComparing CLIP to prior zero-shot transfer image classi-\\nfication results.\\n\\nCLIP improves performance on all three datasets\\nby a large amount.\\n\\nThis improvement reflects many differences\\nin the 4 years since the development of Visual N-Grams (Li et al.,\\n2017).\\n\\n\\n\\nCLIP is a significant step towards flexible and practical\\nzero-shot computer vision classifiers.\\n\\nAs mentioned above,\\nthe comparison to Visual N-Grams is meant for contextu-\\nalizing the performance of CLIP and should not be inter-\\npreted as a direct methods comparison between CLIP and\\nVisual N-Grams as many performance relevant differences\\nbetween the two systems were not controlled for.\\n\\nFor in-\\nstance, we train on a dataset that is 10x larger, use a vision\\nmodel that requires nearly 100x more compute per predic-\\ntion, likely used over 1000x their training compute, and\\nuse a transformer-based model which did not exist when\\nVisual N-Grams was published.\\n\\nAs a closer comparison, we\\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\\nthat Visual N-Grams was trained on and found it matched\\ntheir reported ImageNet performance within a V100 GPU\\nday.\\n\\nThis baseline was also trained from scratch instead of\\nbeing initialized from pre-trained ImageNet weights as in\\nVisual N-Grams.\\n\\nCLIP also outperforms Visual N-Grams on the other 2 re-\\nported datasets.\\n\\nOn aYahoo, CLIP achieves a 95% reduction\\nin the number of errors, and on SUN, CLIP more than dou-\\nbles the accuracy of Visual N-Grams.\\n\\nTo conduct a more\\ncomprehensive analysis and stress test, we implement a\\nmuch larger evaluation suite detailed in Appendix A. In\\ntotal we expand from the 3 datasets reported in Visual N-\\nGrams to include over 30 datasets and compare to over 50\\nexisting computer vision systems to contextualize results.\\n\\n\\n\\n3.1.4. PROMPT ENGINEERING AND ENSEMBLING\\n\\nMost standard image classification datasets treat the infor-\\nmation naming or describing classes which enables natural\\nlanguage based zero-shot transfer as an afterthought.\\n\\nThe\\nvast majority of datasets annotate images with just a numeric\\nid of the label and contain a file mapping these ids back to\\ntheir names in English.\\n\\nSome datasets, such as Flowers102\\nand GTSRB, don’t appear to include this mapping at all\\nin their released versions preventing zero-shot transfer en-\\ntirely.2', 'Some datasets, such as Flowers102\\nand GTSRB, don’t appear to include this mapping at all\\nin their released versions preventing zero-shot transfer en-\\ntirely.2\\n\\nFor many datasets, we observed these labels may be\\n\\n2Alec learned much more about flower species and German\\ntraffic signs over the course of this project than he originally antic-\\nipated.\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\n4X efficiency gain\\n\\n5 point\\nimprovement\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64\\n\\nPrompt engineering and ensembling\\nContextless class names (Li et al. 2017)\\n\\nFigure 4.\\n\\nPrompt engineering and ensembling improve zero-\\nshot performance.\\n\\nCompared to the baseline of using contextless\\nclass names, prompt engineering and ensembling boost zero-shot\\nclassification performance by almost 5 points on average across\\n36 datasets.\\n\\nThis improvement is similar to the gain from using\\n4 times more compute with the baseline zero-shot method but is\\n“free” when amortized over many predictions.\\n\\nchosen somewhat haphazardly and do not anticipate issues\\nrelated to zero-shot transfer which relies on task description\\nin order to transfer successfully.\\n\\n\\n\\nA common issue is polysemy.\\n\\nWhen the name of a class\\nis the only information provided to CLIP’s text encoder it\\nis unable to differentiate which word sense is meant due to\\nthe lack of context.\\n\\nIn some cases multiple meanings of the\\nsame word might be included as different classes in the same\\ndataset!\\n\\nThis happens in ImageNet which contains both\\nconstruction cranes and cranes that fly.\\n\\nAnother example is\\nfound in classes of the Oxford-IIIT Pet dataset where the\\nword boxer is, from context, clearly referring to a breed of\\ndog, but to a text encoder lacking context could just as likely\\nrefer to a type of athlete.\\n\\n\\n\\nAnother issue we encountered is that it’s relatively rare in\\nour pre-training dataset for the text paired with the image\\nto be just a single word.\\n\\nUsually the text is a full sentence\\ndescribing the image in some way.\\n\\nTo help bridge this\\ndistribution gap, we found that using the prompt template\\n“A photo of a {label}.”\\n\\nto be a good default that\\nhelps specify the text is about the content of the image.\\n\\nThis\\noften improves performance over the baseline of using only\\nthe label text.\\n\\nFor instance, just using this prompt improves\\naccuracy on ImageNet by 1.3%.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 8\\n\\nSimilar to the “prompt engineering” discussion around GPT-\\n3 (Brown et al., 2020; Gao et al., 2020), we have also\\nobserved that zero-shot performance can be significantly\\nimproved by customizing the prompt text to each task.\\n\\nA\\nfew, non exhaustive, examples follow.\\n\\nWe found on several\\nfine-grained image classification datasets that it helped to\\nspecify the category.\\n\\nFor example on Oxford-IIIT Pets, us-\\ning “A photo of a {label}, a type of pet.”\\nto help provide context worked well.\\n\\nLikewise, on Food101\\nspecifying a type of food and on FGVC Aircraft a type of\\naircraft helped too.\\n\\nFor OCR datasets, we found that putting\\nquotes around the text or number to be recognized improved\\nperformance.\\n\\nFinally, we found that on satellite image classi-\\nfication datasets it helped to specify that the images were of\\nthis form and we use variants of “a satellite photo\\nof a {label}.”.\\n\\n\\n\\nWe also experimented with ensembling over multiple zero-\\nshot classifiers as another way of improving performance.\\n\\n\\nThese classifiers are computed by using different context\\nprompts such as ‘A photo of a big {label}” and\\n“A photo of a small {label}”.\\n\\nWe construct the\\nensemble over the embedding space instead of probability\\nspace.\\n\\nThis allows us to cache a single set of averaged text\\nembeddings so that the compute cost of the ensemble is the\\nsame as using a single classifier when amortized over many\\npredictions.\\n\\nWe’ve observed ensembling across many gen-\\nerated zero-shot classifiers to reliably improve performance\\nand use it for the majority of datasets.', 'We’ve observed ensembling across many gen-\\nerated zero-shot classifiers to reliably improve performance\\nand use it for the majority of datasets.\\n\\nOn ImageNet, we\\nensemble 80 different context prompts and this improves\\nperformance by an additional 3.5% over the single default\\nprompt discussed above.\\n\\nWhen considered together, prompt\\nengineering and ensembling improve ImageNet accuracy\\nby almost 5%.\\n\\nIn Figure 4 we visualize how prompt engi-\\nneering and ensembling change the performance of a set of\\nCLIP models compared to the contextless baseline approach\\nof directly embedding the class name as done in Li et al.\\n(2017).\\n\\n\\n\\n3.1.5.\\n\\nANALYSIS OF ZERO-SHOT CLIP PERFORMANCE\\n\\n\\n\\nSince task-agnostic zero-shot classifiers for computer vision\\nhave been understudied, CLIP provides a promising oppor-\\ntunity to gain a better understanding of this type of model.\\n\\n\\nIn this section, we conduct a study of various properties of\\nCLIP’s zero-shot classifiers.\\n\\nAs a first question, we look\\nsimply at how well zero-shot classifiers perform.\\n\\nTo con-\\ntextualize this, we compare to the performance of a simple\\noff-the-shelf baseline: fitting a fully supervised, regularized,\\nlogistic regression classifier on the features of the canonical\\nResNet-50.\\n\\nIn Figure 5 we show this comparison across 27\\ndatasets.\\n\\nPlease see Appendix A for details of datasets and\\nsetup.\\n\\n\\n\\nZero-shot CLIP outperforms this baseline slightly more of-\\n\\n40 30 20 10 0 10 20 30 40\\n Score (%)\\n\\n\\n\\nZero-Shot CLIP vs. Linear Probe on ResNet50\\n\\nEuroSAT-37.1\\nKITTI Distance-34.0\\nPatchCamelyon-19.5\\nGTSRB-18.4\\nCLEVRCounts-18.2\\nDTD-16.6\\nFlowers102-12.5\\nRESISC45-11.9\\nFGVCAircraft-11.3\\nMNIST-10.0\\nBirdsnap-3.2\\n+0.5PascalVOC2007\\n+1.1OxfordPets\\n+1.9ImageNet\\n+2.0Caltech101\\n+2.8FER2013\\n+3.0STL10\\n+3.0CIFAR100\\n+3.9CIFAR10\\n\\n+6.7HatefulMemes\\n+7.7UCF101\\n+7.8SUN397\\n\\n+12.4SST2\\n+14.5Kinetics700\\n\\n+22.5Food101\\n+23.2Country211\\n\\n+28.9StanfordCars\\n\\nFigure 5.\\n\\nZero-shot CLIP is competitive with a fully super-\\nvised baseline.\\n\\nAcross a 27 dataset eval suite, a zero-shot CLIP\\nclassifier outperforms a fully supervised linear classifier fitted on\\nResNet-50 features on 16 datasets, including ImageNet.\\n\\nten than not and wins on 16 of the 27 datasets.\\n\\nLooking at\\nindividual datasets reveals some interesting behavior.\\n\\nOn\\nfine-grained classification tasks, we observe a wide spread\\nin performance.\\n\\nOn two of these datasets, Stanford Cars and\\nFood101, zero-shot CLIP outperforms logistic regression\\non ResNet-50 features by over 20% while on two others,\\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\\nforms by over 10%.\\n\\nOn OxfordPets and Birdsnap, per-\\nformance is much closer.\\n\\nWe suspect these difference are\\nprimarily due to varying amounts of per-task supervision\\nbetween WIT and ImageNet.\\n\\nOn “general” object classifica-\\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\\nPascalVOC2007 performance is relatively similar with a\\nslight advantage for zero-shot CLIP in all cases.\\n\\nOn STL10,\\nCLIP achieves 99.3% overall which appears to be a new\\nstate of the art despite not using any training examples.\\n\\nZero-\\nshot CLIP significantly outperforms a ResNet-50 on two\\ndatasets measuring action recognition in videos.\\n\\nOn Kinet-\\nics700, CLIP outperforms a ResNet-50 by 14.5%.\\n\\nZero-\\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\\non UCF101.\\n\\nWe speculate this is due to natural language\\nproviding wider supervision for visual concepts involving\\nverbs, compared to the noun-centric object supervision in\\nImageNet.\\n\\n\\n\\nLooking at where zero-shot CLIP notably underperforms,\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 9\\n\\n0 1 2 4 8 16\\n# of labeled training examples per class\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\nAv\\n\\ner\\nag\\n\\ne \\nSc\\n\\nor\\ne \\n\\n(%\\n)\\n\\nZero-Shot\\nCLIP BiT-M (ImageNet-21K)\\n\\nLinear Probe CLIP\\n\\nSimCLRv2\\n\\nResNet50\\n\\nFigure 6.\\n\\nZero-shot CLIP outperforms few-shot linear probes.', 'Zero-shot CLIP outperforms few-shot linear probes.\\n\\n\\nZero-shot CLIP matches the average performance of a 4-shot linear\\nclassifier trained on the same feature space and nearly matches the\\nbest results of a 16-shot linear classifier across publicly available\\nmodels.\\n\\nFor both BiT-M and SimCLRv2, the best performing\\nmodel is highlighted.\\n\\nLight gray lines are other models in the eval\\nsuite.\\n\\nThe 20 datasets with at least 16 examples per class were\\nused in this analysis.\\n\\n\\n\\nwe see that zero-shot CLIP is quite weak on several spe-\\ncialized, complex, or abstract tasks such as satellite image\\nclassification (EuroSAT and RESISC45), lymph node tumor\\ndetection (PatchCamelyon), counting objects in synthetic\\nscenes (CLEVRCounts), self-driving related tasks such as\\nGerman traffic sign recognition (GTSRB), recognizing dis-\\ntance to the nearest car (KITTI Distance).\\n\\nThese results\\nhighlight the poor capability of zero-shot CLIP on more\\ncomplex tasks.\\n\\nBy contrast, non-expert humans can robustly\\nperform several of these tasks, such as counting, satellite\\nimage classification, and traffic sign recognition, suggesting\\nsignificant room for improvement.\\n\\nHowever, we caution\\nthat it is unclear whether measuring zero-shot transfer, as\\nopposed to few-shot transfer, is a meaningful evaluation for\\ndifficult tasks that a learner has no prior experience with,\\nsuch as lymph node tumor classification for almost all hu-\\nmans (and possibly CLIP).\\n\\n\\n\\nWhile comparing zero-shot performance to fully supervised\\nmodels contextualizes the task-learning capabilities of CLIP,\\ncomparing to few-shot methods is a more direct compari-\\nson, since zero-shot is its limit.\\n\\nIn Figure 6, we visualize\\nhow zero-shot CLIP compares to few-shot logistic regres-\\nsion on the features of many image models including the\\nbest publicly available ImageNet models, self-supervised\\nlearning methods, and CLIP itself.\\n\\nWhile it is intuitive to\\n\\nexpect zero-shot to underperform one-shot, we instead find\\nthat zero-shot CLIP matches the performance of 4-shot lo-\\ngistic regression on the same feature space.\\n\\nThis is likely\\ndue to an important difference between the zero-shot and\\nfew-shot approach.\\n\\nFirst, CLIP’s zero-shot classifier is gen-\\nerated via natural language which allows for visual concepts\\nto be directly specified (“communicated”).\\n\\nBy contrast,\\n“normal” supervised learning must infer concepts indirectly\\nfrom training examples.\\n\\nContext-less example-based learn-\\ning has the drawback that many different hypotheses can\\nbe consistent with the data, especially in the one-shot case.\\n\\n\\nA single image often contains many different visual con-\\ncepts.\\n\\nAlthough a capable learner is able to exploit visual\\ncues and heuristics, such as assuming that the concept being\\ndemonstrated is the primary object in an image, there is no\\nguarantee.\\n\\n\\n\\nA potential resolution of this discrepancy between zero-\\nshot and few-shot performance is to use CLIP’s zero-shot\\nclassifier as a prior for the weights of the few-shot classifier.\\n\\n\\nWhile adding an L2 penalty towards the generated weights\\nis a straightforward implementation of this idea, we found\\nthat hyperparameter optimization would often select for\\nsuch a large value of this regularizer that the resulting few-\\nshot classifier was “just” the zero-shot classifier.\\n\\nResearch\\ninto better methods of combining the strength of zero-shot\\ntransfer with flexibility of few-shot learning is a promising\\ndirection for future work.\\n\\n\\n\\nWhen comparing zero-shot CLIP to few-shot logistic re-\\ngression on the features of other models, zero-shot CLIP\\nroughly matches the performance of the best performing\\n16-shot classifier in our evaluation suite, which uses the fea-\\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\\nWe are certain that a BiT-L model trained on JFT-300M\\nwould perform even better but these models have not been\\npublicly released.', 'That a BiT-M ResNet-152x2 performs\\nbest in a 16-shot setting is somewhat surprising since, as\\nanalyzed in Section 3.2, the Noisy Student EfficientNet-L2\\noutperforms it in a fully supervised setting by almost 5% on\\naverage across 27 datasets.\\n\\n\\n\\nIn addition to studying the average performance of zero-shot\\nCLIP and few-shot logistic regression, we also examine\\nperformance on individual datasets.\\n\\nIn Figure 7, we show\\nestimates for the number of labeled examples per class that\\na logistic regression classifier on the same feature space\\nrequires to match the performance of zero-shot CLIP.\\n\\nSince\\nzero-shot CLIP is also a linear classifier, this estimates the\\neffective data efficiency of zero-shot transfer in this setting.\\n\\n\\nIn order to avoid training thousands of linear classifiers,\\nwe estimate the effective data efficiency based on a log-\\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\\nshot (when possible), and a fully supervised linear classifier\\ntrained on each dataset.\\n\\nWe find that zero-shot transfer can\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 10\\n\\n0 25 50 75 100 125 150 175 200\\n# of labeled examples per class\\n\\nrequired to match zero-shot\\n\\nFlowers102\\nEuroSAT\\n\\nRESISC45\\nCLEVRCounts\\n\\nGTSRB\\nFGVCAircraft\\n\\nDTD\\nBirdsnap\\nUCF101\\n\\nKITTI Distance\\nCaltech101\\n\\nSUN397\\nMNIST\\n\\nStanfordCars\\nHatefulMemes\\n\\nCIFAR100\\nSTL10\\n\\nKinetics700\\nSST2\\n\\nPCam\\nImageNet\\n\\nCountry211\\nOxfordPets\\n\\nFood101\\n\\n\\nCIFAR10\\nFER2013\\n\\n0.9\\n0.9\\n1.5\\n1.5\\n1.6\\n2.0\\n2.6\\n2.7\\n2.9\\n2.9\\n3.5\\n3.9\\n4.8\\n6.0\\n9.8\\n12.0\\n12.7\\n13.6\\n14.4\\n14.7\\n16.0\\n\\n32\\n48\\n\\n64\\n81\\n\\n184\\n\\nMedian: 5.4\\nMean:  20.8\\n\\nFigure 7.\\n\\nThe data efficiency of zero-shot transfer varies\\nwidely.\\n\\nCalculating the number of labeled examples per class\\na linear classifier on the same CLIP feature space requires to match\\nthe performance of the zero-shot classifier contextualizes the ef-\\nfectiveness of zero-shot transfer.\\n\\nValues are estimated based on\\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\\nresults.\\n\\nPerformance varies widely from still underperforming a\\none-shot classifier on two datasets to matching an estimated 184\\nlabeled examples per class.\\n\\n\\n\\nhave widely varying efficiency per dataset from less than 1\\nlabeled example per class to 184.\\n\\nTwo datasets, Flowers102\\nand EuroSAT underperform one-shot models.\\n\\nHalf of the\\ndatasets require less than 5 examples per class with a median\\nof 5.4.\\n\\nHowever, the mean estimated data efficiency is 20.8\\nexamples per class.\\n\\nThis is due to the 20% of datasets\\nwhere supervised classifiers require many labeled examples\\nper class in order to match performance.\\n\\nOn ImageNet,\\nzero-shot CLIP matches the performance of a 16-shot linear\\nclassifier trained on the same feature space.\\n\\n\\n\\nIf we assume that evaluation datasets are large enough that\\nthe parameters of linear classifiers trained on them are well\\nestimated, then, because CLIP’s zero-shot classifier is also\\na linear classifier, the performance of the fully supervised\\nclassifiers roughly sets an upper bound for what zero-shot\\ntransfer can achieve.\\n\\nIn Figure 8 we compare CLIP’s zero-\\nshot performance with fully supervised linear classifiers\\nacross datasets.\\n\\nThe dashed, y = x line represents an “op-\\ntimal” zero-shot classifier that matches the performance of\\nits fully supervised equivalent.\\n\\nFor most datasets, the per-\\nformance of zero-shot classifiers still underperform fully su-\\npervised classifiers by 10% to 25%, suggesting that there is\\nstill plenty of headroom for improving CLIP’s task-learning\\nand zero-shot transfer capabilities.', 'There is a positive correlation of 0.82 (p-value < 10−6)\\nbetween zero-shot performance and fully supervised perfor-\\n\\n20 30 40 50 60 70 80 90 100\\nLinear Probe CLIP Performance\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nZe\\nro\\n\\n-S\\nho\\n\\nt C\\nLI\\n\\nP \\nPe\\n\\nrfo\\nrm\\n\\nan\\nce\\n\\nr = 0.82\\n\\nVOC2007\\n\\nCountry211\\n\\nHatefulMemes\\n\\nMNIST\\n\\nCIFAR10\\n\\nSST2\\n\\nDTD\\n\\nPCAM\\n\\nRESISC45\\n\\nEuroSAT\\n\\nGTSRB\\n\\nCLEVRCounts\\n\\nFER2013\\n\\nUCF101\\n\\nBirdsnap\\n\\nOxfordPets\\n\\nCIFAR100\\n\\nFGVCAircraft\\n\\nFood101\\n\\nFlowers102Stanford Cars\\n\\nCaltech101\\n\\nSUN397\\n\\nImageNet\\n\\nSTL10\\n\\nKITTI Distance\\n\\nKinetics700\\n\\nFigure 8.\\n\\nZero-shot performance is correlated with linear\\nprobe performance but still mostly sub-optimal.\\n\\nComparing\\nzero-shot and linear probe performance across datasets shows a\\nstrong correlation with zero-shot performance mostly shifted 10 to\\n25 points lower.\\n\\nOn only 5 datasets does zero-shot performance\\napproach linear probe performance (≤3 point difference).\\n\\n\\n\\nmance, suggesting that CLIP is relatively consistent at con-\\nnecting underlying representation and task learning to zero-\\nshot transfer.\\n\\nHowever, zero-shot CLIP only approaches\\nfully supervised performance on 5 datasets: STL10, CI-\\nFAR10, Food101, OxfordPets, and Caltech101.\\n\\nOn all 5\\ndatasets, both zero-shot accuracy and fully supervised accu-\\nracy are over 90%.\\n\\nThis suggests that CLIP may be more\\neffective at zero-shot transfer for tasks where its underly-\\ning representations are also high quality.\\n\\nThe slope of a\\nlinear regression model predicting zero-shot performance\\nas a function of fully supervised performance estimates that\\nfor every 1% improvement in fully supervised performance,\\nzero-shot performance improves by 1.28%.\\n\\nHowever, the\\n95th-percentile confidence intervals still include values of\\nless than 1 (0.93-1.79).\\n\\n\\n\\nOver the past few years, empirical studies of deep learning\\nsystems have documented that performance is predictable as\\na function of important quantities such as training compute\\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\\n\\n\\nThe GPT family of models has so far demonstrated consis-\\ntent improvements in zero-shot performance across a 1000x\\nincrease in training compute.\\n\\nIn Figure 9, we check whether\\nthe zero-shot performance of CLIP follows a similar scaling\\npattern.\\n\\nWe plot the average error rate of the 5 ResNet CLIP\\nmodels across 39 evaluations on 36 different datasets and\\nfind that a similar log-log linear scaling trend holds for CLIP\\nacross a 44x increase in model compute.\\n\\nWhile the overall\\ntrend is smooth, we found that performance on individual\\nevaluations can be much noisier.\\n\\nWe are unsure whether\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 11\\n\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nEr\\nro\\n\\nr (\\n%\\n\\n)\\n\\nRN50\\n\\nRN101\\n\\nRN50x4\\n\\nRN50x16\\n\\nRN50x64\\n\\nFigure 9.\\n\\nZero-shot CLIP performance scales smoothly as a\\nfunction of model compute.\\n\\nAcross 39 evals on 36 different\\ndatasets, average zero-shot error is well modeled by a log-log lin-\\near trend across a 44x range of compute spanning 5 different CLIP\\nmodels.\\n\\nLightly shaded lines are performance on individual evals,\\nshowing that performance is much more varied despite the smooth\\noverall trend.\\n\\n\\n\\nthis is caused by high variance between individual training\\nruns on sub-tasks (as documented in D’Amour et al. (2020))\\nmasking a steadily improving trend or whether performance\\nis actually non-monotonic as a function of compute on some\\ntasks.\\n\\n\\n\\n3.2.\\n\\nRepresentation Learning\\n\\nWhile we have extensively analyzed the task-learning ca-\\npabilities of CLIP through zero-shot transfer in the previ-\\nous section, it is more common to study the representation\\nlearning capabilities of a model.\\n\\nThere exist many ways to\\nevaluate the quality of representations as well as disagree-\\nments over what properties an “ideal” representation should\\nhave (Locatello et al., 2020).\\n\\nFitting a linear classifier on\\na representation extracted from the model and measuring\\nits performance on various datasets is a common approach.', 'Fitting a linear classifier on\\na representation extracted from the model and measuring\\nits performance on various datasets is a common approach.\\n\\n\\nAn alternative is measuring the performance of end-to-end\\nfine-tuning of the model.\\n\\nThis increases flexibility, and\\nprior work has convincingly demonstrated that fine-tuning\\noutperforms linear classification on most image classifi-\\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\\n\\n\\nWhile the high performance of fine-tuning motivates its\\nstudy for practical reasons, we still opt for linear classifier\\nbased evaluation for several reasons.\\n\\nOur work is focused\\non developing a high-performing task and dataset-agnostic\\npre-training approach.\\n\\nFine-tuning, because it adapts rep-\\nresentations to each dataset during the fine-tuning phase,\\ncan compensate for and potentially mask failures to learn\\ngeneral and robust representations during the pre-training\\nphase.\\n\\nLinear classifiers, because of their limited flexibility,\\ninstead highlight these failures and provide clear feedback\\nduring development.\\n\\nFor CLIP, training supervised linear\\n\\nclassifiers has the added benefit of being very similar to the\\napproach used for its zero-shot classifiers which enables\\nextensive comparisons and analysis in Section 3.1.\\n\\nFinally,\\nwe aim to compare CLIP to a comprehensive set of existing\\nmodels across many tasks.\\n\\nStudying 66 different models on\\n27 different datasets requires tuning 1782 different evalua-\\ntions.\\n\\nFine-tuning opens up a much larger design and hyper-\\nparameter space, which makes it difficult to fairly evaluate\\nand computationally expensive to compare a diverse set of\\ntechniques as discussed in other large scale empirical studies\\n(Lucic et al., 2018; Choi et al., 2019).\\n\\nBy comparison, linear\\nclassifiers require minimal hyper-parameter tuning and have\\nstandardized implementations and evaluation procedures.\\n\\n\\nPlease see Appendix A for further details on evaluation.\\n\\n\\n\\nFigure 10 summarizes our findings.\\n\\nTo minimize selection\\neffects that could raise concerns of confirmation or reporting\\nbias, we first study performance on the 12 dataset evaluation\\nsuite from Kornblith et al. (2019).\\n\\nWhile small CLIP mod-\\nels such as a ResNet-50 and ResNet-101 outperform other\\nResNets trained on ImageNet-1K (BiT-S and the originals),\\nthey underperform ResNets trained on ImageNet-21K (BiT-\\nM).\\n\\nThese small CLIP models also underperform models\\nin the EfficientNet family with similar compute require-\\nments.\\n\\nHowever, models trained with CLIP scale very well\\nand the largest model we trained (ResNet-50x64) slightly\\noutperforms the best performing existing model (a Noisy\\nStudent EfficientNet-L2) on both overall score and compute\\nefficiency.\\n\\nWe also find that CLIP vision transformers are\\nabout 3x more compute efficient than CLIP ResNets, which\\nallows us to reach higher overall performance within our\\ncompute budget.\\n\\nThese results qualitatively replicate the\\nfindings of Dosovitskiy et al.\\n\\n(2020) which reported that\\nvision transformers are more compute efficient than con-\\nvnets when trained on sufficiently large datasets.\\n\\nOur best\\noverall model is a ViT-L/14 that is fine-tuned at a higher res-\\nolution of 336 pixels on our dataset for 1 additional epoch.\\n\\n\\nThis model outperforms the best existing model across this\\nevaluation suite by an average of 2.6%.\\n\\n\\n\\nAs Figure 21 qualitatively shows, CLIP models learn a wider\\nset of tasks than has previously been demonstrated in a sin-\\ngle computer vision model trained end-to-end from random\\ninitialization.\\n\\nThese tasks include geo-localization, optical\\ncharacter recognition, facial emotion recognition, and action\\nrecognition.\\n\\nNone of these tasks are measured in the evalua-\\ntion suite of Kornblith et al. (2019).\\n\\nThis could be argued\\nto be a form of selection bias in Kornblith et al. (2019)’s\\nstudy towards tasks that overlap with ImageNet.\\n\\nTo address\\nthis, we also measure performance on a broader 27 dataset\\nevaluation suite.', \"To address\\nthis, we also measure performance on a broader 27 dataset\\nevaluation suite.\\n\\nThis evaluation suite, detailed in Appendix\\nA includes datasets representing the aforementioned tasks,\\nGerman Traffic Signs Recognition Benchmark (Stallkamp\\net al., 2011), as well as several other datasets adapted from\\nVTAB (Zhai et al., 2019).\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 12\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\n\\nLinear probe average over Kornblith et al.'s 12 datasets\\n\\n100 101 102\\n\\nForward-pass GFLOPs/image\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\nAv\\ner\\n\\nag\\ne \\n\\nSc\\nor\\n\\ne \\n(%\\n\\n)\\n\\nLinear probe average over all 27 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 10.\\n\\nLinear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\\nEfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\\n2020), and the original ResNet models (He et al., 2016b).\\n\\n(Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\\n\\n\\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions.\\n\\nDotted lines indicate models fine-tuned or\\nevaluated on images at a higher-resolution than pre-training.\\n\\nSee Table 10 for individual scores and Figure 20 for plots for each dataset.\\n\\n\\n\\nOn this broader evaluation suite, the benefits of CLIP are\\nmore clear.\\n\\nAll CLIP models, regardless of scale, outper-\\nform all evaluated systems in terms of compute efficiency.\\n\\n\\nThe improvement in average score of the best model over\\nprevious systems increases from 2.6% to 5%.\\n\\nWe also find\\nthat self-supervised systems do noticeably better on our\\nbroader evaluation suite.\\n\\nFor instance, while SimCLRv2\\nstill underperforms BiT-M on average on the 12 datasets\\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\\non our 27 dataset evaluation suite.\\n\\nThese findings suggest\\ncontinuing to expand task diversity and coverage in order\\nto better understand the “general” performance of systems.\\n\\n\\nWe suspect additional evaluation efforts along the lines of\\nVTAB to be valuable.\\n\\n\\n\\nIn addition to the aggregate analysis above, we visualize\\nper-dataset differences in the performance of the best CLIP\\nmodel and the best model in our evaluation suite across\\nall 27 datasets in Figure 11.\\n\\nCLIP outperforms the Noisy\\nStudent EfficientNet-L2 on 21 of the 27 datasets.\\n\\nCLIP\\nimproves the most on tasks which require OCR (SST2\\n\\nand HatefulMemes), geo-localization and scene recognition\\n(Country211, SUN397), and activity recognition in videos\\n(Kinetics700 and UCF101).\\n\\nIn addition CLIP also does\\nmuch better on fine-grained car and traffic sign recognition\\n(Stanford Cars and GTSRB).\\n\\nThis may reflect a problem\\nwith overly narrow supervision in ImageNet.\\n\\nA result such\\nas the 14.7% improvement on GTSRB could be indicative\\nof an issue with ImageNet-1K, which has only a single la-\\nbel for all traffic and street signs.\\n\\nThis could encourage\\na supervised representation to collapse intra-class details\\nand hurt accuracy on a fine-grained downstream task.\\n\\nAs\\nmentioned, CLIP still underperforms the EfficientNet on\\nseveral datasets.\\n\\nUnsurprisingly, the dataset that the Effi-\\ncientNet does best relative to CLIP on is the one it was\\ntrained on: ImageNet.\\n\\nThe EffcientNet also slightly outper-\\nforms CLIP on low-resolution datasets such as CIFAR10\\nand CIFAR100.\\n\\nWe suspect this is at least partly due to the\\nlack of scale-based data augmentation in CLIP.\", 'The EffcientNet also slightly outper-\\nforms CLIP on low-resolution datasets such as CIFAR10\\nand CIFAR100.\\n\\nWe suspect this is at least partly due to the\\nlack of scale-based data augmentation in CLIP.\\n\\nThe Effi-\\ncientNet also does slightly better on PatchCamelyon and\\nCLEVRCounts, datasets where overall performance is still\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 13\\n\\n10 5 0 5 10 15 20 25\\n Score (%)\\n\\nLogistic Regression on CLIP vs. EfficientNet L2 NS\\n\\nImageNet-3.0\\nCLEVRCounts-2.4\\nCIFAR100-1.7\\nPatchCamelyon-1.2\\nCIFAR10-0.8\\nOxfordPets-0.5\\n+0.0STL10\\n+0.5VOC2007\\n+0.5DTD\\n+0.6MNIST\\n+0.9EuroSAT\\n+1.3Caltech101\\n+1.4Flowers102\\n+1.4Birdsnap\\n+2.3KITTI Distance\\n+3.1UCF101\\n+3.2FGVCAircraft\\n+3.9Food101\\n+4.5FER2013\\n+5.1RESISC45\\n\\n+6.2Kinetics700\\n+6.5SUN397\\n\\n+14.7GTSRB\\n+15.9StanfordCars\\n\\n+18.8HatefulMemes\\n+22.7Country211\\n\\n+23.6SST2\\n\\nFigure 11.\\n\\nCLIP’s features outperform the features of the best\\nImageNet model on a wide variety of datasets.\\n\\nFitting a linear\\nclassifier on CLIP’s features outperforms using the Noisy Student\\nEfficientNet-L2 on 21 out of 27 datasets.\\n\\n\\n\\nlow for both approaches.\\n\\n\\n\\n3.3.\\n\\nRobustness to Natural Distribution Shift\\n\\nIn 2015, it was announced that a deep learning model ex-\\nceeded human performance on the ImageNet test set (He\\net al., 2015).\\n\\nHowever, research in the subsequent years\\nhas repeatedly found that these models still make many sim-\\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\\nAlcorn et al., 2019), and new benchmarks testing these sys-\\ntems has often found their performance to be much lower\\nthan both their ImageNet accuracy and human accuracy\\n(Recht et al., 2019; Barbu et al., 2019).\\n\\nWhat explains this\\ndiscrepancy?\\n\\nVarious ideas have been suggested and stud-\\nied (Ilyas et al., 2019; Geirhos et al., 2020).\\n\\nA common\\ntheme of proposed explanations is that deep learning models\\nare exceedingly adept at finding correlations and patterns\\nwhich hold across their training dataset and thus improve\\nin-distribution performance.\\n\\nHowever many of these corre-\\nlations and patterns are actually spurious and do not hold for\\nother distributions and result in large drops in performance\\non other datasets.\\n\\n\\n\\nWe caution that, to date, most of these studies limit their\\nevaluation to models trained on ImageNet.\\n\\nRecalling the\\ntopic of discussion, it may be a mistake to generalize too\\nfar from these initial findings.\\n\\nTo what degree are these\\nfailures attributable to deep learning, ImageNet, or some\\n\\ncombination of the two?\\n\\nCLIP models, which are trained via\\nnatural language supervision on a very large dataset and are\\ncapable of high zero-shot performance, are an opportunity\\nto investigate this question from a different angle.\\n\\n\\n\\nTaori et al. (2020) is a recent comprehensive study mov-\\ning towards quantifying and understanding these behaviors\\nfor ImageNet models.\\n\\nTaori et al. (2020) study how the\\nperformance of ImageNet models change when evaluated\\non natural distribution shifts.\\n\\nThey measure performance\\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\\nand ImageNet Rendition (Hendrycks et al., 2020a).\\n\\nThey\\ndistinguish these datasets, which all consist of novel images\\ncollected from a variety of sources, from synthetic distri-\\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\\nsarial attacks (Goodfellow et al., 2014) which are created by\\nperturbing existing images in various ways.', \"They propose\\nthis distinction because in part because they find that while\\nseveral techniques have been demonstrated to improve per-\\nformance on synthetic distribution shifts, they often fail to\\nyield consistent improvements on natural distributions.3\\n\\nAcross these collected datasets, the accuracy of ImageNet\\nmodels drop well below the expectation set by the Ima-\\ngeNet validation set.\\n\\nFor the following summary discussion\\nwe report average accuracy across all 7 natural distribution\\nshift datasets and average accuracy across the correspond-\\ning class subsets of ImageNet unless otherwise specified.\\n\\n\\nAdditionally, for Youtube-BB and ImageNet-Vid, which\\nhave two different evaluation settings, we use the average\\nof pm-0 and pm-10 accuracy.\\n\\n\\n\\nA ResNet-101 makes 5 times as many mistakes when eval-\\nuated on these natural distribution shifts compared to the\\nImageNet validation set.\\n\\nEncouragingly however, Taori et al.\\n(2020) find that accuracy under distribution shift increases\\npredictably with ImageNet accuracy and is well modeled\\nas a linear function of logit-transformed accuracy.\\n\\nTaori\\net al. (2020) use this finding to propose that robustness\\nanalysis should distinguish between effective and relative\\nrobustness.\\n\\nEffective robustness measures improvements\\nin accuracy under distribution shift above what is predicted\\nby the documented relationship between in-distribution and\\nout-of-distribution accuracy.\\n\\nRelative robustness captures\\nany improvement in out-of-distribution accuracy.\\n\\nTaori et al.\\n(2020) argue that robustness techniques should aim to im-\\nprove both effective robustness and relative robustness.\\n\\n\\n\\nAlmost all models studied in Taori et al. (2020) are trained\\n3We refer readers to Hendrycks et al. (2020a) for additional\\n\\nexperiments and discussion on this claim.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 14\\n\\n65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\\n\\n\\nLinear probe average over Kornblith et al.'s 12 datasets\\n\\n65 70 75 80 85 90\\nImageNet Score (%)\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nTr\\nan\\n\\nsf\\ner\\n\\n S\\nco\\n\\nre\\n (%\\n\\n)\\n\\nLinear probe average over 26 datasets\\n\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\n\\nInstagram\\nSimCLRv2\\nBYOL\\nMoCo\\n\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 12.\\n\\nCLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet.\\n\\nFor both dataset\\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\\nImageNet performance.\\n\\nThis suggests that the representations of models trained on ImageNet are somewhat overfit to their task.\\n\\nor fine-tuned on the ImageNet dataset.\\n\\nReturning to the\\ndiscussion in the introduction to this section - is training\\nor adapting to the ImageNet dataset distribution the cause\\nof the observed robustness gap?\\n\\nIntuitively, a zero-shot\\nmodel should not be able to exploit spurious correlations\\nor patterns that hold only on a specific distribution, since it\\nis not trained on that distribution.\\n\\n4\\n\\nThus it is reasonable\\nto expect zero-shot models to have much higher effective\\nrobustness.\\n\\nIn Figure 13, we compare the performance of\\nzero-shot CLIP with existing ImageNet models on natural\\ndistribution shifts.\\n\\nAll zero-shot CLIP models improve\\neffective robustness by a large amount and reduce the size\\nof the gap between ImageNet accuracy and accuracy under\\ndistribution shift by up to 75%.\\n\\n\\n\\nWhile these results show that zero-shot models can be much\\nmore robust, they do not necessarily mean that supervised\\nlearning on ImageNet causes a robustness gap.\\n\\nOther details\\nof CLIP, such as its large and diverse pre-training dataset\\nor use of natural language supervision could also result\\n\\n4We caution that a zero-shot model can still exploit spurious\\ncorrelations that are shared between the pre-training and evaluation\\ndistributions.\", 'in much more robust models regardless of whether they\\nare zero-shot or fine-tuned.\\n\\nAs an initial experiment to\\npotentially begin narrowing this down, we also measure\\nhow the performance of CLIP models change after adapting\\nto the ImageNet distribution via a L2 regularized logistic\\nregression classifier fit to CLIP features on the ImageNet\\ntraining set.\\n\\nWe visualize how performance changes from\\nthe zero-shot classifier in Figure 14.\\n\\nAlthough adapting\\nCLIP to the ImageNet distribution increases its ImageNet\\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\\nof the 2018 SOTA from Mahajan et al.\\n\\n(2018), average\\naccuracy under distribution shift slightly decreases.\\n\\n\\n\\nIt is surprising to see a 9.2% increase in accuracy, which cor-\\nresponds to roughly 3 years of improvement in SOTA, fail\\nto translate into any improvement in average performance\\nunder distribution shift.\\n\\nWe also break down the differences\\nbetween zero-shot accuracy and linear classifier accuracy\\nper dataset in Figure 14 and find performance still increases\\nsignificantly on one dataset, ImageNetV2.\\n\\nImageNetV2\\nclosely followed the creation process of the original Ima-\\ngeNet dataset which suggests that gains in accuracy from\\nsupervised adaptation are closely concentrated around the\\nImageNet distribution.\\n\\nPerformance decreases by 4.7% on\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 15\\n\\n65 70 75 80 85 90 95 100\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n\\n100\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\n\\n\\nIdeal robust model (y = x)\\nZero-Shot CLIP\\nStandard ImageNet training\\nExisiting robustness techniques ImageNet\\n\\nImageNetV2\\n\\nImageNet-A\\n\\nImageNet-R\\n\\nObjectNet\\n\\nImageNet \\nSketch\\n\\n76.2 76.2\\n\\n64.3 70.1\\n\\n2.7 77.1\\n\\n37.7 88.9\\n\\n32.6 72.3\\n\\n25.2 60.2\\n\\nImageNet\\nResNet101\\n\\nZero-Shot\\nCLIP\\n\\n0%\\n\\n+5.8%\\n\\n+74.4%\\n\\n+51.2%\\n\\n+39.7%\\n\\n+35.0%\\n\\nΔ ScoreDataset Examples\\n\\nFigure 13.\\n\\nZero-shot CLIP is much more robust to distribution shift than standard ImageNet models.\\n\\n(Left) An ideal robust model\\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions.\\n\\nZero-shot CLIP models shrink\\nthis “robustness gap” by up to 75%.\\n\\nLinear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals.\\n\\n\\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets.\\n\\nThe performance of\\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\\nset, ResNet-101.\\n\\nImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\\nand 1.9% on ImageNet-A. The change in accuracy on the\\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\\nsignificant.\\n\\n\\n\\nHow is it possible to improve accuracy by 9.2% on the Im-\\nageNet dataset with little to no increase in accuracy under\\ndistribution shift?\\n\\nIs the gain primarily from “exploiting\\nspurious correlations”?\\n\\nIs this behavior unique to some com-\\nbination of CLIP, the ImageNet datatset, and the distribution\\nshifts studied, or a more general phenomena?\\n\\nDoes it hold\\nfor end-to-end finetuning as well as linear classifiers?\\n\\nWe\\ndo not have confident answers to these questions at this time.\\n\\n\\nPrior work has also pre-trained models on distributions other\\nthan ImageNet, but it is common to study and release mod-\\nels only after they have been fine-tuned to ImageNet.\\n\\nAs a\\nstep towards understanding whether pre-trained zero-shot\\nmodels consistently have higher effective robustness than\\nfine-tuned models, we encourage the authors of Mahajan\\net al.\\n\\n(2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\\n(2020) to, if possible, study these questions on their models\\nas well.\\n\\n\\n\\nWe also investigate another robustness intervention enabled\\nby flexible zero-shot natural-language-based image classi-\\nfiers.', 'We also investigate another robustness intervention enabled\\nby flexible zero-shot natural-language-based image classi-\\nfiers.\\n\\nThe target classes across the 7 transfer datasets are\\nnot always perfectly aligned with those of ImageNet.\\n\\nTwo\\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\\nclasses of ImageNet.\\n\\nThis presents a problem when trying\\nto use the fixed 1000-way classifier of an ImageNet model\\nto make predictions.\\n\\nTaori et al. (2020) handle this by max-\\n\\npooling predictions across all sub-classes according to the\\nImageNet class hierarchy.\\n\\nSometimes this mapping is much\\nless than perfect.\\n\\nFor the person class in Youtube-BB, pre-\\ndictions are made by pooling over the ImageNet classes for\\na baseball player, a bridegroom, and a scuba diver.\\n\\nWith\\nCLIP we can instead generate a custom zero-shot classi-\\nfier for each dataset directly based on its class names.\\n\\nIn\\nFigure 14 we see that this improves average effective ro-\\nbustness by 5% but is concentrated in large improvements\\non only a few datasets.\\n\\nCuriously, accuracy on ObjectNet\\nalso increases by 2.3%.\\n\\nAlthough the dataset was designed\\nto closely overlap with ImageNet classes, using the names\\nprovided for each class by ObjectNet’s creators still helps a\\nsmall amount compared to using ImageNet class names and\\npooling predictions when necessary.\\n\\n\\n\\nWhile zero-shot CLIP improves effective robustness, Figure\\n14 shows that the benefit is almost entirely gone in a fully\\nsupervised setting.\\n\\nTo better understand this difference, we\\ninvestigate how effective robustness changes on the contin-\\nuum from zero-shot to fully supervised.\\n\\nIn Figure 15 we\\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\\n..., 128-shot, and fully supervised logistic regression classi-\\nfiers on the best CLIP model’s features.\\n\\nWe see that while\\nfew-shot models also show higher effective robustness than\\nexisting models, this benefit fades as in-distribution per-\\nformance increases with more training data and is mostly,\\nthough not entirely, gone for the fully supervised model.\\n\\n\\nAdditionally, zero-shot CLIP is notably more robust than\\na few-shot model with equivalent ImageNet performance.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 16\\n\\n70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\n\\n\\nAdapt to class shift\\n\\nAdapt to ImageNet\\n\\nIdeal robust model (y = x)\\nAdaptive Zero-Shot CLIP\\nImageNet Zero-Shot CLIP\\nLogistic Regression CLIP\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\n\\n10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\nImageNet-R-4.7\\nObjectNet-3.8\\nImageNet Sketch-2.8\\nImageNet-A-1.9\\nImageNet Vid-0.5\\n+0.6Youtube-BB\\n\\n+5.8ImageNetV2\\n+9.2ImageNet\\n\\nAdapt to ImageNet\\n\\n10 5 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)\\n\\n0ImageNet\\n0ImageNetV2\\n0ImageNet-A\\n0ImageNet-R\\n0ImageNet Sketch\\n\\n+2.3ObjectNet\\n+8.3ImageNet Vid\\n\\n+26.9Youtube-BB\\nAdapt to class shift\\n\\nFigure 14.\\n\\nWhile supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\\n\\n\\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier\\nand pooling predictions across similar classes as in Taori et al.\\n\\n(2020).\\n\\nCLIP models adapted to ImageNet have similar effective robustness\\nas the best prior ImageNet models.\\n\\n(Right) Details of per dataset changes in accuracy for the two robustness interventions.\\n\\nAdapting to\\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions.\\n\\nDataset specific zero-shot\\nclassifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\\nwith ImageNet categories.', 'Dataset specific zero-shot\\nclassifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\\nwith ImageNet categories.\\n\\n\\n\\nAcross our experiments, high effective robustness seems to\\nresult from minimizing the amount of distribution specific\\ntraining data a model has access to, but this comes at a cost\\nof reducing dataset-specific performance.\\n\\n\\n\\nTaken together, these results suggest that the recent shift\\ntowards large-scale task and dataset agnostic pre-training\\ncombined with a reorientation towards zero-shot and few-\\nshot benchmarking on broad evaluation suites (as advocated\\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\\ndevelopment of more robust systems and provides a more\\naccurate assessment of performance.\\n\\nWe are curious to see\\nif the same results hold for zero-shot models in the field\\nof NLP such as the GPT family.\\n\\nWhile Hendrycks et al.\\n(2020b) has reported that pre-training improves relative ro-\\nbustness on sentiment analysis, Miller et al.\\n\\n(2020)’s\\n\\nstudy\\nof the robustness of question answering models under nat-\\nural distribution shift finds, similar to Taori et al. (2020),\\nlittle evidence of effective robustness improvements to date.\\n\\n\\n\\n4.\\n\\nComparison to Human Performance\\nHow does CLIP compare to human performance and human\\nlearning?\\n\\nTo get a better understanding of how well humans\\nperform in similar evaluation settings to CLIP, we evaluated\\n\\nhumans on one of our tasks.\\n\\nWe wanted to get a sense of\\nhow strong human zero-shot performance is at these tasks,\\nand how much human performance is improved if they are\\nshown one or two image samples.\\n\\nThis can help us to\\ncompare task difficulty for humans and CLIP, and identify\\ncorrelations and differences between them.\\n\\n\\n\\nWe had five different humans look at each of 3669 images\\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\\n2012) and select which of the 37 cat or dog breeds best\\nmatched the image (or ‘I don’t know’ if they were com-\\npletely uncertain).\\n\\nIn the zero-shot case the humans were\\ngiven no examples of the breeds and asked to label them\\nto the best of their ability without an internet search.\\n\\nIn\\nthe one-shot experiment the humans were given one sample\\nimage of each breed and in the two-shot experiment they\\nwere given two sample images of each breed.5\\n\\nOne possible concern was that the human workers were not\\nsufficiently motivated in the zero-shot task.\\n\\nHigh human\\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)\\n\\n5There is not a perfect correspondence between the human\\nfew-shot tasks and the model’s few-shot performance since the\\nmodel cannot refer to sample images in the way that the humans\\ncan.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 17\\n\\n65 70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nAv\\ner\\n\\nag\\ne \\n\\non\\n 7\\n\\n n\\nat\\n\\nur\\nal\\n\\n d\\nist\\n\\nrib\\nut\\n\\nio\\nn \\n\\nsh\\nift\\n\\n d\\nat\\n\\nas\\net\\n\\ns (\\nto\\n\\np-\\n1,\\n\\n %\\n)\\n\\n1 shot\\n\\n2 shot\\n\\n4 shot\\n\\n8 shot\\n\\n16 shot\\n\\n32\\n64\\n\\n128\\nall0 shot\\n\\nIdeal robust model (y = x)\\nFew-Shot CLIP (best model)\\n\\n\\nZero-Shot CLIP (best model)\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\n\\nFigure 15.\\n\\nFew-shot CLIP also increases effective robustness\\ncompared to existing ImageNet models but is less robust than\\nzero-shot CLIP.\\n\\nMinimizing the amount of ImageNet training\\ndata used for adaption increases effective robustness at the cost of\\ndecreasing relative robustness.\\n\\n16-shot logistic regression CLIP\\nmatches zero-shot CLIP on ImageNet, as previously reported in\\nFigure 7, but is less robust.\\n\\nand 97-100% accuracy on the subset of attention check\\nimages increased our trust in the human workers.\\n\\n\\n\\nInterestingly, humans went from a performance average of\\n54% to 76% with just one training example per class, and\\nthe marginal gain from an additional training example is\\nminimal.', 'Interestingly, humans went from a performance average of\\n54% to 76% with just one training example per class, and\\nthe marginal gain from an additional training example is\\nminimal.\\n\\nThe gain in accuracy going from zero to one shot\\nis almost entirely on images that humans were uncertain\\nabout.\\n\\nThis suggests that humans “know what they don’t\\nknow” and are able to update their priors on the images they\\nare most uncertain in based on a single example.\\n\\nGiven this,\\nit seems that while CLIP is a promising training strategy\\nfor zero-shot performance (Figure 5) and does well on tests\\nof natural distribution shift (Figure 13), there is a large\\ndifference between how humans learn from a few examples\\nand the few-shot methods in this paper.\\n\\n\\n\\nThis suggests that there are still algorithmic improvements\\nwaiting to be made to decrease the gap between machine\\nand human sample efficiency, as noted by Lake et al. (2016)\\nand others.\\n\\nBecause these few-shot evaluations of CLIP\\ndon’t make effective use of prior knowledge and the humans\\ndo, we speculate that finding a method to properly integrate\\nprior knowledge into few-shot learning is an important step\\nin algorithmic improvements to CLIP.\\n\\nTo our knowledge,\\nusing a linear classifier on top of the features of a high-\\n\\nAccuracy Majority Vote\\non Full Dataset\\n\\nAccuracy\\non Guesses\\n\\nMajority Vote\\nAccuracy\\n\\non Guesses\\n\\nZero-shot human 53.7 57.0 69.7 63.9\\nZero-shot CLIP 93.5 93.5 93.5 93.5\\nOne-shot human 75.7 80.3 78.5 81.2\\nTwo-shot human 75.7 85.0 79.2 86.1\\n\\nTable 2.\\n\\nComparison of human performance on Oxford IIT Pets.\\n\\n\\nAs in Parkhi et al. (2012), the metric is average per-class classifica-\\ntion accuracy.\\n\\nMost of the gain in performance when going from\\nthe human zero shot case to the human one shot case is on images\\nthat participants were highly uncertain on.\\n\\n“Guesses” refers to\\nrestricting the dataset to where participants selected an answer\\nother than “I don’t know”, the “majority vote” is taking the most\\nfrequent (exclusive of ties) answer per image.\\n\\n\\n\\nquality pre-trained model is near state-of-the-art for few\\nshot learning (Tian et al., 2020), which suggests that there is\\na gap between the best few-shot machine learning methods\\nand human few-shot learning.\\n\\n\\n\\nIf we plot human accuracy vs CLIP’s zero shot accuracy\\n(Figure 16), we see that the hardest problems for CLIP are\\nalso hard for humans.\\n\\nTo the extent that errors are consistent,\\nour hypothesis is that this is due to at least a two factors:\\nnoise in the dataset (including mislabeled images) and out of\\ndistribution images being hard for both humans and models.\\n\\n\\n\\n5. Data Overlap Analysis\\nA concern with pre-training on a very large internet dataset\\nis unintentional overlap with downstream evals.\\n\\nThis is\\nimportant to investigate since, in a worst-case scenario, a\\ncomplete copy of an evaluation dataset could leak into the\\npre-training dataset and invalidate the evaluation as a mean-\\ningful test of generalization.\\n\\nOne option to prevent this is to\\nidentify and remove all duplicates before training a model.\\n\\n\\nWhile this guarantees reporting true hold-out performance,\\nit requires knowing all possible data which a model might\\nbe evaluated on ahead of time.\\n\\nThis has the downside of\\nlimiting the scope of benchmarking and analysis.\\n\\nAdding a\\nnew evaluation would require an expensive re-train or risk\\nreporting an un-quantified benefit due to overlap.\\n\\n\\n\\nInstead, we document how much overlap occurs and how\\nperformance changes due to these overlaps.\\n\\nTo do this, we\\nuse the following procedure:\\n\\n1) For each evaluation dataset, we run a duplicate detector\\n(see Appendix C) on its examples.\\n\\nWe then manually inspect\\nthe found nearest neighbors and set a per dataset threshold\\nto keep high precision while maximizing recall.', 'We then manually inspect\\nthe found nearest neighbors and set a per dataset threshold\\nto keep high precision while maximizing recall.\\n\\nUsing\\nthis threshold, we then create two new subsets, Overlap,\\nwhich contains all examples which have a similarity to a\\ntraining example above the threshold, and Clean, which\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 18\\n\\npu\\ng\\n\\nsp\\nhy\\n\\nnx\\nge\\n\\nrm\\nan\\n\\n_s\\nho\\n\\nrth\\nai\\n\\nre\\nd\\n\\nsh\\nib\\n\\na_\\nin\\n\\nu\\nbe\\n\\nag\\nle\\n\\ngr\\nea\\n\\nt_\\npy\\n\\nre\\nne\\n\\nes\\nen\\n\\ngl\\nish\\n\\n_s\\net\\n\\nte\\nr\\n\\nsa\\nm\\n\\noy\\ned\\n\\nsa\\nin\\n\\nt_\\nbe\\n\\nrn\\nar\\n\\nd\\npo\\n\\nm\\ner\\n\\nan\\nia\\n\\nn\\nne\\n\\nwf\\nou\\n\\nnd\\nla\\n\\nnd\\nwh\\n\\nea\\nte\\n\\nn_\\nte\\n\\nrri\\ner\\n\\nsc\\not\\n\\ntis\\nh_\\n\\nte\\nrri\\n\\ner\\nyo\\n\\nrk\\nsh\\n\\nire\\n_t\\n\\ner\\nrie\\n\\nr\\nsia\\n\\nm\\nes\\n\\ne\\nm\\n\\nin\\nia\\n\\ntu\\nre\\n\\n_p\\nin\\n\\nsc\\nhe\\n\\nr\\nha\\n\\nva\\nne\\n\\nse\\nke\\n\\nes\\nho\\n\\nnd\\nbo\\n\\nm\\nba\\n\\ny\\nm\\n\\nai\\nne\\n\\n_c\\noo\\n\\nn\\nch\\n\\nih\\nua\\n\\nhu\\na\\n\\nba\\nss\\n\\net\\n_h\\n\\nou\\nnd\\n\\nja\\npa\\n\\nne\\nse\\n\\n_c\\nhi\\n\\nn\\nru\\n\\nss\\nia\\n\\nn_\\nbl\\n\\nue\\nam\\n\\ner\\nica\\n\\nn_\\nbu\\n\\nlld\\nog\\n\\npe\\nrs\\n\\nia\\nn\\n\\nbe\\nng\\n\\nal\\nle\\n\\non\\nbe\\n\\nrg\\ner\\n\\nab\\nys\\n\\nsin\\nia\\n\\nn\\nbo\\n\\nxe\\nr\\n\\nbr\\niti\\n\\nsh\\n_s\\n\\nho\\nrth\\n\\nai\\nr\\n\\nst\\naf\\n\\nfo\\nrd\\n\\nsh\\nire\\n\\n_b\\nul\\n\\nl_t\\ner\\n\\nrie\\nr\\n\\nam\\ner\\n\\nica\\nn_\\n\\npi\\nt_\\n\\nbu\\nll_\\n\\nte\\nrri\\n\\ner\\neg\\n\\nyp\\ntia\\n\\nn_\\nm\\n\\nau\\nbi\\n\\nrm\\nan\\n\\nen\\ngl\\n\\nish\\n_c\\n\\noc\\nke\\n\\nr_\\nsp\\n\\nan\\nie\\n\\nl\\nra\\n\\ngd\\nol\\n\\nl\\n\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n (%\\n)\\n\\nZero-Shot CLIP\\nOne-Shot Human\\nZero-Shot Human\\n\\nFigure 16.\\n\\nThe hardest problems for CLIP also tend to be the hard-\\nest problems for humans.\\n\\nHere we rank image categories by diffi-\\nculty for CLIP as measured as probability of the correct label.\\n\\n\\n\\ncontains all examples that are below this threshold.\\n\\nWe\\ndenote the unaltered full dataset All for reference.\\n\\nFrom\\nthis we first record the degree of data contamination as the\\nratio of the number of examples in Overlap to the size of\\nAll.\\n\\n2)\\n\\nWe then compute the zero-shot accuracy of CLIP\\nRN50x64 on the three splits and report All - Clean\\nas our main metric.\\n\\nThis is the difference in accuracy due\\nto contamination.\\n\\nWhen positive it is our estimate of how\\nmuch the overall reported accuracy on the dataset was in-\\nflated by over-fitting to overlapping data.\\n\\n3)\\n\\nThe amount of overlap is often small so we also run a\\nbinomial significance test where we use the accuracy on\\nClean as the null hypothesis and compute the one-tailed\\n(greater) p-value for the Overlap subset.\\n\\nWe also calculate\\n99.5% Clopper-Pearson confidence intervals on Dirty as\\nanother check.\\n\\n\\n\\nA summary of this analysis is presented in Figure 17.\\n\\nOut\\nof 35 datasets studied, 9 datasets have no detected overlap\\nat all.\\n\\nMost of these datasets are synthetic or specialized\\nmaking them unlikely to be posted as normal images on\\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\\nare guaranteed to have no overlap due to containing novel\\ndata from after the date our dataset was created (ObjectNet\\nand Hateful Memes).\\n\\nThis demonstrates our detector has\\na low-false positive rate which is important as false posi-\\ntives would under-estimate the effect of contamination in\\n\\nour analysis.\\n\\nThere is a median overlap of 2.2% and an av-\\nerage overlap of 3.2%.\\n\\nDue to this small amount of overlap,\\noverall accuracy is rarely shifted by more than 0.1% with\\nonly 7 datasets above this threshold.\\n\\nOf these, only 2 are\\nstatistically significant after Bonferroni correction.\\n\\nThe max\\ndetected improvement is only 0.6% on Birdsnap which has\\nthe second largest overlap at 12.1%.\\n\\nThe largest overlap is\\nfor Country211 at 21.5%.\\n\\nThis is due to it being constructed\\nout of YFCC100M, which our pre-training dataset contains\\na filtered subset of.\\n\\nDespite this large overlap there is only\\na 0.2% increase in accuracy on Country211.\\n\\nThis may be\\nbecause the training text accompanying an example is often\\nnot related to the specific task a downstream eval measures.\\n\\n\\nCountry211 measures geo-localization ability, but inspect-\\ning the training text for these duplicates showed they often\\ndo not mention the location of the image.\\n\\n\\n\\nWe are aware of two potential concerns with our analysis.\\n\\n\\nFirst our detector is not perfect.', 'We are aware of two potential concerns with our analysis.\\n\\n\\nFirst our detector is not perfect.\\n\\nWhile it achieves near\\n100% accuracy on its proxy training task and manual in-\\nspection + threshold tuning results in very high precision\\nwith good recall among the found nearest-neighbors, we can\\nnot tractably check its recall across 400 million examples.\\n\\n\\nAnother potential confounder of our analysis is that the un-\\nderlying data distribution may shift between the Overlap\\nand Clean subsets.\\n\\nFor example, on Kinetics-700 many\\n“overlaps” are in fact all black transition frames.\\n\\nThis ex-\\nplains why Kinetics-700 has an apparent 20% accuracy drop\\non Overlap.\\n\\nWe suspect more subtle distribution shifts\\nlikely exist.\\n\\nOne possibility we noticed on CIFAR-100 is\\nthat, due to the very low resolution of its images, many\\nduplicates were false positives of small objects such as birds\\nor planes.\\n\\nChanges in accuracy could instead be due to\\nchanges in the class distribution or difficulty of the dupli-\\ncates.\\n\\nUnfortunately, these distribution and difficulty shifts\\ncould also mask the effects of over-fitting.\\n\\n\\n\\nHowever, these results closely follow the findings of simi-\\nlar duplicate analysis in previous work on large scale pre-\\ntraining.\\n\\nMahajan et al. (2018) and Kolesnikov et al.\\n\\n(2019)\\ndetected similar overlap rates and found minimal changes in\\noverall performance.\\n\\nImportantly, Kolesnikov et al. (2019)\\nalso compared the alternative de-duplication strategy dis-\\ncussed in the introduction to this section with the approach\\nwe settled on and observed little difference between the two\\napproaches.\\n\\n\\n\\n6.\\n\\nLimitations\\nThere are still many limitations to CLIP.\\n\\nWhile several of\\nthese are discussed as part of analysis in various sections,\\nwe summarize and collect them here.\\n\\n\\n\\nOn datasets with training splits, the performance of zero-\\nshot CLIP is on average competitive with the simple su-\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 19\\n\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-20\\n\\n-10\\n\\n0\\n\\n10\\n\\n20\\n\\nDi\\nffe\\n\\nre\\nnc\\n\\ne \\nin\\n\\n A\\ncc\\n\\nur\\nac\\n\\ny \\non\\n\\n O\\nve\\n\\nrla\\npp\\n\\nin\\ng \\n\\nvs\\n.\\n\\nC\\n\\nle\\nan\\n\\n D\\nat\\n\\na \\n(%\\n\\n)\\n\\nSUN397\\n\\nCIFAR-100\\n\\nImageNet Sketch\\n\\nSUN\\n\\nKinetics-700\\n\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)\\n\\n-0.75\\n\\n-0.5\\n\\n-0.25\\n\\n0\\n\\n0.25\\n\\n0.5\\n\\n0.75\\n\\nOv\\ner\\n\\nal\\nl A\\n\\ncc\\nur\\n\\nac\\ny \\n\\nCh\\nan\\n\\nge\\n D\\n\\nue\\n T\\n\\no \\nOv\\n\\ner\\nla\\n\\np \\n(%\\n\\n)\\n\\nStanford CarsSUN397\\n\\nBirdsnap\\nCIFAR-100\\n\\nFER2013\\n\\nCountry211\\nSUN\\n\\np <\\n\\n1e-3\\np < 0.05\\np > 0.05\\n\\nFigure 17.\\n\\nFew statistically significant improvements in accuracy due to detected data overlap.\\n\\n(Left) While several datasets have\\nup to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\\n99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference.\\n\\n2 of these datasets do worse on overlapping data.\\n\\n\\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap.\\n\\nSimilarly, for only 6 datasets are the accuracy\\nimprovements statistically significant when calculated using a one-sided binomial test.\\n\\n\\n\\npervised baseline of a linear classifier on top of ResNet-50\\nfeatures.\\n\\nOn most of these datasets, the performance of\\nthis baseline is now well below the overall state of the art.\\n\\n\\nSignificant work is still needed to improve the task learning\\nand transfer capabilities of CLIP.\\n\\nWhile scaling has so far\\nsteadily improved performance and suggests a route for con-\\ntinued improvement, we estimate around a 1000x increase\\nin compute is required for zero-shot CLIP to reach overall\\nstate-of-the-art performance.\\n\\nThis is infeasible to train with\\ncurrent hardware.\\n\\nFurther research into improving upon the\\ncomputational and data efficiency of CLIP will be necessary.', 'This is infeasible to train with\\ncurrent hardware.\\n\\nFurther research into improving upon the\\ncomputational and data efficiency of CLIP will be necessary.\\n\\n\\n\\nAnalysis in Section 3.1 found that CLIP’s zero-shot perfor-\\nmance is still quite weak on several kinds of tasks.\\n\\nWhen\\ncompared to task-specific models, the performance of CLIP\\nis poor on several types of fine-grained classification such\\nas differentiating models of cars, species of flowers, and\\nvariants of aircraft.\\n\\nCLIP also struggles with more abstract\\nand systematic tasks such as counting the number of objects\\nin an image.\\n\\nFinally for novel tasks which are unlikely to be\\nincluded in CLIP’s pre-training dataset, such as classifying\\nthe distance to the nearest car in a photo, CLIP’s perfor-\\nmance can be near random.\\n\\nWe are confident that there are\\nstill many, many, tasks where CLIP’s zero-shot performance\\nis near chance level.\\n\\n\\n\\nWhile zero-shot CLIP generalizes well to many natural im-\\nage distributions as investigated in Section 3.3, we’ve ob-\\nserved that zero-shot CLIP still generalizes poorly to data\\nthat is truly out-of-distribution for it.\\n\\nAn illustrative exam-\\nple occurs for the task of OCR as reported in Appendix E.\\n\\nCLIP learns a high quality semantic OCR representation that\\nperforms well on digitally rendered text, which is common\\nin its pre-training dataset, as evidenced by performance on\\nRendered SST2.\\n\\nHowever, CLIP only achieves 88% accu-\\nracy on the handwritten digits of MNIST.\\n\\nAn embarrassingly\\nsimple baseline of logistic regression on raw pixels outper-\\nforms zero-shot CLIP.\\n\\nBoth semantic and near-duplicate\\nnearest-neighbor retrieval verify that there are almost no im-\\nages that resemble MNIST digits in our pre-training dataset.\\n\\n\\nThis suggests CLIP does little to address the underlying\\nproblem of brittle generalization of deep learning models.\\n\\n\\nInstead CLIP tries to circumvent the problem and hopes that\\nby training on such a large and varied dataset that all data\\nwill be effectively in-distribution.\\n\\nThis is a naive assumption\\nthat, as MNIST demonstrates, is easy to violate.\\n\\n\\n\\nAlthough CLIP can flexibly generate zero-shot classifiers\\nfor a wide variety of tasks and datasets, CLIP is still limited\\nto choosing from only those concepts in a given zero-shot\\nclassifier.\\n\\nThis is a significant restriction compared to a\\ntruly flexible approach like image captioning which could\\ngenerate novel outputs.\\n\\nUnfortunately, as described in Sec-\\ntion 2.3 we found the computational efficiency of the image\\ncaption baseline we tried to be much lower than CLIP.\\n\\nA\\nsimple idea worth trying is joint training of a contrastive\\nand generative objective with the hope of combining the\\nefficiency of CLIP with the flexibility of a caption model.\\n\\n\\nAs another alternative, search could be performed at infer-\\nence time over many natural language explanations of a\\ngiven image, similar to approach proposed in Learning with\\nLatent Language Andreas et al. (2017).\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 20\\n\\nCLIP also does not address the poor data efficiency of deep\\nlearning.\\n\\nInstead CLIP compensates by using a source of\\nsupervision that can be scaled to hundreds of millions of\\ntraining examples.\\n\\nIf every image seen during training of\\na CLIP model was presented at a rate of one per second,\\nit would take 405 years to iterate through the 12.8 billion\\nimages seen over 32 training epochs.\\n\\nCombining CLIP\\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\\nself-training (Lee; Xie et al., 2020) methods is a promising\\ndirection given their demonstrated ability to improve data\\nefficiency over standard supervised learning.\\n\\n\\n\\nOur methodology has several significant limitations.\\n\\nDe-\\nspite our focus on zero-shot transfer, we repeatedly queried\\nperformance on full validation sets to guide the develop-\\nment of CLIP.\\n\\nThese validation sets often have thousands\\nof examples, which is unrealistic for true zero-shot sce-\\nnarios.', 'These validation sets often have thousands\\nof examples, which is unrealistic for true zero-shot sce-\\nnarios.\\n\\nSimilar concerns have been raised in the field of\\nsemi-supervised learning (Oliver et al., 2018).\\n\\nAnother po-\\ntential issue is our selection of evaluation datasets.\\n\\nWhile\\nwe have reported results on Kornblith et al. (2019)’s 12\\ndataset evaluation suite as a standardized collection, our\\nmain results use a somewhat haphazardly assembled col-\\nlection of 27 datasets that is undeniably co-adapted with\\nthe development and capabilities of CLIP.\\n\\nCreating a new\\nbenchmark of tasks designed explicitly to evaluate broad\\nzero-shot transfer capabilities, rather than re-using existing\\nsupervised datasets, would help address these issues.\\n\\n\\n\\nCLIP is trained on text paired with images on the internet.\\n\\n\\nThese image-text pairs are unfiltered and uncurated and\\nresult in CLIP models learning many social biases.\\n\\nThis\\nhas been previously demonstrated for image caption models\\n(Bhargava & Forsyth, 2019).\\n\\nWe refer readers to Section 7\\nfor detailed analysis and quantification of these behaviors for\\nCLIP as well as discussion of potential mitigation strategies.\\n\\n\\n\\nWhile we have emphasized throughout this work that speci-\\nfying image classifiers through natural language is a flexible\\nand general interface, it has its own limitations.\\n\\nMany com-\\nplex tasks and visual concepts can be difficult to specify\\njust through text.\\n\\nActual training examples are undeniably\\nuseful but CLIP does not optimize for few-shot performance\\ndirectly.\\n\\nIn our work, we fall back to fitting linear classifiers\\non top of CLIP’s features.\\n\\nThis results in a counter-intuitive\\ndrop in performance when transitioning from a zero-shot\\nto a few-shot setting.\\n\\nAs discussed in Section 4, this is\\nnotably different from human performance which shows a\\nlarge increase from a zero to a one shot setting.\\n\\nFuture work\\nis needed to develop methods that combine CLIP’s strong\\nzero-shot performance with efficient few-shot learning.\\n\\n\\n\\n7.\\n\\nBroader Impacts\\nCLIP has a wide range of capabilities due to its ability to\\ncarry out arbitrary image classification tasks.\\n\\nOne can give\\nit images of cats and dogs and ask it to classify cats, or give\\nit images taken in a department store and ask it to classify\\nshoplifters–a task with significant social implications and\\nfor which AI may be unfit.\\n\\nLike any image classification\\nsystem, CLIP’s performance and fitness for purpose need to\\nbe evaluated, and its broader impacts analyzed in context.\\n\\n\\nCLIP also introduces a capability that will magnify and alter\\nsuch issues: CLIP makes it possible to easily create your\\nown classes for categorization (to ‘roll your own classifier’)\\n\\n\\nwithout a need for re-training.\\n\\nThis capability introduces\\nchallenges similar to those found in characterizing other,\\nlarge-scale generative models like GPT-3 (Brown et al.,\\n2020); models that exhibit non-trivial zero-shot (or few-\\nshot) generalization can have a vast range of capabilities,\\nmany of which are made clear only after testing for them.\\n\\n\\n\\nOur studies of CLIP in a zero-shot setting show that the\\nmodel displays significant promise for widely-applicable\\ntasks like image retrieval or search.\\n\\nFor example, it can find\\nrelevant images in a database given text, or relevant text\\ngiven an image.\\n\\nFurther, the relative ease of steering CLIP\\ntoward bespoke applications with little or no additional data\\nor training could unlock a variety of novel applications that\\nare hard for us to envision today, as has occurred with large\\nlanguage models over the past few years.\\n\\n\\n\\nIn addition to the more than 30 datasets studied in earlier\\nsections of this paper, we evaluate CLIP’s performance on\\nthe FairFace benchmark and undertake exploratory bias\\nprobes.\\n\\nWe then characterize the model’s performance in\\na downstream task, surveillance, and discuss its usefulness\\nas compared with other available systems.', 'We then characterize the model’s performance in\\na downstream task, surveillance, and discuss its usefulness\\nas compared with other available systems.\\n\\nMany of CLIP’s\\ncapabilities are omni-use in nature (e.g. OCR can be used\\nto make scanned documents searchable, to power screen\\nreading technologies, or to read license plates).\\n\\nSeveral\\nof the capabilities measured, from action recognition, ob-\\nject classification, and geo-localization, to facial emotion\\nrecognition, can be used in surveillance.\\n\\nGiven its social\\nimplications, we address this domain of use specifically in\\nthe Surveillance section.\\n\\n\\n\\nWe have also sought to characterize the social biases inher-\\nent to the model.\\n\\nOur bias tests represent our initial efforts\\nto probe aspects of how the model responds in different sce-\\nnarios, and are by nature limited in scope.\\n\\nCLIP and models\\nlike it will need to be analyzed in relation to their specific\\ndeployments to understand how bias manifests and iden-\\ntify potential interventions.\\n\\nFurther community exploration\\nwill be required to develop broader, more contextual, and\\nmore robust testing schemes so that AI developers can bet-\\nter characterize biases in general purpose computer vision\\nmodels.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 21\\n\\nModel Race Gender Age\\n\\nFairFace Model 93.7\\n\\n94.2 59.7\\nLinear Probe CLIP 93.4 96.5 63.8\\nZero-Shot CLIP 58.3 95.9 57.1\\nLinear Probe Instagram 90.8 93.2 54.2\\n\\nTable 3.\\n\\nPercent accuracy on Race, Gender, and Age classification\\nof images in FairFace category ‘White’\\n\\nModel Race Gender Age\\n\\nFairFace Model 75.4 94.4 60.7\\nLinear Probe CLIP 92.8 97.7 63.1\\nZero-Shot CLIP 91.3 97.2 54.3\\nLinear Probe Instagram 87.2 93.9 54.1\\n\\nTable 4.\\n\\nPercent accuracy on Race, Gender, and Age classification\\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\\ngether as FairFace category ‘Non-White’)\\n\\nMiddle Southeast East\\nModel Gender Black White Indian Latino Eastern Asian Asian Average\\n\\nMale 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\\nLinear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\\n\\n97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5\\n\\n\\n\\nMale 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\\nZero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\\n\\n96.7 95.9 98.0 97.5\\n\\n98.0 96.3 96.6\\n\\nMale 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\\nLinear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\\n\\n91.3 93.2 95.6 94.0 95.6 93.4 93.9\\n\\nTable 5.\\n\\nPercent accuracy on gender classification of images by FairFace race category\\n\\n7.1.\\n\\nBias\\n\\nAlgorithmic decisions, training data, and choices about how\\nclasses are defined and taxonomized (which we refer to in-\\nformally as “class design”) can all contribute to and amplify\\nsocial biases and inequalities resulting from the use of AI\\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\\n& Star, 2000).\\n\\nClass design is particularly relevant to mod-\\nels like CLIP, since any developer can define a class and the\\nmodel will provide some result.\\n\\n\\n\\nIn this section, we provide preliminary analysis of some\\nof the biases in CLIP, using bias probes inspired by those\\noutlined in Buolamwini & Gebru (2018) and Kärkkäinen\\n& Joo (2019).\\n\\nWe also conduct exploratory bias research\\nintended to find specific examples of biases in the model,\\nsimilar to that conducted by Solaiman et al. (2019).\\n\\n\\n\\nWe start by analyzing the performance of Zero-Shot CLIP on\\nthe face image dataset FairFace (Kärkkäinen & Joo, 2019)6\\n\\n6FairFace is a face image dataset designed to balance age, gen-\\nder, and race, in order to reduce asymmetries common in previous\\nface datasets.\\n\\nIt categorizes gender into 2 groups: female and male\\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\\nAsian, Middle Eastern, and Latino.', 'It categorizes gender into 2 groups: female and male\\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\\nAsian, Middle Eastern, and Latino.\\n\\nThere are inherent problems\\nwith race and gender classifications, as e.g. Bowker & Star (2000)\\n\\nas an initial bias probe, then probe the model further to\\nsurface additional biases and sources of biases, including\\nclass design.\\n\\n\\n\\nWe evaluated two versions of CLIP on the FairFace dataset:\\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\\nsion classifier fitted to FairFace’s dataset on top of CLIP’s\\nfeatures (“LR CLIP”).\\n\\nWe find that LR CLIP gets higher\\naccuracy on the FairFace dataset than both the ResNext-101\\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\\nhajan et al., 2018) and FairFace’s own model on most of the\\nclassification tests we ran7.\\n\\nZS CLIP’s performance varies\\nby category and is worse than that of FairFace’s model for a\\nfew categories, and better for others.\\n\\n(See Table 3 and Table\\n4).\\n\\nand Keyes (2018) have shown.\\n\\nWhile FairFace’s dataset reduces\\nthe proportion of White faces, it still lacks representation of entire\\nlarge demographic groups, effectively erasing such categories.\\n\\nWe\\nuse the 2 gender categories and 7 race categories defined in the\\nFairFace dataset in a number of our experiments not in order to\\nreinforce or endorse the use of such reductive categories, but in\\norder to enable us to make comparisons to prior work.\\n\\n\\n\\n7One challenge with this comparison is that the FairFace model\\nuses binary classes for race (“White” and “Non-White”), instead\\nof breaking down races into finer-grained sub-groups.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 22\\n\\nMiddle Southeast East\\nCategory Black White Indian Latino Eastern Asian Asian\\n\\nCrime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\\nNon-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0\\n\\nTable 6.\\n\\nPercent of images classified into crime-related and non-human categories by FairFace Race category.\\n\\nThe label set included 7\\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\\n\\n\\n\\nCategory Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\\n\\nDefault Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\\nDefault Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4\\n\\nTable 7.\\n\\nPercent of images classified into crime-related and non-human categories by FairFace Age category, showing comparison between\\nresults obtained using a default label set and a label set to which the label ’child’ has been added.\\n\\nThe default label set included 7 FairFace\\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\\n\\n\\n\\nAdditionally, we test the performance of the LR CLIP and\\nZS CLIP models across intersectional race and gender cate-\\ngories as they are defined in the FairFace dataset.\\n\\nWe find\\nthat model performance on gender classification is above\\n95% for all race categories.\\n\\nTable 5 summarizes these re-\\nsults.\\n\\n\\n\\nWhile LR CLIP achieves higher accuracy than the Linear\\nProbe Instagram model on the FairFace benchmark dataset\\nfor gender, race and age classification of images by intersec-\\ntional categories, accuracy on benchmarks offers only one\\napproximation of algorithmic fairness, as Raji et al. (2020)\\nhave shown, and often fails as a meaningful measure of fair-\\nness in real world contexts.\\n\\nEven if a model has both higher\\naccuracy and lower disparities in performance on different\\nsub-groups, this does not mean it will have lower disparities\\nin impact (Scheuerman et al., 2019).\\n\\nFor example, higher\\nperformance on underrepresented groups might be used by\\na company to justify their use of facial recognition, and to\\nthen deploy it ways that affect demographic groups dispro-\\nportionately.', 'Our use of facial classification benchmarks to\\nprobe for biases is not intended to imply that facial classi-\\nfication is an unproblematic task, nor to endorse the use of\\nrace, age, or gender classification in deployed contexts.\\n\\n\\n\\nWe also probed the model using classification terms with\\nhigh potential to cause representational harm, focusing on\\ndenigration harms in particular (Crawford, 2017).\\n\\nWe car-\\nried out an experiment in which the ZS CLIP model was\\nrequired to classify 10,000 images from the FairFace dataset.\\n\\n\\nIn addition to the FairFace classes, we added in the follow-\\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\\n‘thief’, ‘criminal’ and ‘suspicious person’.\\n\\nThe goal of this\\nexperiment was to check if harms of denigration dispropor-\\ntionately impact certain demographic subgroups.\\n\\n\\n\\nWe found that 4.9% (confidence intervals between 4.6%\\nand 5.4%) of the images were misclassified into one of\\nthe non-human classes we used in our probes (‘animal’,\\n‘chimpanzee’, ‘gorilla’, ‘orangutan’).\\n\\nOut of these, ‘Black’\\nimages had the highest misclassification rate (approximately\\n14%; confidence intervals between [12.6% and 16.4%])\\nwhile all other races had misclassification rates under 8%.\\n\\n\\nPeople aged 0-20 years had the highest proportion being\\nclassified into this category at 14% .\\n\\n\\n\\nWe also found that 16.5% of male images were misclassified\\ninto classes related to crime (‘thief’, ‘suspicious person’ and\\n‘criminal’) as compared to 9.8% of female images.\\n\\nInter-\\nestingly, we found that people aged 0-20 years old were\\nmore likely to fall under these crime-related classes (approx-\\nimately 18%) compared to images of people in different\\nage ranges (approximately 12% for people aged 20-60 and\\n0% for people over 70).\\n\\nWe found significant disparities in\\nclassifications across races for crime related terms, which is\\ncaptured in Table 6.\\n\\nGiven that we observed that people under 20 were the most\\nlikely to be classified in both the crime-related and non-\\nhuman animal categories, we carried out classification for\\nthe images with the same classes but with an additional\\ncategory ‘child’ added to the categories.\\n\\nOur goal here\\nwas to see if this category would significantly change the\\nbehaviour of the model and shift how the denigration harms\\nare distributed by age.\\n\\nWe found that this drastically reduced\\nthe number of images of people under 20 classified in either\\ncrime-related categories or non-human animal categories\\n(Table 7).\\n\\nThis points to how class design has the potential\\nto be a key factor determining both the model performance\\nand the unwanted biases or behaviour the model may exhibit\\nwhile also asks overarching questions about the use of face\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 23\\n\\nimages to automatically classify people along such lines\\n(y Arcas et al., 2017).\\n\\n\\n\\nThe results of these probes can change based on the class\\ncategories one chooses to include as well as the specific\\nlanguage one uses to describe each class.\\n\\nPoor class design\\ncan lead to poor real world performance; this concern is\\nparticularly relevant to a model like CLIP, given how easily\\ndevelopers can design their own classes.\\n\\n\\n\\nWe also carried out experiments similar to those outlined by\\nSchwemmer et al. (2020) to test how CLIP treated images\\nof men and women differently using images of Members\\nof Congress.\\n\\nAs part of these experiments, we studied\\nhow certain additional design decisions such as deciding\\nthresholds for labels can impact the labels output by CLIP\\nand how biases manifest.\\n\\n\\n\\nWe carried out three experiments - we tested for accuracy\\non gender classification and we tested for how labels were\\ndifferentially distributed across two different label sets.\\n\\nFor\\nour first label set, we used a label set of 300 occupations and\\nfor our second label set we used a combined set of labels that\\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\\nAzure Computer Vision returned for all the images.', 'We first simply looked into gender prediction performance\\nof the model on the images of Members of Congress, in\\norder to check to see if the model correctly recognized\\nmen as men and women as women given the image of a\\nperson who appeared to be in an official setting/position of\\npower.\\n\\nWe found that the model got 100% accuracy on the\\nimages.\\n\\nThis is slightly better performance than the model’s\\nperformance on the FairFace dataset.\\n\\nWe hypothesize that\\none of the reasons for this is that all the images in the\\nMembers of Congress dataset were high-quality and clear,\\nwith the people clearly centered, unlike those in the FairFace\\ndataset.\\n\\n\\n\\nIn order to study how the biases in returned labels depend on\\nthe thresholds set for label probability, we did an experiment\\nin which we set threshold values at 0.5% and 4.0%.\\n\\nWe\\nfound that the lower threshold led to lower quality of labels.\\n\\n\\nHowever, even the differing distributions of labels under\\nthis threshold can hold signals for bias.\\n\\nFor example, we\\nfind that under the 0.5% threshold labels such as ‘nanny’\\nand ‘housekeeper’ start appearing for women whereas labels\\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\\n\\n\\nThis points to gendered associations similar to those that\\nhave previously been found for occupations (Schwemmer\\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\\n\\n\\n\\nAt the higher 4% threshold, the labels with the highest prob-\\nability across both genders include “lawmaker”, “legislator”\\nand “congressman”.\\n\\nHowever, the presence of these biases\\namongst lower probability labels nonetheless point to larger\\nquestions about what ‘sufficiently’ safe behaviour may look\\n\\nlike for deploying such systems.\\n\\n\\n\\nWhen given the combined set of labels that Google Cloud\\nVision (GCV), Amazon Rekognition and Microsoft returned\\nfor all the images, similar to the biases Schwemmer et al.\\n(2020) found in GCV systems, we found our system also\\ndisproportionately attached labels to do with hair and ap-\\npearance in general to women more than men.\\n\\nFor ex-\\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\\nappeared significantly more often for women.\\n\\nAdditionally,\\nCLIP attached some labels that described high status occu-\\npations disproportionately more often to men such as ‘ex-\\necutive’ and ‘doctor’.\\n\\nOut of the only four occupations that\\nit attached more often to women, three were ‘newscaster’,\\n‘television presenter’ and ‘newsreader’ and the fourth was\\n‘Judge’.\\n\\nThis is again similar to the biases found in GCV\\nand points to historical gendered differences (Schwemmer\\net al., 2020).\\n\\n\\n\\nInterestingly, when we lowered the threshold to 0.5% for\\nthis set of labels, we found that the labels disproportionately\\ndescribing men also shifted to appearance oriented words\\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18).\\n\\nMany occupa-\\ntion oriented words such as ‘military person’ and ‘executive’\\n- which were not used to describe images of women at the\\nhigher 4% threshold - were used for both men and women\\nat the lower 0.5% threshold, which could have caused the\\nchange in labels for men.\\n\\nThe reverse was not true.\\n\\nDescrip-\\ntive words used to describe women were still uncommon\\namongst men.\\n\\n\\n\\nDesign decisions at every stage of building a model impact\\nhow biases manifest and this is especially true for CLIP\\ngiven the flexibility it offers.\\n\\nIn addition to choices about\\ntraining data and model architecture, decisions about things\\nlike class designs and thresholding values can alter the labels\\na model outputs and as a result heighten or lower certain\\nkinds of harm, such as those described by Crawford (2017).\\n\\n\\nPeople designing and developing models and AI systems\\nhave considerable power.\\n\\nDecisions about things like class\\ndesign are a key determiner not only of model performance,\\nbut also of how and in what contexts model biases manifest.\\n\\n\\n\\nThese experiments are not comprehensive.', 'Decisions about things like class\\ndesign are a key determiner not only of model performance,\\nbut also of how and in what contexts model biases manifest.\\n\\n\\n\\nThese experiments are not comprehensive.\\n\\nThey illus-\\ntrate potential issues stemming from class design and other\\nsources of bias, and are intended to spark inquiry.\\n\\n\\n\\n7.2.\\n\\nSurveillance\\n\\nWe next sought to characterize model performance in re-\\nlation to a downstream task for which there is significant\\nsocietal sensitivity: surveillance.\\n\\nOur analysis aims to better\\nembody the characterization approach described above and\\nto help orient the research community towards the potential\\nfuture impacts of increasingly general purpose computer\\nvision models and aid the development of norms and checks\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 24\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nblouse\\npurple\\n\\nnewsreader\\nbangs\\n\\npink\\npixie cut\\n\\nblack hair\\nbob cut\\n\\nmagenta\\nhot\\n\\nlaughing\\nblazer\\n\\nspokesperson\\nblonde\\n\\npublic speaking\\nsenior citizen\\n\\nlooking\\nfemale\\n\\nlady\\nwoman\\n\\nTop labels,\\nimages of women\\n\\nWomen\\nMen\\n\\n0 20 40 60 80 100\\nFrequency (%)\\n\\nyellow\\nnecktie\\n\\nkid\\nfrown\\n\\nshoulder\\ntie\\n\\ndisplay\\nelder\\n\\nphotograph\\nwalking\\n\\nmilitary officer\\nphoto\\n\\nsuit\\nfacial expression\\n\\nhead\\nblack\\n\\nplayer\\nface\\n\\nmale\\nman\\n\\nTop labels,\\nimages of men\\n\\nWomen\\nMen\\n\\nFigure 18.\\n\\nCLIP performance on Member of Congress images when given the combined returned label set for the images from Google\\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision.\\n\\nThe 20 most gendered labels for men and women were\\nidentified with χ2 tests with the threshold at 0.5%.\\n\\nLabels are sorted by absolute frequencies.\\n\\nBars denote the percentage of images for a\\ncertain label by gender.\\n\\naround such systems.\\n\\nOur inclusion of surveillance is not\\nintended to indicate enthusiasm for this domain - rather, we\\nthink surveillance is an important domain to try to make\\npredictions about given its societal implications (Zuboff,\\n2015; Browne, 2015).\\n\\n\\n\\nWe measure the model’s performance on classification of\\nimages from CCTV cameras and zero-shot celebrity identifi-\\ncation.\\n\\nWe first tested model performance on low-resolution\\nimages captured from surveillance cameras (e.g. CCTV\\ncameras).\\n\\nWe used the VIRAT dataset (Oh et al., 2011) and\\ndata captured by Varadarajan & Odobez (2009), which both\\nconsist of real world outdoor scenes with non-actors.\\n\\n\\n\\nGiven CLIP’s flexible class construction, we tested 515\\nsurveillance images captured from 12 different video se-\\nquences on self-constructed general classes for coarse and\\nfine grained classification.\\n\\nCoarse classification required the\\nmodel to correctly identify the main subject of the image (i.e.\\ndetermine if the image was a picture of an empty parking\\nlot, school campus, etc.).\\n\\nFor fine-grained classification, the\\nmodel had to choose between two options constructed to\\ndetermine if the model could identify the presence/absence\\nof smaller features in the image such as a person standing\\nin the corner.\\n\\n\\n\\nFor coarse classification, we constructed the classes by hand-\\ncaptioning the images ourselves to describe the contents\\nof the image and there were always at least 6 options for\\n\\nthe model to choose from.\\n\\nAdditionally, we carried out a\\n‘stress test’ where the class set included at least one more\\ncaption for something that was ‘close’ to the image (for\\nexample, ‘parking lot with white car’ vs. ‘parking lot with\\nred car’).\\n\\nWe found that the model had a top-1 accuracy\\nof 91.8% on the CCTV images for the initial evaluation.\\n\\n\\nThe accuracy dropped significantly to 51.1% for the second\\nevaluation, with the model incorrectly choosing the ‘close’\\nanswer 40.7% of the time.\\n\\n\\n\\nFor fine-grained detection, the zero-shot model performed\\npoorly, with results near random.\\n\\nNote that this experiment\\nwas targeted only towards detecting the presence or absence\\nof small objects in image sequences.', 'Note that this experiment\\nwas targeted only towards detecting the presence or absence\\nof small objects in image sequences.\\n\\n\\n\\nWe also tested CLIP’s zero-shot performance for ‘in the\\nwild’ identity detection using the CelebA dataset8.\\n\\nWe did\\nthis to evaluate the model’s performance for identity detec-\\ntion using just the publicly available data it was pre-trained\\non.\\n\\nWhile we tested this on a dataset of celebrities who have\\na larger number of images on the internet, we hypothesize\\nthat the number of images in the pre-training data needed\\nfor the model to associate faces with names will keep de-\\ncreasing as models get more powerful (see Table 8), which\\nhas significant societal implications (Garvie, 2019).\\n\\nThis\\n\\n8Note: The CelebA dataset is more representative of faces with\\nlighter skin tones.\\n\\nDue to the nature of the dataset, we were not\\nable to control for race, gender, age, etc.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 25\\n\\nModel 100 Classes 1k Classes 2k Classes\\n\\nCLIP L/14\\n\\n59.2 43.3 42.2\\nCLIP RN50x64 56.4 39.5 38.4\\nCLIP RN50x16 52.7 37.4 36.3\\nCLIP RN50x4 52.8 38.1 37.3\\n\\nTable 8.\\n\\nCelebA Zero-Shot Top-1 Identity Recognition Accuracy\\n\\nmirrors recent developments in natural language processing,\\nin which recent large language models trained on Internet\\ndata often exhibit a surprising ability to provide informa-\\ntion related to relatively minor public figures (Brown et al.,\\n2020).\\n\\n\\n\\nWe found that the model had 59.2% top-1 accuracy out\\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\\nages.\\n\\nHowever, this performance dropped to 43.3% when\\nwe increased our class sizes to 1k celebrity names.\\n\\nThis\\nperformance is not competitive when compared to produc-\\ntion level models such as Google’s Celebrity Recognition\\n(Google).\\n\\nHowever, what makes these results noteworthy is\\nthat this analysis was done using only zero-shot identifica-\\ntion capabilities based on names inferred from pre-training\\ndata - we didn’t use any additional task-specific dataset, and\\nso the (relatively) strong results further indicate that before\\ndeploying multimodal models, people will need to carefully\\nstudy them for behaviors in a given context and domain.\\n\\n\\n\\nCLIP offers significant benefit for tasks that have relatively\\nlittle data given its zero-shot capabilities.\\n\\nHowever, large\\ndatasets and high performing supervised models exist for\\nmany in-demand surveillance tasks such as facial recogni-\\ntion.\\n\\nAs a result, CLIP’s comparative appeal for such uses\\nis low.\\n\\nAdditionally, CLIP is not designed for common\\nsurveillance-relevant tasks like object detection and seman-\\ntic segmentation.\\n\\nThis means it has limited use for certain\\nsurveillance tasks when models that are designed with these\\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\\navailable.\\n\\n\\n\\nHowever, CLIP does unlock a certain aspect of usability\\ngiven how it removes the need for training data.\\n\\nThus, CLIP\\nand similar models could enable bespoke, niche surveillance\\nuse cases for which no well-tailored models or datasets exist,\\nand could lower the skill requirements to build such appli-\\ncations.\\n\\nAs our experiments show, ZS CLIP displays non-\\ntrivial, but not exceptional, performance on a few surveil-\\nlance relevant tasks today.\\n\\n\\n\\n7.3.\\n\\nFuture Work\\n\\nThis preliminary analysis is intended to illustrate some of\\nthe challenges that general purpose computer vision models\\npose and to give a glimpse into their biases and impacts.\\n\\n\\n\\nWe hope that this work motivates future research on the\\ncharacterization of the capabilities, shortcomings, and biases\\nof such models, and we are excited to engage with the\\nresearch community on such questions.\\n\\n\\n\\nWe believe one good step forward is community exploration\\nto further characterize the capabilities of models like CLIP\\nand - crucially - identify application areas where they have\\npromising performance and areas where they may have\\nreduced performance9.', 'This process of characterization can\\nhelp researchers increase the likelihood models are used\\nbeneficially by:\\n\\n• Identifying potentially beneficial downstream uses of\\nmodels early in the research process, enabling other\\nresearchers to think about applications.\\n\\n\\n\\n•\\n\\nSurfacing tasks with significant sensitivity and a large\\nset of societal stakeholders, which may call for inter-\\nvention by policymakers.\\n\\n\\n\\n• Better characterizing biases in models, alerting other\\nresearchers to areas of concern and areas for interven-\\ntions.\\n\\n\\n\\n•\\n\\nCreating suites of tests to evaluate systems like CLIP\\non, so we can better characterize model capabilities\\nearlier in the development cycle.\\n\\n\\n\\n• Identifying potential failure modes and areas for further\\nwork.\\n\\n\\n\\nWe plan to contribute to this work, and hope this analysis\\nprovides some motivating examples for subsequent research.\\n\\n\\n\\n8.\\n\\nRelated Work\\nAny model that leverages written, spoken, signed or any\\nother form of human language as part of its training signal\\nis arguably using natural language as a source of supervi-\\nsion.\\n\\nThis is an admittedly extremely broad area and covers\\nmost work in the field of distributional semantics including\\ntopic models (Blei et al., 2003), word, sentence, and para-\\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\\nMikolov, 2014), and language models (Bengio et al., 2003).\\n\\n\\nIt also includes much of the broader field of NLP that deals\\nwith predicting or modeling sequences of natural language\\nin some way.\\n\\nWork in NLP intentionally leveraging natural\\nlanguage supervision in the form of explanations, feedback,\\ninstructions, and advice for tasks such as classification (as\\nopposed to the commonly used representation of supervision\\nas a set of arbitrarily encoded discrete category labels) has\\n\\n9A model could be unfit for use due to inadequate performance\\nor due to the inappropriateness of AI use in the application area\\nitself.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 26\\n\\nbeen explored in many creative and advanced ways.\\n\\nDialog\\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\\n2019) develops techniques to learn from interactive natural\\nlanguage feedback in dialog.\\n\\nSeveral papers have leveraged\\nsemantic parsing to convert natural language explanations\\ninto features (Srivastava et al., 2017) or additional training\\nlabels (Hancock et al., 2018).\\n\\nMore recently, ExpBERT\\n(Murty et al., 2020) uses feature representations produced\\nby conditioning a deep contextual language model on nat-\\nural language explanations and descriptions of relations to\\nimprove performance on the task of relation extraction.\\n\\n\\n\\nCLIP is an example of using natural language as a training\\nsignal for learning about a domain other than language.\\n\\nIn\\nthis context, the earliest use of the term natural language\\nsupervision that we are aware of is the work of Ramanathan\\net al.\\n\\n(2013) which showed that natural language descrip-\\ntions could be used along side other sources of supervision\\nto improve performance on the task of video event under-\\nstanding.\\n\\nHowever, as mentioned in the introduction and\\napproach section, methods of leveraging natural language\\ndescriptions in computer vision well predate the use of this\\nspecific term, especially for image retrieval (Mori et al.,\\n1999) and object classification (Wang et al., 2009).\\n\\nOther\\nearly work leveraged tags (but not natural language) asso-\\nciated with images for the task of semantic segmentation\\n(Barnard et al., 2003).\\n\\nMore recently, He & Peng (2017)\\nand Liang et al. (2020) demonstrated using natural language\\ndescriptions and explanations to improve fine-grained vi-\\nsual classification of birds.\\n\\nOthers have investigated how\\ngrounded language can be used to improve visual represen-\\ntations and classifiers on the ShapeWorld dataset (Kuhnle\\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).', 'Finally, techniques which combine natural language with\\nreinforcement learning environments (Narasimhan et al.,\\n2015) have demonstrated exciting emergent behaviors such\\nas systematically accomplishing zero-shot tasks (Hill et al.,\\n2019).\\n\\n\\n\\nCLIP’s pre-training task optimizes for text-image retrieval.\\n\\n\\nThis areas of research dates back to the mid-90s with the\\npreviously mentioned Mori et al.\\n\\n(1999) as representative of\\nearly work.\\n\\nWhile initial efforts focused primarily on predic-\\ntive objectives over time research shifted towards learning\\njoint multi-modal embedding spaces with techniques like\\nkernel Canonical Correlation Analysis and various ranking\\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\\nHodosh et al., 2013).\\n\\nOver time work explored many combi-\\nnations of training objective, transfer, and more expressive\\nmodels and steadily improved performance (Frome et al.,\\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\\n2014; Faghri et al., 2017).\\n\\n\\n\\nOther work has leveraged natural language supervision for\\ndomains other than images.\\n\\nStroud et al. (2020) explores\\n\\nlarge scale representation learning by training a system to\\npair descriptive text with videos instead of images.\\n\\nSeveral\\nworks have explored using dense spoken natural language\\nsupervision for videos (Miech et al., 2019; 2020b).\\n\\nWhen\\nconsidered together with CLIP, these works suggest that\\nlarge scale natural language supervision is a promising way\\nto learn high quality perceptual systems for many domains.\\n\\n\\nAlayrac et al.\\n\\n(2020) extended this line of work to an addi-\\ntional modality by adding raw audio as an additional super-\\nvision source and demonstrated benefits from combining all\\nthree sources of supervision.\\n\\n\\n\\nAs part of our work on CLIP we also construct a new dataset\\nof image-text pairs.\\n\\nModern work on image-text retrieval\\nhas relied on a set of crowd-sourced sentence level im-\\nage caption evaluation datasets like Pascal1K (Rashtchian\\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\\n(Young et al., 2014).\\n\\nHowever, these datasets are still rel-\\natively small and limit achievable performance.\\n\\nSeveral\\nmethods have been proposed to create larger datasets au-\\ntomatically with Ordonez et al. (2011) as a notable early\\nexample.\\n\\nIn the deep learning era, Mithun et al. (2018)\\ndemonstrated an additional set of (image, text) pairs col-\\nlected from the internet could improve retrieval performance\\nand several new automatically constructed datasets such as\\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\\n2020), and OCR-CC (Yang et al., 2020) have been created.\\n\\n\\nHowever, these datasets still use significantly more aggres-\\nsive filtering or are designed for a specific task such as OCR\\nand as a result are still much smaller than WIT with between\\n1 and 10 million training examples.\\n\\n\\n\\nA related idea to CLIP is webly supervised learning.\\n\\nThis\\nline of work queries image search engines to build image\\ndatasets by querying for terms and uses the queries as the\\nlabels for the returned images (Fergus et al., 2005).\\n\\nClassi-\\nfiers trained on these large but noisily labeled datasets can\\nbe competitive with those trained on smaller carefully la-\\nbeled datasets.\\n\\nThese image-query pairs are also often used\\nto improve performance on standard datasets as additional\\ntraining data (Chen & Gupta, 2015).\\n\\nCLIP also uses search\\nqueries as part of its dataset creation process.\\n\\nHowever\\nCLIP only uses full text sequences co-occuring with images\\nas supervision rather than just the queries, which are often\\nonly a single word or short n-gram.\\n\\nWe also restrict this step\\nin CLIP to text only querying for sub-string matches while\\nmost webly supervised work uses standard image search\\nengines which have their own complex retrieval and filter-\\ning pipelines that often involve computer vision systems.', 'Of this line of work, Learning Everything about Anything:\\nWebly-Supervised Visual Concept Learning (Divvala et al.,\\n2014) has a notably similar ambition and goal as CLIP.\\n\\n\\n\\nFinally, CLIP is related to a recent burst of activity on learn-\\ning joint models of vision and language (Lu et al., 2019; Tan\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 27\\n\\n& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\\n2020).\\n\\nThis line of work focuses on richly connecting vision\\nand language in order to solve complex downstream tasks\\nsuch as visual question answering, visual commonsense\\nreasoning, or multimodal entailment.\\n\\nThese approaches\\nleverage impressively engineered models which combine 3\\n(or more) pre-trained subsystems, typically an image feature\\nmodel, a region proposal / object detection model, and a\\npre-trained masked language model such as BERT.\\n\\nThese\\nsystems are then jointly fine-tuned via various training objec-\\ntives on image-text pairs and applied to the aforementioned\\ntasks and achieve impressive results.\\n\\nCLIP is instead fo-\\ncused on learning visual models from scratch via natural\\nlanguage supervision and does not densely connect the two\\ndomains with a joint attention model.\\n\\nThe only interaction\\nin a CLIP model between the image and text domain is a\\nsingle dot product in a learned joint embedding space.\\n\\nWe\\nare excited to see CLIP hybridized with this line of work.\\n\\n\\n\\n9.\\n\\nConclusion\\nWe have investigated whether it is possible to transfer the\\nsuccess of task-agnostic web-scale pre-training in NLP to\\nanother domain.\\n\\nWe find that adopting this formula re-\\nsults in similar behaviors emerging in the field of computer\\nvision and discuss the social implications of this line of\\nresearch.\\n\\nIn order to optimize their training objective, CLIP\\nmodels learn to perform a wide variety of tasks during pre-\\ntraining.\\n\\nThis task learning can then be leveraged via natural\\nlanguage prompting to enable zero-shot transfer to many\\nexisting datasets.\\n\\nAt sufficient scale, the performance of this\\napproach can be competitive with task-specific supervised\\nmodels although there is still room for much improvement.\\n\\n\\n\\nACKNOWLEDGMENTS\\n\\nWe’d like to thank the millions of people involved in creating\\nthe data CLIP is trained on.\\n\\nWe’d also like to thank Susan\\nZhang for her work on image conditional language models\\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\\nGillian Hadfield for their thoughtful feedback on the broader\\nimpacts section of the paper.\\n\\nWe are also grateful to the\\nAcceleration and Supercomputing teams at OpenAI for their\\ncritical work on software and hardware infrastructure this\\nproject used.\\n\\nFinally, we’d also like to thank the developers\\nof the many software packages used throughout this project\\nincluding, but not limited, to Numpy (Harris et al., 2020),\\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\\npandas (pandas development team, 2020), and scikit-learn\\n(Pedregosa et al., 2011).\\n\\n\\n\\nReferences\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\\n\\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\\nTensorflow: A system for large-scale machine learning.\\n\\nIn\\n12th {USENIX} symposium on operating systems design\\nand implementation ({OSDI} 16), pp. 265–283, 2016.\\n\\n\\n\\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelović,\\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\\nand Zisserman, A. Self-supervised multimodal versatile\\nnetworks.\\n\\narXiv preprint arXiv:2006.16228, 2020.\\n\\n\\n\\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\\nS., and Nguyen, A. Strike (with) a pose: Neural networks\\nare easily fooled by strange poses of familiar objects.\\n\\nIn\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp.\\n\\n4845–4854, 2019.\\n\\n\\n\\nAndreas, J., Klein, D., and Levine, S. Learning with latent\\nlanguage.', 'In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp.\\n\\n4845–4854, 2019.\\n\\n\\n\\nAndreas, J., Klein, D., and Levine, S. Learning with latent\\nlanguage.\\n\\narXiv preprint arXiv:1711.00482, 2017.\\n\\n\\n\\nAssiri, Y. Stochastic optimization of plain convolutional\\nneural networks with simple methods.\\n\\narXiv preprint\\narXiv:2001.08856, 2020.\\n\\n\\n\\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning\\nrepresentations by maximizing mutual information across\\nviews.\\n\\nIn Advances in Neural Information Processing\\nSystems, pp. 15535–15545, 2019.\\n\\n\\n\\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\\nlarge-scale bias-controlled dataset for pushing the lim-\\nits of object recognition models.\\n\\nIn Advances in Neural\\nInformation Processing Systems, pp.\\n\\n9453–9463, 2019.\\n\\n\\n\\nBarnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\\nD. M., and Jordan, M. I. Matching words and pictures.\\n\\n\\nJournal of machine learning research, 3(Feb):1107–1135,\\n2003.\\n\\n\\n\\nBechmann, A. and Bowker, G. C. Unsupervised by any\\nother name: Hidden layers of knowledge production in\\nartificial intelligence on social media.\\n\\nBig Data & Society,\\n6(1):205395171881956, January 2019.\\n\\ndoi: 10.1177/\\n2053951718819569.\\n\\nURL https://doi.org/10.\\n\\n\\n1177/2053951718819569.\\n\\n\\n\\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\\nneural probabilistic language model.\\n\\nJournal of machine\\nlearning research, 3(Feb):1137–1155, 2003.\\n\\n\\n\\nBhargava, S. and Forsyth, D. Exposing and correcting the\\ngender bias in image captioning datasets and models.\\narXiv preprint arXiv:1912.00578, 2019.\\n\\n\\n\\nhttps://doi.org/10.1177/2053951718819569\\nhttps://doi.org/10.1177/2053951718819569\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 28\\n\\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\\nallocation.\\n\\nJournal of machine Learning research, 3(Jan):\\n993–1022, 2003.\\n\\n\\n\\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\\nKalai, A. T. Man is to computer programmer as woman\\nis to homemaker?\\n\\ndebiasing word embeddings.\\n\\nAdvances\\nin neural information processing systems, 29:4349–4357,\\n2016.\\n\\n\\n\\nBowker, G. C. and Star, S. L. Sorting things out: Classifica-\\ntion and its consequences.\\n\\nMIT press, 2000.\\n\\n\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al.\\n\\nLanguage models are few-shot learners.\\n\\n\\narXiv preprint arXiv:2005.14165, 2020.\\n\\n\\n\\nBrowne, S. Dark Matters: Surveillance of Blackness.\\n\\nDuke\\nUniversity Press, 2015.\\n\\n\\n\\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\\nvisual representations with caption annotations.\\n\\narXiv\\ne-prints, pp.\\n\\narXiv–2008, 2020.\\n\\n\\n\\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\\ntional accuracy disparities in commercial gender classi-\\nfication.\\n\\nIn Conference on fairness, accountability and\\ntransparency, pp.\\n\\n77–91, 2018.\\n\\n\\n\\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\\nshort note on the kinetics-700 human action dataset.\\n\\narXiv\\npreprint arXiv:1907.06987, 2019.\\n\\n\\n\\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\\nD., and Sutskever, I. Generative pretraining from pixels.\\n\\n\\nIn International Conference on Machine Learning, pp.\\n\\n\\n1691–1703.\\n\\nPMLR, 2020a.\\n\\n\\n\\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\\ndeep nets with sublinear memory cost.\\n\\narXiv preprint\\narXiv:1604.06174, 2016.\\n\\n\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\\nsimple framework for contrastive learning of visual rep-\\nresentations.\\n\\narXiv preprint arXiv:2002.05709, 2020b.\\n\\n\\n\\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\\nHinton, G. Big self-supervised models are strong semi-\\nsupervised learners.\\n\\narXiv preprint arXiv:2006.10029,\\n2020c.\\n\\n\\n\\nChen, X. and Gupta, A. Webly supervised learning of\\nconvolutional networks.\\n\\nIn Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 1431–\\n1439, 2015.', 'Chen, X. and Gupta, A. Webly supervised learning of\\nconvolutional networks.\\n\\nIn Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 1431–\\n1439, 2015.\\n\\n\\n\\nChen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning.\\n\\narXiv\\npreprint arXiv:2003.04297, 2020d.\\n\\n\\n\\nChen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\\nCheng, Y., and Liu, J. Uniter: Learning universal image-\\ntext representations.\\n\\narXiv preprint arXiv:1909.11740,\\n2019.\\n\\n\\n\\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\\nclassification: Benchmark and state of the art.\\n\\nProceed-\\nings of the IEEE, 105(10):1865–1883, 2017.\\n\\n\\n\\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\\nand Dahl, G. E. On empirical comparisons of optimiz-\\ners for deep learning.\\n\\narXiv preprint arXiv:1910.05446,\\n2019.\\n\\n\\n\\nCoates, A., Ng, A., and Lee, H. An analysis of single-\\nlayer networks in unsupervised feature learning.\\n\\nIn Pro-\\nceedings of the fourteenth international conference on\\nartificial intelligence and statistics, pp.\\n\\n215–223, 2011.\\n\\n\\n\\nCrawford, K. The trouble with bias.\\n\\nNIPS 2017\\nKeynote, 2017.\\n\\nURL https://www.youtube.com/\\nwatch?v=fMym_BKWQzk.\\n\\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning.\\n\\n\\nIn Advances in neural information processing systems,\\npp.\\n\\n3079–3087, 2015.\\n\\nD’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al.\\n\\nUnderspecification presents\\nchallenges for credibility in modern machine learning.\\n\\n\\narXiv preprint arXiv:2011.03395, 2020.\\n\\n\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\\nFei, L. ImageNet: A Large-Scale Hierarchical Image\\nDatabase.\\n\\nIn CVPR09, 2009.\\n\\n\\n\\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\\nand Fei-Fei, L. Ilsvrc 2012, 2012.\\n\\nURL http://www.\\n\\n\\nimage-net.org/challenges/LSVRC/2012/.\\n\\nDesai, K. and Johnson, J. Virtex: Learning visual rep-\\nresentations from textual annotations.\\n\\narXiv preprint\\narXiv:2006.06666, 2020.\\n\\n\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding.\\n\\narXiv preprint arXiv:1810.04805,\\n2018.\\n\\n\\n\\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\\nand Sutskever, I. Jukebox:\\n\\nA generative model for music.\\narXiv preprint arXiv:2005.00341, 2020.\\n\\n\\n\\nhttps://www.youtube.com/watch?v=fMym_BKWQzk\\nhttps://www.youtube.com/watch?v=fMym_BKWQzk\\nhttp://www.image-net.org/challenges/LSVRC/2012/\\nhttp://www.image-net.org/challenges/LSVRC/2012/\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 29\\n\\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning\\neverything about anything: Webly-supervised visual con-\\ncept learning.\\n\\nIn Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 3270–\\n3277, 2014.\\n\\n\\n\\nDodge, S. and Karam, L.\\n\\nA study and comparison of human\\nand deep learning recognition performance under visual\\ndistortions.\\n\\nIn 2017 26th international conference on\\ncomputer communication and networks (ICCCN), pp. 1–\\n7. IEEE, 2017.\\n\\n\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al.\\n\\nAn image is worth 16x16\\nwords: Transformers for image recognition at scale.\\n\\narXiv\\npreprint arXiv:2010.11929, 2020.\\n\\n\\n\\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\\nfier: Zero-shot learning using purely textual descriptions.\\n\\n\\nIn Proceedings of the IEEE International Conference on\\nComputer Vision, pp.\\n\\n2584–2591, 2013.\\n\\n\\n\\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\\nproving visual-semantic embeddings with hard negatives.\\n\\n\\narXiv preprint arXiv:1707.05612, 2017.\\n\\n\\n\\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\\ning object categories from google’s image search.\\n\\nIn\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1, volume 2, pp.\\n\\n1816–1823.\\n\\nIEEE,\\n2005.', 'In\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1, volume 2, pp.\\n\\n1816–1823.\\n\\nIEEE,\\n2005.\\n\\n\\n\\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\\nRanzato, M., and Mikolov, T. Devise: A deep visual-\\nsemantic embedding model.\\n\\nIn Advances in neural infor-\\nmation processing systems, pp.\\n\\n2121–2129, 2013.\\n\\n\\n\\nGan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\\nLarge-scale adversarial training for vision-and-language\\nrepresentation learning.\\n\\narXiv preprint arXiv:2006.06195,\\n2020.\\n\\n\\n\\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\\nguage models better few-shot learners.\\n\\narXiv preprint\\narXiv:2012.15723, 2020.\\n\\n\\n\\nGarvie, C., May 2019.\\n\\nURL https://www.\\nflawedfacedata.com/.\\n\\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for\\nautonomous driving?\\n\\nthe kitti vision benchmark suite.\\n\\nIn\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), 2012.\\n\\n\\n\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\\nmann, F. A., and Brendel, W. Imagenet-trained cnns are\\n\\nbiased towards texture; increasing shape bias improves ac-\\ncuracy and robustness.\\n\\narXiv preprint arXiv:1811.12231,\\n2018.\\n\\n\\n\\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\\ncut learning in deep neural networks.\\n\\narXiv preprint\\narXiv:2004.07780, 2020.\\n\\nGomez, L., Patel, Y., Rusiñol, M., Karatzas, D., and Jawahar,\\nC. Self-supervised learning of visual features through\\nembedding images into text topic spaces.\\n\\nIn Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp.\\n\\n4230–4239, 2017.\\n\\n\\n\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\\ning and harnessing adversarial examples.\\n\\narXiv preprint\\narXiv:1412.6572, 2014.\\n\\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\\nD., Lee, D.-H., et al.\\n\\nChallenges in representation learn-\\ning: A report on three machine learning contests.\\n\\nNeural\\nNetworks, 64:59–63, 2015.\\n\\n\\n\\nGoogle.\\n\\nGoogle cloud api: Celebrity recognition.\\n\\nURL\\nhttps://cloud.google.com/vision/docs/\\ncelebrity-recognition.\\n\\n\\n\\nGriewank, A. and Walther, A. Algorithm 799: revolve: an\\nimplementation of checkpointing for the reverse or ad-\\njoint mode of computational differentiation.\\n\\nACM Trans-\\nactions on Mathematical Software (TOMS), 26(1):19–45,\\n2000.\\n\\n\\n\\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,\\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\\nZ. D., Azar, M. G., et al.\\n\\nBootstrap your own latent: A\\nnew approach to self-supervised learning.\\n\\narXiv preprint\\narXiv:2006.07733, 2020.\\n\\n\\n\\nHa, D., Dai, A., and Le, Q. V. Hypernetworks.\\n\\narXiv\\npreprint arXiv:1609.09106, 2016.\\n\\n\\n\\nHancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\\nS., and Ré, C. Training classifiers with natural language\\nexplanations.\\n\\nIn Proceedings of the conference.\\n\\nAssoci-\\nation for Computational Linguistics.\\n\\nMeeting, volume\\n2018, pp. 1884.\\n\\nNIH Public Access, 2018.\\n\\n\\n\\nHancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\\nLearning from dialogue after deployment: Feed yourself,\\nchatbot!\\n\\narXiv preprint arXiv:1901.05415, 2019.\\n\\n\\n\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\\nKerkwijk, M. H., Brett, M., Haldane, A., Fernández del\\n\\nhttps://www.flawedfacedata.com/\\nhttps://www.flawedfacedata.com/\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\nhttps://cloud.google.com/vision/docs/celebrity-recognition\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 30\\n\\nRı́o, J., Wiebe, M., Peterson, P., Gérard-Marchant, P.,\\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\\nGohlke, C., and Oliphant, T. E. Array programming\\nwith NumPy.\\n\\nNature, 585:357–362, 2020.\\n\\ndoi:\\n\\n10.1038/\\ns41586-020-2649-2.\\n\\nHays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image.', 'Nature, 585:357–362, 2020.\\n\\ndoi:\\n\\n10.1038/\\ns41586-020-2649-2.\\n\\nHays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image.\\n\\nIn 2008 ieee confer-\\nence on computer vision and pattern recognition, pp.\\n\\n1–8.\\n\\n\\nIEEE, 2008.\\n\\n\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\\ninto rectifiers: Surpassing human-level performance on\\nimagenet classification.\\n\\nIn Proceedings of the IEEE inter-\\nnational conference on computer vision, pp. 1026–1034,\\n2015.\\n\\n\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition.\\n\\nIn Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp.\\n\\n770–778, 2016a.\\n\\n\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition.\\n\\nIn Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp.\\n\\n770–778, 2016b.\\n\\n\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning.\\n\\nIn Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pp. 9729–\\n9738, 2020.\\n\\n\\n\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks.\\n\\nIn Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pp. 558–\\n567, 2019.\\n\\n\\n\\nHe, X. and Peng, Y. Fine-grained image classification via\\ncombining vision and language.\\n\\nIn Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition, pp.\\n\\n5994–6002, 2017.\\n\\n\\n\\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\\nA novel dataset and deep learning benchmark for land\\nuse and land cover classification.\\n\\nIEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote\\nSensing, 12(7):2217–2226, 2019.\\n\\n\\n\\nHenaff, O. Data-efficient image recognition with contrastive\\npredictive coding.\\n\\nIn International Conference on Ma-\\nchine Learning, pp. 4182–4192.\\n\\nPMLR, 2020.\\n\\n\\n\\nHendrycks, D. and Dietterich, T. Benchmarking neural\\nnetwork robustness to common corruptions and perturba-\\ntions.\\n\\narXiv preprint arXiv:1903.12261, 2019.\\n\\n\\n\\nHendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus).\\n\\narXiv preprint arXiv:1606.08415, 2016.\\n\\n\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\\nSong, D. Natural adversarial examples.\\n\\narXiv preprint\\narXiv:1907.07174, 2019.\\n\\n\\n\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\\net al.\\n\\nThe many faces of robustness: A critical analy-\\nsis of out-of-distribution generalization.\\n\\narXiv preprint\\narXiv:2006.16241, 2020a.\\n\\n\\n\\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\\nR., and Song, D. Pretrained transformers improve out-of-\\ndistribution robustness.\\n\\narXiv preprint arXiv:2004.06100,\\n2020b.\\n\\n\\n\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\\nY. Deep learning scaling is predictable, empirically.\\n\\narXiv\\npreprint arXiv:1712.00409, 2017.\\n\\n\\n\\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\\nM., McClelland, J. L., and Santoro, A. Environmental\\ndrivers of systematicity and generalization in a situated\\nagent.\\n\\nIn International Conference on Learning Repre-\\nsentations, 2019.\\n\\n\\n\\nHodosh, M., Young, P., and Hockenmaier, J. Framing image\\ndescription as a ranking task: Data, models and evaluation\\nmetrics.\\n\\nJournal of Artificial Intelligence Research, 47:\\n853–899, 2013.\\n\\n\\n\\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\\n\\n\\nEnhancing image geolocalization by combinatorial parti-\\ntioning of maps.\\n\\nIn Proceedings of the European Confer-\\nence on Computer Vision (ECCV), pp.\\n\\n536–551, 2018.\\n\\n\\n\\nHoward, J. and Ruder, S. Universal language model\\nfine-tuning for text classification.\\n\\narXiv preprint\\narXiv:1801.06146, 2018.\\n\\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\\nB., and Madry, A. Adversarial examples are not bugs,\\nthey are features.', 'arXiv preprint\\narXiv:1801.06146, 2018.\\n\\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\\nB., and Madry, A. Adversarial examples are not bugs,\\nthey are features.\\n\\nIn Advances in Neural Information\\nProcessing Systems, pp. 125–136, 2019.\\n\\n\\n\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\n\\n\\narXiv preprint arXiv:1502.03167, 2015.\\n\\n\\n\\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\\nA. Deep structured output learning for unconstrained text\\nrecognition.\\n\\narXiv preprint arXiv:1412.5903, 2014.\\n\\n\\n\\nJaderberg, M., Simonyan, K., Zisserman, A., et al.\\n\\nSpatial\\ntransformer networks.\\n\\nAdvances in neural information\\nprocessing systems, 28:2017–2025, 2015.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 31\\n\\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\\nnostic dataset for compositional language and elementary\\nvisual reasoning.\\n\\nIn Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pp.\\n2901–2910, 2017.\\n\\n\\n\\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\\nLearning visual features from large weakly supervised\\ndata.\\n\\nIn European Conference on Computer Vision, pp.\\n67–84.\\n\\nSpringer, 2016.\\n\\n\\n\\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\\nmodeling in 3d cnn architectures with bert for action\\nrecognition.\\n\\narXiv preprint arXiv:2008.01232, 2020.\\n\\n\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\n\\n\\narXiv preprint arXiv:2001.08361, 2020.\\n\\n\\n\\nKarpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\\nembeddings for bidirectional image sentence mapping.\\n\\n\\nIn Advances in neural information processing systems,\\npp.\\n\\n1889–1897, 2014.\\n\\n\\n\\nKeyes, O. The misgendering machines: Trans/hci implica-\\ntions of automatic gender recognition.\\n\\nProceedings of the\\nACM on Human-Computer Interaction, 2(CSCW):1–22,\\n2018.\\n\\n\\n\\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\\nRingshia, P., and Testuggine, D.\\n\\nThe hateful memes\\nchallenge:\\n\\nDetecting hate speech in multimodal memes.\\n\\n\\narXiv preprint arXiv:2005.04790, 2020.\\n\\n\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization.\\n\\narXiv preprint arXiv:1412.6980, 2014.\\n\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\\nvisual-semantic embeddings with multimodal neural lan-\\nguage models.\\n\\narXiv preprint arXiv:1411.2539, 2014.\\n\\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\\n\\n\\nAdvances in neural information processing systems, 28:\\n3294–3302, 2015.\\n\\n\\n\\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\\nJ., Gelly, S., and Houlsby, N. Large scale learning of\\ngeneral visual representations for transfer.\\n\\narXiv preprint\\narXiv:1912.11370, 2019.\\n\\n\\n\\nKornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\\nmodels transfer better?\\n\\nIn Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp.\\n\\n2661–2671, 2019.\\n\\n\\n\\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\\nD. A., et al.\\n\\nVisual genome: Connecting language and\\nvision using crowdsourced dense image annotations.\\n\\nIn-\\nternational journal of computer vision, 123(1):32–73,\\n2017.\\n\\n\\n\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassification with deep convolutional neural networks.\\n\\n\\nIn Advances in neural information processing systems,\\npp.\\n\\n1097–1105, 2012.\\n\\n\\n\\nKuhnle, A. and Copestake, A. Shapeworld-a new test\\nmethodology for multimodal language understanding.\\n\\n\\narXiv preprint arXiv:1704.04517, 2017.\\n\\n\\n\\nKärkkäinen, K. and Joo, J. Fairface:\\n\\nFace attribute dataset\\nfor balanced race, gender, and age, 2019.\\n\\n\\n\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.', 'Face attribute dataset\\nfor balanced race, gender, and age, 2019.\\n\\n\\n\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.\\n\\n\\n\\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning\\nto detect unseen object classes by between-class attribute\\ntransfer.\\n\\nIn 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition, pp.\\n\\n951–958.\\n\\nIEEE, 2009.\\n\\n\\n\\nLarochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\\nof new tasks.\\n\\n2008.\\n\\n\\n\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents.\\n\\nIn International conference on\\nmachine learning, pp. 1188–1196, 2014.\\n\\n\\n\\nLeCun, Y. The mnist database of handwritten digits.\\n\\n\\nhttp://yann. lecun.\\n\\ncom/exdb/mnist/.\\n\\nLee, D.-H. Pseudo-label: The simple and efficient semi-\\nsupervised learning method for deep neural networks.\\n\\n\\n\\nLei Ba, J., Swersky, K., Fidler, S., et al.\\n\\nPredicting deep\\nzero-shot convolutional neural networks using textual\\ndescriptions.\\n\\nIn Proceedings of the IEEE International\\nConference on Computer Vision, pp. 4247–4255, 2015.\\n\\n\\n\\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\\nvisual n-grams from web data.\\n\\nIn Proceedings of the\\nIEEE International Conference on Computer Vision, pp.\\n4183–4192, 2017.\\n\\n\\n\\nLi, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\\nUnicoder-vl: A universal encoder for vision and language\\nby cross-modal pre-training.\\n\\n2020a.\\n\\n\\n\\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\\nLearning through dialogue interactions by asking ques-\\ntions.\\n\\narXiv preprint arXiv:1612.04936, 2016.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 32\\n\\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F.,\\n\\net al. Oscar:\\n\\nObject-\\nsemantics aligned pre-training for vision-language tasks.\\n\\n\\narXiv preprint arXiv:2004.06165, 2020b.\\n\\n\\n\\nLiang, W., Zou, J., and Yu, Z. Alice:\\n\\nActive learning with\\ncontrastive natural language explanations.\\n\\narXiv preprint\\narXiv:2009.10259, 2020.\\n\\n\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context.\\n\\nIn European conference on\\ncomputer vision, pp.\\n\\n740–755.\\n\\nSpringer, 2014.\\n\\n\\n\\nLinzen, T. How can we accelerate progress towards\\nhuman-like linguistic generalization?\\n\\narXiv preprint\\narXiv:2005.00955, 2020.\\n\\n\\n\\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\\ntimodal framework for the detection of hateful memes.\\n\\n\\narXiv preprint arXiv:2012.12871, 2020.\\n\\n\\n\\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\\nssi, R., Kaiser, L., and Shazeer, N. Generating\\nwikipedia by summarizing long sequences.\\n\\narXiv preprint\\narXiv:1801.10198, 2018.\\n\\n\\n\\nLocatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S.,\\nSchölkopf, B., and Bachem, O. A sober look at the\\nunsupervised learning of disentangled representations\\nand their evaluation.\\n\\narXiv preprint arXiv:2010.14766,\\n2020.\\n\\n\\n\\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\\ndient descent with warm restarts.\\n\\narXiv preprint\\narXiv:1608.03983, 2016.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization.\\n\\narXiv preprint arXiv:1711.05101, 2017.\\n\\n\\n\\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\\ntask-agnostic visiolinguistic representations for vision-\\nand-language tasks.\\n\\nIn Advances in Neural Information\\nProcessing Systems, pp.\\n\\n13–23, 2019.\\n\\n\\n\\nLu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\\nweakly supervised data and pose representation for action\\nrecognition, 2020.\\n\\nURL https://www.youtube.\\n\\n\\ncom/watch?v=KOQFxbPPLOE&t=1390s.\\n\\n\\n\\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\\nquet, O. Are gans created equal?\\n\\na large-scale study.\\n\\n\\nAdvances in neural information processing systems, 31:\\n700–709, 2018.', 'Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\\nquet, O. Are gans created equal?\\n\\na large-scale study.\\n\\n\\nAdvances in neural information processing systems, 31:\\n700–709, 2018.\\n\\n\\n\\nMahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\\nM., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\\nploring the limits of weakly supervised pretraining.\\n\\nIn\\n\\nProceedings of the European Conference on Computer\\nVision (ECCV), pp.\\n\\n181–196, 2018.\\n\\n\\n\\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\\nLearned in translation: Contextualized word vectors.\\n\\nIn\\nAdvances in neural information processing systems, pp.\\n6294–6305, 2017.\\n\\n\\n\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\\nnatural language decathlon:\\n\\nMultitask learning as ques-\\ntion answering.\\n\\narXiv preprint arXiv:1806.08730, 2018.\\n\\n\\n\\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\\nVenkatesh, G., et al. Mixed precision training.\\n\\narXiv\\npreprint arXiv:1710.03740, 2017.\\n\\n\\n\\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\\nI., and Sivic, J. Howto100m: Learning a text-video em-\\nbedding by watching hundred million narrated video clips.\\n\\n\\nIn Proceedings of the IEEE international conference on\\ncomputer vision, pp.\\n\\n2630–2640, 2019.\\n\\n\\n\\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\\nman, A. Rareact: A video dataset of unusual interactions.\\n\\n\\narXiv preprint arXiv:2008.01018, 2020a.\\n\\n\\n\\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\\nand Zisserman, A. End-to-end learning of visual represen-\\ntations from uncurated instructional videos.\\n\\nIn Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp.\\n\\n9879–9889, 2020b.\\n\\n\\n\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality.\\n\\nAdvances in neural informa-\\ntion processing systems, 26:3111–3119, 2013.\\n\\n\\n\\nMiller, J., Krauth, K., Recht, B., and Schmidt, L.\\n\\nThe effect\\nof natural distribution shift on question answering models.\\n\\n\\narXiv preprint arXiv:2004.14444, 2020.\\n\\n\\n\\nMishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\\ntion using higher order language priors.\\n\\n2012.\\n\\n\\n\\nMithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\\nChowdhury, A. K. Webly supervised joint embedding for\\ncross-modal image-text retrieval.\\n\\nIn Proceedings of the\\n26th ACM international conference on Multimedia, pp.\\n1856–1864, 2018.\\n\\n\\n\\nMori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\\nformation based on dividing and vector quantizing images\\nwith words.\\n\\nCiteseer, 1999.\\n\\n\\n\\nMu, J., Liang, P., and Goodman, N. Shaping visual represen-\\ntations with language for few-shot classification.\\n\\narXiv\\npreprint arXiv:1911.02683, 2019.\\n\\n\\n\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\\nhttps://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 33\\n\\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\\ncation estimation of photos using a hierarchical model\\nand scene classification.\\n\\nIn Proceedings of the European\\nConference on Computer Vision (ECCV), pp.\\n\\n563–579,\\n2018.\\n\\n\\n\\nMurty, S., Koh, P. W., and Liang, P. Expbert: Representation\\nengineering with natural language explanations.\\n\\narXiv\\npreprint arXiv:2005.01932, 2020.\\n\\n\\n\\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language\\nunderstanding for text-based games using deep reinforce-\\nment learning.\\n\\narXiv preprint arXiv:1506.08941, 2015.\\n\\n\\n\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y. Reading digits in natural images with\\nunsupervised feature learning.\\n\\n2011.\\n\\n\\n\\nNoble, S. U. Algorithms of oppression: How search engines\\nreinforce racism.\\n\\n2018.\\n\\n\\n\\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site.\\n\\nGroup Dynamics: Theory, Research, and\\nPractice, 6(1):101, 2002.', 'Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site.\\n\\nGroup Dynamics: Theory, Research, and\\nPractice, 6(1):101, 2002.\\n\\n\\n\\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\\n\\n\\nA large-scale benchmark dataset for event recognition in\\nsurveillance video.\\n\\nIn CVPR 2011, pp. 3153–3160.\\n\\nIEEE,\\n2011.\\n\\n\\n\\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\\nfellow, I. Realistic evaluation of deep semi-supervised\\nlearning algorithms.\\n\\nAdvances in neural information pro-\\ncessing systems, 31:3235–3246, 2018.\\n\\n\\n\\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\\ning with contrastive predictive coding.\\n\\narXiv preprint\\narXiv:1807.03748, 2018.\\n\\n\\n\\nOrdonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs.\\n\\nAdvances\\nin neural information processing systems, 24:1143–1151,\\n2011.\\n\\n\\n\\npandas development team, T. pandas-dev/pandas: Pan-\\ndas, February 2020.\\n\\nURL https://doi.org/10.\\n\\n\\n5281/zenodo.3509134.\\n\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\\nC. V. Cats and dogs.\\n\\nIn IEEE Conference on Computer\\nVision and Pattern Recognition, 2012.\\n\\n\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\\n\\nBai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library.\\n\\nIn Advances\\nin Neural Information Processing Systems 32, pp. 8024–\\n8035, 2019.\\n\\n\\n\\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\\nScikit-learn: Machine learning in Python.\\n\\nJournal of\\nMachine Learning Research, 12:2825–2830, 2011.\\n\\n\\n\\nPennington, J., Socher, R., and Manning, C. D. Glove:\\nGlobal vectors for word representation.\\n\\nIn Proceedings\\nof the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pp. 1532–1543, 2014.\\n\\n\\n\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations.\\n\\narXiv preprint arXiv:1802.05365,\\n2018.\\n\\n\\n\\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\\nA. Imagebert: Cross-modal pre-training with large-\\nscale weak-supervised image-text data.\\n\\narXiv preprint\\narXiv:2001.07966, 2020.\\n\\n\\n\\nQuattoni, A., Collins, M., and Darrell, T. Learning visual\\nrepresentations using images with captions.\\n\\nIn 2007 IEEE\\nConference on Computer Vision and Pattern Recognition,\\npp.\\n\\n1–8.\\n\\nIEEE, 2007.\\n\\n\\n\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018.\\n\\n\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\n\\n\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer.\\n\\narXiv preprint arXiv:1910.10683, 2019.\\n\\n\\n\\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\\nJ., and Denton, E. Saving face: Investigating the ethical\\nconcerns of facial recognition auditing, 2020.\\n\\n\\n\\nRamanathan, V., Liang, P., and Fei-Fei, L. Video event\\nunderstanding using natural language descriptions.\\n\\nIn\\nProceedings of the IEEE International Conference on\\nComputer Vision, pp. 905–912, 2013.\\n\\n\\n\\nRashtchian, C., Young, P., Hodosh, M., and Hockenmaier,\\n\\nJ.\\nCollecting image annotations using amazon’s mechanical\\nturk.\\n\\nIn Proceedings of the NAACL HLT 2010 Workshop\\non Creating Speech and Language Data with Amazon’s\\nMechanical Turk, pp.\\n\\n139–147, 2010.', 'J.\\nCollecting image annotations using amazon’s mechanical\\nturk.\\n\\nIn Proceedings of the NAACL HLT 2010 Workshop\\non Creating Speech and Language Data with Amazon’s\\nMechanical Turk, pp.\\n\\n139–147, 2010.\\n\\n\\n\\nhttps://doi.org/10.5281/zenodo.3509134\\nhttps://doi.org/10.5281/zenodo.3509134\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 34\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\\nagenet classifiers generalize to imagenet?\\n\\narXiv preprint\\narXiv:1902.10811, 2019.\\n\\nSalimans, T. and Kingma, D. P. Weight normalization: A\\nsimple reparameterization to accelerate training of deep\\nneural networks.\\n\\nIn Advances in neural information pro-\\ncessing systems, pp. 901–909, 2016.\\n\\n\\n\\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\\ncomputers see gender:\\n\\nAn evaluation of gender classifica-\\ntion in commercial facial analysis services.\\n\\nProceedings\\nof the ACM on Human-Computer Interaction, 3(CSCW):\\n1–33, 2019.\\n\\n\\n\\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\\ngender bias in image recognition systems.\\n\\nSocius, 6:\\n2378023120967171, 2020.\\n\\n\\n\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units.\\n\\narXiv\\npreprint arXiv:1508.07909, 2015.\\n\\nShankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\\nand Schmidt,\\n\\nL. Do image classifiers generalize across\\ntime?\\n\\narXiv preprint arXiv:1906.02168, 2019.\\n\\n\\n\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning.\\n\\nIn Proceedings\\nof the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\\n2565, 2018.\\n\\n\\n\\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\\nmodels that can read.\\n\\nIn Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, pp.\\n\\n\\n8317–8326, 2019.\\n\\n\\n\\nSocher, R. and Fei-Fei, L. Connecting modalities: Semi-\\nsupervised segmentation and annotation of images using\\nunaligned text corpora.\\n\\nIn 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition,\\npp.\\n\\n966–973.\\n\\nIEEE, 2010.\\n\\n\\n\\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\\nsemantic compositionality over a sentiment treebank.\\n\\nIn\\nProceedings of the 2013 conference on empirical methods\\nin natural language processing, pp.\\n\\n1631–1642, 2013.\\n\\n\\n\\nSocher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\\nA. Y. Grounded compositional semantics for finding and\\ndescribing images with sentences.\\n\\nTransactions of the\\nAssociation for Computational Linguistics, 2:207–218,\\n2014.\\n\\n\\n\\nSohn, K. Improved deep metric learning with multi-class\\nn-pair loss objective.\\n\\nIn Advances in neural information\\nprocessing systems, pp.\\n\\n1857–1865, 2016.\\n\\n\\n\\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\\nVoss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\\nMcGuffie, K., and Wang, J. Release strategies and the\\nsocial impacts of language models, 2019.\\n\\n\\n\\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\\nof 101 human actions classes from videos in the wild.\\n\\n\\narXiv preprint arXiv:1212.0402, 2012.\\n\\n\\n\\nSpeer, R. ftfy.\\n\\nZenodo, 2019.\\n\\nURL https://doi.org/\\n10.5281/zenodo.2591652. Version 5.5.\\n\\n\\n\\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\\nwith deep boltzmann machines.\\n\\nIn NIPS, 2012.\\n\\n\\n\\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept\\nlearning and semantic parsing from natural language ex-\\nplanations.\\n\\nIn Proceedings of the 2017 conference on\\nempirical methods in natural language processing, pp.\\n1527–1536, 2017.\\n\\n\\n\\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Traffic Sign Recognition Benchmark: A multi-\\nclass classification competition.', 'Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Traffic Sign Recognition Benchmark: A multi-\\nclass classification competition.\\n\\nIn IEEE International\\nJoint Conference on Neural Networks, pp. 1453–1460,\\n2011.\\n\\n\\n\\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\\nand Schmid, C. Learning video representations from tex-\\ntual web supervision.\\n\\narXiv preprint arXiv:2007.14937,\\n2020.\\n\\n\\n\\nSzegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\\nA. Inception-v4, inception-resnet and the impact\\nof residual connections on learning.\\n\\narXiv preprint\\narXiv:1602.07261, 2016.\\n\\n\\n\\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\\nencoder representations from transformers.\\n\\narXiv preprint\\narXiv:1908.07490, 2019.\\n\\n\\n\\nTan, M. and Le, Q. V. Efficientnet: Rethinking model\\nscaling for convolutional neural networks.\\n\\narXiv preprint\\narXiv:1905.11946, 2019.\\n\\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\\nand Schmidt,\\n\\nL. Measuring robustness to natural dis-\\ntribution shifts in image classification.\\n\\narXiv preprint\\narXiv:2007.00644, 2020.\\n\\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m:\\n\\nThe\\nnew data in multimedia research.\\n\\nCommunications of the\\nACM, 59(2):64–73, 2016.\\n\\n\\n\\nhttps://doi.org/10.5281/zenodo.2591652\\nhttps://doi.org/10.5281/zenodo.2591652\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 35\\n\\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview\\ncoding.\\n\\narXiv preprint arXiv:1906.05849, 2019.\\n\\n\\n\\nTian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\\nIsola, P. Rethinking few-shot image classification: a\\ngood embedding is all you need?\\n\\narXiv preprint\\narXiv:2003.11539, 2020.\\n\\n\\n\\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\\nimages: A large data set for nonparametric object and\\nscene recognition.\\n\\nIEEE transactions on pattern analysis\\nand machine intelligence, 30(11):1958–1970, 2008.\\n\\n\\n\\nTouvron, H., Vedaldi, A., Douze, M., and Jégou, H. Fix-\\ning the train-test resolution discrepancy.\\n\\nIn Advances in\\nneural information processing systems, pp. 8252–8262,\\n2019.\\n\\n\\n\\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\\nanalysis and abnormality detection.\\n\\nIn 2009 IEEE 12th\\nInternational Conference on Computer Vision Workshops,\\nICCV Workshops, pp.\\n\\n1338–1345.\\n\\nIEEE, 2009.\\n\\n\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need.\\n\\nIn Advances in neural information\\nprocessing systems, pp. 5998–6008, 2017.\\n\\n\\n\\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\\nWelling, M. Rotation equivariant CNNs for digital pathol-\\nogy.\\n\\nJune 2018.\\n\\n\\n\\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, İ.,\\nFeng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\\nF., van Mulbregt, P., and SciPy 1.0 Contributors.\\n\\nSciPy\\n1.0: Fundamental Algorithms for Scientific Computing\\nin Python.\\n\\nNature Methods, 17:261–272, 2020.\\n\\ndoi:\\n10.1038/s41592-019-0686-2.\\n\\nVo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\\ndeep learning era.\\n\\nIn Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pp.\\n\\n2621–2630,\\n2017.\\n\\n\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding.\\n\\narXiv\\npreprint arXiv:1804.07461, 2018.\\n\\n\\n\\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\\nbust global representations by penalizing local predictive\\npower.\\n\\nIn Advances in Neural Information Processing\\nSystems, pp. 10506–10518, 2019.', 'In Advances in Neural Information Processing\\nSystems, pp. 10506–10518, 2019.\\n\\n\\n\\nWang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\\nM., Wang, Y., and Liu, W. All you need is boundary: To-\\nward arbitrary-shaped text spotting.\\n\\nIn Proceedings of the\\nAAAI Conference on Artificial Intelligence, volume 34,\\npp.\\n\\n12160–12167, 2020.\\n\\n\\n\\nWang, J., Markert, K., and Everingham, M. Learning mod-\\nels for object recognition from natural language descrip-\\ntions.\\n\\nIn BMVC, volume 1, pp. 2, 2009.\\n\\n\\n\\nWeston, J., Bengio, S., and Usunier, N. Large scale im-\\nage annotation: learning to rank with joint word-image\\nembeddings.\\n\\nMachine learning, 81(1):21–35, 2010.\\n\\n\\n\\nWeston, J. E. Dialog-based language learning.\\n\\nIn Advances\\nin Neural Information Processing Systems, pp.\\n\\n829–837,\\n2016.\\n\\n\\n\\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\\ncation with convolutional neural networks.\\n\\nIn European\\nConference on Computer Vision, pp.\\n\\n37–55.\\n\\nSpringer,\\n2016.\\n\\n\\n\\nWu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\\nshick, R. Detectron2.\\n\\nhttps://github.com/\\nfacebookresearch/detectron2, 2019.\\n\\n\\n\\nWu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\\nlearning via non-parametric instance-level discrimination.\\n\\n\\narXiv preprint arXiv:1805.01978, 2018.\\n\\n\\n\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\\nwith noisy student improves imagenet classification.\\n\\nIn\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 10687–10698, 2020.\\n\\n\\n\\ny Arcas, B. A., Mitchell, M., and Todorov,\\nA. Physiognomy’s new clothes. 2017.\\n\\n\\nURL https://medium.com/@blaisea/\\nphysiognomys-new-clothes-f2d4b59fdd6a.\\n\\nYang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\\npre-training for text-vqa and text-caption.\\n\\narXiv preprint\\narXiv:2012.04638, 2020.\\n\\n\\n\\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\\nYu, L., Dyer, C., et al.\\n\\nLearning and evaluating general\\nlinguistic intelligence.\\n\\narXiv preprint arXiv:1901.11373,\\n2019.\\n\\n\\n\\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\\nimage descriptions to visual denotations: New similarity\\nmetrics for semantic inference over event descriptions.\\n\\n\\nTransactions of the Association for Computational Lin-\\nguistics, 2:67–78, 2014.\\n\\n\\n\\nhttps://github.com/facebookresearch/detectron2\\nhttps://github.com/facebookresearch/detectron2\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 36\\n\\nYu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\\nlanguage representations through scene graph.\\n\\narXiv\\npreprint arXiv:2006.16934, 2020.\\n\\n\\n\\nZeiler, M. D. and Fergus, R. Visualizing and understand-\\ning convolutional networks.\\n\\nIn European conference on\\ncomputer vision, pp. 818–833.\\n\\nSpringer, 2014.\\n\\n\\n\\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\\nmann, M., Dosovitskiy, A., et al.\\n\\nA large-scale study of\\nrepresentation learning with the visual task adaptation\\nbenchmark.\\n\\narXiv preprint arXiv:1910.04867, 2019.\\n\\n\\n\\nZhang, R. Making convolutional networks shift-invariant\\nagain.\\n\\narXiv preprint arXiv:1904.11486, 2019.\\n\\n\\n\\nZhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\\nglotz, C. P. Contrastive learning of medical visual repre-\\nsentations from paired images and text.\\n\\narXiv preprint\\narXiv:2010.00747, 2020.\\n\\n\\n\\nZuboff, S. Big other: surveillance capitalism and the\\nprospects of an information civilization.\\n\\nJournal of Infor-\\nmation Technology, 30(1):75–89, 2015.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 37\\n\\nA. Linear-probe evaluation\\nWe provide additional details for linear probe experiments\\npresented in this paper, including the list of the datasets and\\nmodels used for evaluation.\\n\\nA.1.', 'Datasets\\n\\nWe use the 12 datasets from the well-studied evaluation\\nsuite introduced by (Kornblith et al., 2019) and add 15\\nadditional datasets in order to assess the performance of\\nmodels on a wider variety of distributions and tasks.\\n\\nThese\\ndatasets include MNIST, the Facial Expression Recognition\\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\\nfic Sign Recognition Benchmark (GTSRB) dataset (Stal-\\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\\n\\n\\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\\nreira et al., 2019), 2,500 random samples of the CLEVR\\ndataset (Johnson et al., 2017), the Hateful Memes dataset\\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\\net al., 2012).\\n\\nFor the two video datasets (UCF101 and Ki-\\nnetics700), we use the middle frame of each video clip as\\nthe input image.\\n\\nSTL-10 and UCF101 have multiple pre-\\ndefined train/validation/test splits, 10 and 3 respectively, and\\nwe report the average over all splits.\\n\\nDetails on each dataset\\nand the corresponding evaluation metrics are provided in\\nTable 9.\\n\\n\\n\\nAdditionally, we created two datasets that we call Coun-\\ntry211 and Rendered SST2.\\n\\nThe Country211 dataset is\\ndesigned to assess the geolocation capability of visual rep-\\nresentations.\\n\\nWe filtered the YFCC100m dataset (Thomee\\net al., 2016) to find 211 countries (defined as having an\\nISO-3166 country code) that have at least 300 photos with\\nGPS coordinates, and we built a balanced dataset with 211\\ncategories, by sampling 200 photos for training and 100\\nphotos for testing, for each country.\\n\\n\\n\\nThe Rendered SST2 dataset is designed to measure the opti-\\ncal character recognition capability of visual representations.\\n\\n\\nTo do so, we used the sentences from the Stanford Sentiment\\nTreebank dataset (Socher et al., 2013) and rendered them\\ninto images, with black texts on a white background, in a\\n448×448 resolution.\\n\\nTwo example images from this dataset\\nare shown in Figure 19.\\n\\nA.2.\\n\\nModels\\n\\n\\n\\nIn combination with the datasets listed above, we evaluate\\nthe following series of models using linear probes.\\n\\n\\n\\nLM RN50\\n\\nThis is a multimodal model that uses an au-\\ntoregressive loss instead of a contrastive loss, while using\\n\\nthe ResNet-50 architecture as in the smallest contrastive\\nmodel.\\n\\nTo do so, the output from the CNN is projected into\\nfour tokens, which are then fed as a prefix to a language\\nmodel autoregressively predicting the text tokens.\\n\\nApart\\nfrom the training objective, the model was trained on the\\nsame dataset for the same number of epochs as other CLIP\\nmodels.\\n\\n\\n\\nCLIP-RN Five ResNet-based contrastive CLIP models\\nare included.\\n\\nAs discussed in the paper, the first two models\\nfollow ResNet-50 and ResNet-101, and we use EfficientNet-\\nstyle (Tan & Le, 2019) scaling for the next three models\\nwhich simultaneously scale the model width, the number\\nof layers, and the input resolution to obtain models with\\nroughly 4x, 16x, and 64x computation.\\n\\n\\n\\nCLIP-ViT We include four CLIP models that use the Vi-\\nsion Transformer (Dosovitskiy et al., 2020) architecture as\\nthe image encoder.\\n\\nWe include three models trained on 224-\\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\\nthe ViT-L/14 model fine-tuned on 336-by-336 pixel input\\nimages.\\n\\n\\n\\nEfficietNet We use the nine models (B0-B8) from the\\noriginal EfficientNet paper (Tan & Le, 2019), as well as\\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\\n(Tan & Le, 2019).\\n\\nThe largest models (L2-475 and L2-800)\\ntake the input resolutions of 475x475 and 800x800 pixels,\\nrespectively.\\n\\n\\n\\nInstagram-pretrained ResNeXt We use the four models\\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\\net al., 2018), as well as their two FixRes variants which use\\nhigher input resolutions (Touvron et al., 2019).', 'Big Transfer (BiT) We use BiT-S and BiT-M models\\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\\nImageNet-21k datasets.\\n\\nThe model weights for BiT-L is not\\npublicly available.\\n\\n\\n\\nVision Transformer (ViT)\\n\\nWe also include four ViT\\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\\nL/16, and ViT-H/14.\\n\\nWe note that their best-performing\\nmodels, trained on the JFT-300M dataset, are not available\\npublicly.\\n\\n\\n\\nSimCLRv2\\n\\nThe SimCLRv2 (Chen et al., 2020c) project\\nreleased pre-trained and fine-tuned models in various set-\\ntings.\\n\\nWe use the seven pretrain-only checkpoints with\\nselective kernels.\\n\\n\\n\\nBYOL We use the recently released model weights of\\nBYOL (Grill et al., 2020), specifically their 50x1 and 200x2\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 38\\n\\nFigure 19.\\n\\nTwo example images from the Rendered SST2 dataset\\n\\ncheckpoints.\\n\\n\\n\\nMomentum Contrast (MoCo)\\n\\nWe include the MoCo-v1\\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\\ncheckpoints.\\n\\n\\n\\nVirTex We use the pretrained model of VirTex (Desai &\\nJohnson, 2020).\\n\\nWe note that VirTex has a similar model\\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\\nof high-quality captions from MSCOCO.\\n\\n\\n\\nResNet We add the original ResNet checkpoints released\\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\\nResNet152.\\n\\nA.3.\\n\\nEvaluation\\n\\nWe use image features taken from the penultimate layer of\\neach model, ignoring any classification layer provided.\\n\\nFor\\nCLIP-ViT models, we used the features before the linear\\nprojection to the embedding space, which corresponds to\\nI f in Figure 3.\\n\\nWe train a logistic regression classifier\\nusing scikit-learn’s L-BFGS implementation, with maxi-\\nmum 1,000 iterations, and report the corresponding met-\\nric for each dataset.\\n\\nWe determine the L2 regularization\\nstrength λ using a hyperparameter sweep on the validation\\nsets over the range between 10−6 and 106, with 96 log-\\narithmically spaced steps.\\n\\nTo save compute required for\\nthe sweeps, we perform a parametric binary search that\\nstarts with λ =\\n\\n[10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\\neratively halves the interval around the peak until it reaches\\na resolution of 8 steps per decade.\\n\\nThe hyperparameter\\nsweeps are performed on a validation split of each dataset.\\n\\n\\nFor the datasets that contain a validation split in addition to\\n\\na test split, we use the provided validation set to perform\\nthe hyperparameter search, and for the datasets that do not\\nprovide a validation split or have not published labels for\\nthe test data, we split the training dataset to perform the\\nhyperparameter search.\\n\\nFor the final result, we combine the\\nvalidation split back with the training split and report the\\nperformance on the unused split.\\n\\n\\n\\nA.4.\\n\\nResults\\n\\nThe individual linear probe scores are provided in Table 10\\nand plotted in Figure 20.\\n\\nThe best-performing CLIP model,\\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\\nachieved the state of the art in 21 of the 27 datasets, i.e.\\nincluded in the Clopper-Pearson 99.5% confidence interval\\naround each dataset’s top score.\\n\\nFor many datasets, CLIP\\nperforms significantly better than other models, demonstrat-\\ning the advantage of natural language supervision over tradi-\\ntional pre-training approaches based on image classification.\\n\\n\\nSee Section 3.2 for more discussions on the linear probe\\nresults.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 39\\n\\nDataset Classes Train size Test size Evaluation metric\\n\\nFood-101 102 75,750 25,250 accuracy\\nCIFAR-10 10 50,000 10,000 accuracy\\nCIFAR-100 100 50,000 10,000 accuracy\\nBirdsnap 500 42,283 2,149 accuracy\\nSUN397 397 19,850 19,850 accuracy\\nStanford Cars 196 8,144 8,041 accuracy\\nFGVC Aircraft 100 6,667 3,333 mean per class\\nPascal VOC 2007', 'Classification 20 5,011 4,952 11-point mAP\\nDescribable Textures 47 3,760 1,880 accuracy\\nOxford-IIIT Pets 37 3,680 3,669 mean per class\\nCaltech-101 102 3,060 6,085 mean-per-class\\nOxford Flowers 102 102 2,040 6,149 mean per class\\n\\nMNIST 10 60,000 10,000 accuracy\\nFacial Emotion Recognition 2013 8 32,140 3,574 accuracy\\nSTL-10 10 1000 8000 accuracy\\nEuroSAT 10 10,000 5,000 accuracy\\nRESISC45 45 3,150 25,200 accuracy\\nGTSRB 43 26,640 12,630 accuracy\\nKITTI 4 6,770 711 accuracy\\nCountry211 211 43,200 21,100 accuracy\\nPatchCamelyon 2 294,912 32,768 accuracy\\nUCF101 101 9,537 1,794 accuracy\\nKinetics700 700 494,801 31,669 mean(top1, top5)\\n\\n\\nCLEVR Counts 8 2,000 500 accuracy\\nHateful Memes 2 8,500 500 ROC AUC\\nRendered SST2 2 7,792 1,821 accuracy\\nImageNet 1000 1,281,167 50,000 accuracy\\n\\nTable 9.\\n\\nDatasets examined for linear probes.\\n\\nWe note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\\navailable online at the time of this writing.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 40\\n\\n\\n\\nFo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nC\\nar\\n\\ns\\n\\nA\\nir\\n\\ncr\\naf\\n\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n?\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nA\\n\\nM\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nSS\\nT\\n\\n\\n\\nIm\\nag\\n\\neN\\net\\n\\nLM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nN\\n\\n50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\\n101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\\n50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\\n\\n50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0', '66.3 53.8 71.1 80.0 81.5\\n50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6\\n\\nC\\nL\\n\\nIP\\n-V\\n\\niT B/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\\nB/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\\nL/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\\n\\nL/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4\\n\\nE\\nffi\\n\\nci\\nen\\n\\ntN\\net\\n\\nB0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\\nB1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\\nB2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\\nB3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\\nB4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\\nB5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\\nB6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\\nB7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\\nB8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3\\n\\nE\\nffi\\n\\nci\\nen\\n\\ntN\\net\\n\\nN\\noi\\n\\nsy\\nSt\\n\\nud\\nen\\n\\nt B0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\\nB1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\\nB2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\\nB3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\\nB4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4', 'B5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\\nB6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\\nB7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\\n\\nL2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\\nL2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4\\n\\nIn\\nst\\n\\nag\\nra\\n\\nm\\n\\n32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\\n32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\\n32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\\n32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\\n\\nFixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\\nFixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0\\n\\nB\\niT\\n\\n-S\\n\\nR50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\\nR50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\\n\\nR101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\\nR101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\\nR152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\\nR152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5\\n\\nB\\niT\\n\\n-M\\n\\nR50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\\nR50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\\n\\nR101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\\nR101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\\nR152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\\nR152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5\\n\\nV\\niT\\n\\nB/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\\nB/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\\nL/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\\nH/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4', '52.1 41.1 55.9 54.1 75.9\\n\\nSi\\nm\\n\\nC\\nL\\n\\nR\\nv2\\n\\nR50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\\nR50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\\n\\nR101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\\nR101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\\nR152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\\nR152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\\nR152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0\\n\\nB\\nY\\n\\nO\\nL 50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\\n\\n200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2\\n\\nM\\noC\\n\\no v1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\\nv2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1\\n\\nVirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7\\n\\nR\\nes\\n\\nN\\net 50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\\n\\n101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\\n152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1\\n\\nTable 10.\\n\\nLinear probe performance of various pre-trained models over 27 datasets.\\n\\nScores within the 99.5% Clopper-Pearson confidence\\ninterval of each dataset’s top score are shown in bold.\\n\\n?\\n\\nWe updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.', 'We updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 41\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\nac\\n\\ncu\\nra\\n\\ncy\\nFood101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n100 101 102\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n100 101 102\\n\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n100 101 102\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n100 101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n100 101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n100 101 102\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\n\\n88\\n\\n89\\n\\n90\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes\\n\\nPascalVOC2007\\n\\n100 101 102\\n\\n72\\n\\n74\\n\\n76\\n\\n78\\n\\n80\\n\\n82\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n100 101 102\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n100 101 102\\n\\n90\\n\\n91\\n\\n92\\n\\n93\\n\\n94\\n\\n95\\n\\n96\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n100 101 102\\n\\n90\\n\\n92\\n\\n94\\n\\n96\\n\\n98\\n\\n100\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n100 101 102\\n\\n97.25\\n\\n97.50\\n\\n97.75\\n\\n98.00\\n\\n98.25\\n\\n98.50\\n\\n98.75\\n\\n99.00\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n100 101 102\\n\\n55.0\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n100 101 102\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\n98.5\\n\\n99.0\\n\\n99.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n100 101 102\\n\\n95.5\\n\\n96.0\\n\\n96.5\\n\\n97.0\\n\\n97.5\\n\\n98.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n100 101 102\\n82\\n\\n84\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\n94\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n100 101 102\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n100 101 102\\n\\n57.5\\n\\n60.0\\n\\n62.5\\n\\n65.0\\n\\n67.5\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n100 101 102\\n\\n81\\n\\n82\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\nac\\n\\ncu\\nra\\n\\ncy\\nPatchCamelyon\\n\\n100 101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n100 101 102\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n100 101 102\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\n45\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n100 101 102\\n\\nGFLOPs/image\\n\\n70.0\\n\\n72.5\\n\\n75.0\\n\\n77.5\\n\\n80.0\\n\\n82.5\\n\\n85.0\\n\\n87.5\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\n\\nFigure 20.\\n\\nLinear probe performance plotted for each of the 27 datasets, using the data from Table 10.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 42\\n\\ncorrect label:\\n\\nred and white triangle with exclamation mark warning\\n\\n0 20 40 60 80 100\\n\\na zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\\n\\n\\n\\na zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.\\n\\n\\n\\na zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\\n\\n\\n\\na zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.\\n\\n\\n\\na zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.\\n\\n\\n\\ncorrect rank: 1/43    correct probability: 45.75%\\nGerman Traffic Sign Recognition Benchmark (GTSRB)\\n\\ncorrect label: positive\\n\\n0 20 40 60 80 100\\n\\na positive review of a movie.\\n\\n\\n\\na negative review of a movie.\\n\\n\\n\\ncorrect rank: 1/2    correct probability: 78.21%\\nStanford Sentiment Treebank\\n\\ncorrect label: meme\\n\\n0 20 40 60 80 100\\n\\na meme.\\n\\n\\n\\na hatespeech meme.\\n\\n\\n\\ncorrect rank: 1/2    correct probability: 99.20%\\nHateful Memes\\n\\ncorrect label: barn\\n\\n0 20 40 60 80 100\\n\\na photo of a barn.\\n\\n\\n\\na photo of a church.\\n\\n\\n\\na photo of a threshing machine.\\n\\n\\n\\na photo of a sawmill.\\n\\n\\n\\na photo of a prison.\\n\\n\\n\\ncorrect rank: 1/1000    correct probability: 79.56%\\nImageNet Sketch\\n\\ncorrect label(s): antelope\\n\\n0 20 40 60 80 100\\n\\na photo of a antelope.\\n\\n\\n\\na photo of a zebra.\\n\\n\\n\\na photo of a car.\\n\\n\\n\\na photo of a cattle.\\n\\n\\n\\na photo of a elephant.', 'a photo of a zebra.\\n\\n\\n\\na photo of a car.\\n\\n\\n\\na photo of a cattle.\\n\\n\\n\\na photo of a elephant.\\n\\n\\n\\ncorrect rank: 1/30    correct probability: 99.77%\\nImageNet Vid\\n\\ncorrect label: 158\\n\\n0 20 40 60 80 100\\n\\na street sign of the number: \"1157\".\\n\\n\\n\\na street sign of the number: \"1165\".\\n\\n\\n\\na street sign of the number: \"1164\".\\n\\n\\n\\na street sign of the number: \"1155\".\\n\\n\\n\\na street sign of the number: \"1364\".\\n\\n\\n\\ncorrect rank: 83/2000    correct probability: 0.27%\\nStreet View House Numbers (SVHN)\\n\\ncorrect label: 7\\n\\n0 20 40 60 80 100\\n\\na photo of the number: \"7\".\\n\\n\\n\\na photo of the number: \"2\".\\n\\n\\n\\na photo of the number: \"1\".\\n\\n\\n\\na photo of the number: \"6\".\\n\\n\\n\\na photo of the number: \"4\".\\n\\n\\n\\ncorrect rank: 1/10    correct probability: 85.32%\\nMNIST\\n\\ncorrect label(s): motorcycle\\n\\n0 20 40 60 80 100\\n\\na photo of a motorcycle.\\n\\n\\n\\na photo of a bicycle.\\n\\n\\n\\na photo of a car.\\n\\n\\n\\na photo of a horse.\\n\\n\\n\\na photo of a dining table.\\n\\n\\n\\ncorrect rank: 1/20    correct probability: 99.69%\\nPASCAL VOC 2007\\n\\ncorrect label: perforated\\n\\n0 20 40 60 80 100\\n\\na photo of a polka-dotted texture.\\n\\n\\n\\na photo of a perforated texture.\\n\\n\\n\\na photo of a dotted texture.\\n\\n\\n\\na photo of a studded texture.\\n\\n\\n\\na photo of a freckled texture.\\n\\n\\n\\ncorrect rank: 2/47    correct probability: 20.50%\\nDescribable Textures Dataset (DTD)\\n\\ncorrect label: marimba\\n\\n0 20 40 60 80 100\\n\\na photo of a marimba.\\n\\n\\n\\na photo of a abacus.\\n\\n\\n\\na photo of a steel drum.\\n\\n\\n\\na photo of a computer keyboard.\\n\\n\\n\\na photo of a pool table.\\n\\n\\n\\ncorrect rank: 1/1000    correct probability: 79.54%\\nImageNet Blurry\\n\\ncorrect label: Pill bottle\\n\\n0 20 40 60 80 100\\n\\na photo of a pill bottle.\\n\\n\\n\\na photo of a bottle cap.\\n\\n\\n\\na photo of a beer bottle.\\n\\n\\n\\na photo of a pillow.\\n\\n\\n\\na photo of a wine bottle.\\n\\n\\n\\ncorrect rank: 1/113    correct probability: 98.34%\\nObjectNet ImageNet Overlap\\n\\ncorrect label: building\\n\\n0 20 40 60 80 100\\n\\na photo of a building.\\n\\n\\n\\na photo of a carriage.\\n\\n\\n\\na photo of a statue.\\n\\n\\n\\na photo of a bag.\\n\\n\\n\\na photo of a mug.\\n\\n\\n\\ncorrect rank: 1/12    correct probability: 97.69%\\naYahoo\\n\\ncorrect label:\\n\\nBlack chinned Hummingbird\\n\\n0 20 40 60 80 100\\n\\na photo of a broad tailed hummingbird, a type of bird.\\n\\n\\n\\na photo of a calliope hummingbird, a type of bird.\\n\\n\\n\\na photo of a costas hummingbird, a type of bird.\\n\\n\\n\\na photo of a black chinned hummingbird, a type of bird.\\n\\n\\n\\na photo of a annas hummingbird, a type of bird.\\n\\n\\n\\ncorrect rank: 4/500    correct probability: 12.00%\\nBirdsnap\\n\\ncorrect label: King Charles Spaniel\\n\\n0 20 40 60 80 100\\n\\na photo of a king charles spaniel.\\n\\n\\n\\na photo of a brittany dog.\\n\\n\\n\\na photo of a cocker spaniel.\\n\\n\\n\\na photo of a papillon.\\n\\n\\n\\na photo of a sussex spaniel.\\n\\n\\n\\ncorrect rank: 1/1000    correct probability: 91.61%\\nImageNet\\n\\ncorrect label: great masterwort\\n\\n0 20 40 60 80 100\\n\\na photo of a great masterwort, a type of flower.\\n\\n\\n\\na photo of a bishop of llandaff, a type of flower.\\n\\n\\n\\na photo of a pincushion flower, a type of flower.\\n\\n\\n\\na photo of a globe flower, a type of flower.\\n\\n\\n\\na photo of a prince of wales feathers, a type of flower.\\n\\n\\n\\ncorrect rank: 1/102    correct probability: 74.25%\\nFlowers-102\\n\\ncorrect label: country line dancing\\n\\n0 20 40 60 80 100\\n\\na photo of country line dancing.\\n\\n\\n\\na photo of square dancing.\\n\\n\\n\\na photo of swing dancing.\\n\\n\\n\\na photo of dancing charleston.\\n\\n\\n\\na photo of salsa dancing.\\n\\n\\n\\ncorrect rank: 1/700    correct probability: 98.98%\\nKinetics-700\\n\\ncorrect label: kennel indoor\\n\\n0 20 40 60 80 100\\n\\na photo of a kennel indoor.\\n\\n\\n\\na photo of a kennel outdoor.\\n\\n\\n\\na photo of a jail cell.\\n\\n\\n\\na photo of a jail indoor.\\n\\n\\n\\na photo of a veterinarians office.\\n\\n\\n\\ncorrect rank: 1/723    correct probability: 98.63%\\nSUN\\n\\ncorrect label: 2012 Honda Accord Coupe\\n\\n0 20 40 60 80 100\\n\\na photo of a 2012 honda accord coupe.\\n\\n\\n\\na photo of a 2012 honda accord sedan.\\n\\n\\n\\na photo of a 2012 acura tl sedan.\\n\\n\\n\\na photo of a 2012 acura tsx sedan.', 'a photo of a 2012 honda accord sedan.\\n\\n\\n\\na photo of a 2012 acura tl sedan.\\n\\n\\n\\na photo of a 2012 acura tsx sedan.\\n\\n\\n\\na photo of a 2008 acura tl type-s.\\n\\ncorrect rank: 1/196    correct probability: 63.30%\\nStanford Cars\\n\\ncorrect label: roundabout\\n\\n0 20 40 60 80 100\\n\\nsatellite imagery of roundabout.\\n\\n\\n\\nsatellite imagery of intersection.\\n\\n\\n\\nsatellite imagery of church.\\n\\n\\n\\nsatellite imagery of medium residential.\\n\\n\\n\\nsatellite imagery of chaparral.\\n\\n\\n\\ncorrect rank: 1/45    correct probability: 96.39%\\nRESISC45\\n\\ncorrect label: Belize\\n\\n0 20 40 60 80 100\\n\\na photo\\n\\ni took in french guiana.\\n\\n\\n\\na photo i took in gabon.\\n\\n\\n\\na photo i took in cambodia.\\n\\n\\n\\na photo i took in guyana.\\n\\n\\n\\na photo i took in belize.\\n\\n\\n\\ncorrect rank: 5/211    correct probability: 3.92%\\nCountry211\\n\\ncorrect label: Boeing 717\\n\\n0 20 40 60 80 100\\n\\na photo of a mcdonnell douglas md-90, a type of aircraft.\\n\\n\\n\\na photo of a boeing 717, a type of aircraft.\\n\\n\\n\\na photo of a fokker 100, a type of aircraft.\\n\\n\\n\\na photo of a mcdonnell douglas dc-9-30, a type of aircraft.\\n\\n\\n\\na photo of a boeing 727-200, a type of aircraft.\\n\\n\\n\\ncorrect rank: 2/100    correct probability: 9.91%\\nFGVC Aircraft\\n\\ncorrect label: beer bottle\\n\\n0\\n\\n20 40 60 80 100\\n\\na photo of a beer bottle.\\n\\n\\n\\na photo of a pirate ship.\\n\\n\\n\\na photo of a chocolate syrup.\\n\\n\\n\\na photo of a product packet / packaging.\\n\\n\\n\\na photo of a wine bottle.\\n\\n\\n\\ncorrect rank: 1/1000    correct probability: 88.27%\\nImageNetV2 Matched Frequency\\n\\ncorrect label: snake\\n\\n0 20 40 60 80 100\\n\\na photo of a snake.\\n\\n\\n\\na photo of a sweet pepper.\\n\\n\\n\\na photo of a flatfish.\\n\\n\\n\\na photo of a turtle.\\n\\n\\n\\na photo of a lizard.\\n\\n\\n\\ncorrect rank: 1/100    correct probability: 38.02%\\nCIFAR-100\\n\\ncorrect label: Maine Coon\\n\\n0 20 40 60 80 100\\n\\na photo of a maine coon, a type of pet.\\n\\n\\n\\na photo of a persian, a type of pet.\\n\\n\\n\\na photo of a ragdoll, a type of pet.\\n\\n\\n\\na photo of a birman, a type of pet.\\n\\n\\n\\na photo of a siamese, a type of pet.\\n\\n\\n\\ncorrect rank: 1/37    correct probability: 99.99%\\nOxford-IIIT Pets\\n\\ncorrect label: Siberian Husky\\n\\n0 20 40 60 80 100\\n\\na photo of a siberian husky.\\n\\n\\n\\na photo of a german shepherd dog.\\n\\n\\n\\na photo of a collie.\\n\\n\\n\\na photo of a border collie.\\n\\n\\n\\na photo of a rottweiler.\\n\\n\\n\\ncorrect rank: 1/200    correct probability: 76.02%\\nImageNet-R (Rendition)\\n\\ncorrect label: kangaroo\\n\\n0 20 40 60 80 100\\n\\na photo of a kangaroo.\\n\\n\\n\\na photo of a gerenuk.\\n\\n\\n\\na photo of a emu.\\n\\na photo of a wild cat.\\n\\na photo of a scorpion.\\n\\n\\n\\ncorrect rank: 1/102    correct probability: 99.81%\\nCaltech-101\\n\\ncorrect label: Volleyball Spiking\\n\\n0 20 40 60 80 100\\n\\na photo of a person volleyball spiking.\\n\\n\\n\\na photo of a person jump rope.\\n\\n\\n\\na photo of a person long jump.\\n\\n\\n\\na photo of a person soccer penalty.\\n\\n\\n\\na photo of a person table tennis shot.\\n\\n\\n\\ncorrect rank: 1/101    correct probability: 99.30%\\nUCF101\\n\\ncorrect label: angry\\n\\n0 20 40 60 80 100\\n\\na photo of a happy looking face.\\n\\n\\n\\na photo of a neutral looking face.\\n\\n\\n\\na photo of a surprised looking face.\\n\\n\\n\\na photo of a fearful looking face.\\n\\n\\n\\na photo of a angry looking face.\\n\\n\\n\\ncorrect rank: 5/7    correct probability: 8.16%\\nFacial Emotion Recognition 2013 (FER2013)\\n\\ncorrect label: 4\\n\\n0 20 40 60 80 100\\n\\na photo of 3 objects.\\n\\n\\n\\na photo of 4 objects.\\n\\n\\n\\na photo of 5 objects.\\n\\n\\n\\na photo of 6 objects.\\n\\n\\n\\na photo of 10 objects.\\n\\n\\n\\ncorrect rank: 2/8    correct probability: 17.11%\\nCLEVR Count\\n\\ncorrect label: bird\\n\\n0 20 40 60 80 100\\n\\na photo of a bird.\\n\\n\\n\\na photo of a cat.\\n\\na photo of a deer.\\n\\n\\n\\na photo of a frog.\\n\\n\\n\\na photo of a dog.\\n\\n\\n\\ncorrect rank: 1/10    correct probability: 40.86%\\nCIFAR-10\\n\\ncorrect label: lynx\\n\\n0 20 40 60 80 100\\n\\na photo of a fox squirrel.\\n\\n\\n\\na photo of a mongoose.\\n\\n\\n\\na photo of a skunk.\\n\\n\\n\\na photo of a red fox.\\n\\n\\n\\na photo of a lynx.', 'a photo of a mongoose.\\n\\n\\n\\na photo of a skunk.\\n\\n\\n\\na photo of a red fox.\\n\\n\\n\\na photo of a lynx.\\n\\n\\n\\ncorrect rank: 5/200    correct probability: 4.18%\\nImageNet-A (Adversarial)\\n\\ncorrect label: healthy lymph node tissue\\n\\n0 20 40 60 80 100\\n\\nthis is a photo of lymph node tumor tissue\\n\\nthis is a photo of healthy lymph node tissue\\n\\ncorrect rank: 2/2    correct probability: 22.81%\\nPatchCamelyon (PCam)\\n\\ncorrect label: annual crop land\\n\\n0 20 40 60 80 100\\n\\na centered satellite photo of permanent crop land.\\n\\n\\n\\na centered satellite photo of pasture land.\\n\\n\\n\\na centered satellite photo of highway or road.\\n\\n\\n\\na centered satellite photo of annual crop land.\\n\\n\\n\\na centered satellite photo of brushland or shrubland.\\n\\n\\n\\ncorrect rank: 4/10    correct probability: 12.90%\\nEuroSAT\\n\\ncorrect label(s): airplane,person\\n\\n0 20 40 60 80 100\\n\\na photo of a airplane.\\n\\n\\n\\na photo of a bird.\\n\\n\\n\\na photo of a bear.\\n\\n\\n\\na photo of a giraffe.\\n\\n\\n\\na photo of a car.\\n\\n\\n\\ncorrect rank: 1/23    correct probability: 88.98%\\nYoutube-BB\\n\\ncorrect label: television studio\\n\\n0 20 40 60 80 100\\n\\na photo of a television studio.\\n\\n\\n\\na photo of a podium indoor.\\n\\n\\n\\na photo of a conference room.\\n\\n\\n\\na photo of a lecture room.\\n\\n\\n\\na photo of a control room.\\n\\n\\n\\ncorrect rank: 1/397    correct probability: 90.22%\\nSUN397\\n\\ncorrect label: guacamole\\n\\n0 20 40 60 80 100\\n\\na photo of guacamole, a type of food.\\n\\n\\n\\na photo of ceviche, a type of food.\\n\\n\\n\\na photo of edamame, a type of food.\\n\\n\\n\\na photo of tuna tartare, a type of food.\\n\\n\\n\\na photo of hummus, a type of food.\\n\\n\\n\\ncorrect rank: 1/101    correct probability: 90.15%\\nFood101\\n\\nFigure 21.\\n\\nVisualization of predictions from 36 CLIP zero-shot classifiers.\\n\\nAll examples are random with the exception of reselecting\\nHateful Memes to avoid offensive content.\\n\\nThe predicted probability of the top 5 classes is shown along with the text used to represent\\nthe class.\\n\\nWhen more than one template is used, the first template is shown.\\n\\nThe ground truth label is colored green while an incorrect\\nprediction is colored orange.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 43\\n\\n\\n\\nFo\\nod\\n\\n10\\n1\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n\\nC\\nIF\\n\\nA\\nR\\n\\n10\\n0\\n\\nB\\nir\\n\\nds\\nna\\n\\np\\n\\nSU\\nN\\n\\n39\\n7\\n\\nSt\\nan\\n\\nfo\\nrd\\n\\nC\\nar\\n\\ns\\n\\nFG\\nV\\n\\nC\\nA\\n\\nir\\ncr\\n\\naf\\nt\\n\\nV\\nO\\n\\nC\\n20\\n\\n07\\n\\nD\\nT\\n\\nD\\n\\nO\\nxf\\n\\nor\\nd\\n\\nPe\\nts\\n\\nC\\nal\\n\\nte\\nch\\n\\n10\\n1\\n\\nFl\\now\\n\\ner\\ns1\\n\\n02\\n\\nM\\nN\\n\\nIS\\nT\\n\\nFE\\nR\\n\\n20\\n13\\n\\nST\\nL\\n\\n10\\n\\nE\\nur\\n\\noS\\nA\\n\\nT\\n\\nR\\nE\\n\\nSI\\nSC\\n\\n45\\n\\nG\\nT\\n\\nSR\\nB\\n\\nK\\nIT\\n\\nT\\nI\\n\\nC\\nou\\n\\nnt\\nry\\n\\n21\\n1\\n\\nPC\\nam\\n\\nU\\nC\\n\\nF1\\n01\\n\\nK\\nin\\n\\net\\nic\\n\\ns7\\n00\\n\\nC\\nL\\n\\nE\\nV\\n\\nR\\n\\nH\\nat\\n\\nef\\nul\\n\\nM\\nem\\n\\nes\\n\\nR\\nen\\n\\nde\\nre\\n\\nd\\nSS\\n\\nT\\n2\\n\\n\\n\\nIm\\nag\\n\\neN\\net\\n\\nC\\nL\\n\\nIP\\n-R\\n\\nes\\nN\\n\\net RN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\\nRN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\\n\\nRN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\\nRN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\\nRN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6\\n\\nC\\nL\\n\\nIP\\n-V\\n\\niT B/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\\nB/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4\\n\\n56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\\nL/14\\n\\n92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\\n\\nL/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\\n\\nTable 11.\\n\\nZero-shot performance of CLIP models over 27 datasets.', 'Zero-shot performance of CLIP models over 27 datasets.\\n\\n\\n\\n101 102\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFood101\\n\\n101 102\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\n95\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR10\\n\\n101 102\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCIFAR100\\n\\n101 102\\n\\n35\\n\\n40\\n\\n45\\n\\n50\\n\\n55\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nBirdsnap\\n\\n101 102\\n\\n60\\n\\n62\\n\\n64\\n\\n66\\n\\n68\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSUN397\\n\\n101 102\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nStanfordCars\\n\\n101 102\\n\\n20\\n\\n30\\n\\n40\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFGVCAircraft\\n\\n101 102\\n82.0\\n\\n82.5\\n\\n83.0\\n\\n83.5\\n\\n84.0\\n\\n84.5\\n\\n11\\n-p\\n\\noi\\nnt\\n\\n m\\nAP\\n\\n o\\nve\\n\\nr 2\\n0 \\n\\ncla\\nss\\n\\nes PascalVOC2007\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nDescribableTextures\\n\\n101 102\\n\\n86\\n\\n88\\n\\n90\\n\\n92\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nOxfordPets\\n\\n101 102\\n82\\n84\\n86\\n88\\n90\\n92\\n\\nm\\nea\\n\\nn-\\npe\\n\\nr-c\\nla\\n\\nss\\n\\nCaltech101\\n\\n101 102\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\n90\\n\\nm\\nea\\n\\nn \\npe\\n\\nr c\\nla\\n\\nss\\n\\nFlowers102\\n\\n101 102\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nMNIST\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nFacialEmotionRecognition2013\\n\\n101 102\\n\\n95\\n\\n96\\n\\n97\\n\\n98\\n\\n99\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSTL10\\n\\n101 102\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nEuroSAT\\n\\n101 102\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nRESISC45\\n\\n101 102\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nGTSRB\\n\\n101 102\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nKITTI\\n\\n101 102\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nPatchCamelyon\\n\\n101 102\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nUCF101\\n\\n101 102\\n\\n45\\n\\n50\\n\\n55\\n\\n60\\n\\nm\\nea\\n\\nn(\\nto\\n\\np1\\n, t\\n\\nop\\n5)\\n\\nKinetics700\\n\\n101 102\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\n40\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCLEVRCounts\\n\\n101 102\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n35\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nCountry211\\n\\n101 102\\n\\nGFLOPs/image\\n\\n54\\n\\n56\\n\\n58\\n\\n60\\n\\n62\\n\\nRO\\nCA\\n\\nUC\\n\\nHatefulMemes\\n\\n101 102\\n\\nGFLOPs/image\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nSST2\\n\\n101 102\\n\\nGFLOPs/image\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\nac\\ncu\\n\\nra\\ncy\\n\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nResNet\\n\\nFigure 22.\\n\\nCLIP’s zero-shot performance compared to linear-probe ResNet performance\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 44\\n\\nB. Zero-Shot Prediction\\nTo provide a qualitative summary / overview of CLIP’s zero-\\nshot performance we visualize a randomly selected predic-\\ntion for 36 different zero-shot CLIP classifiers in Figure\\n21.\\n\\nIn addition, Table 11 and Figure 22 show the individual\\nzero-shot performance scores for each dataset.\\n\\n\\n\\nC. Duplicate Detector\\nOur early attempts at duplicate detection and analysis used\\nnearest neighbors in the model’s learned embedding space.\\n\\n\\nWhile it is intuitive to use a model’s own notion of similar-\\nity, we encountered issues.\\n\\nWe found the model’s feature\\nspace is weighted very heavily towards semantic similar-\\nity.\\n\\nMany false positives occurred due to distinct objects\\nthat would be described similarly (soccer balls, flowers of\\nthe same species, etc...) having almost perfect similarity.\\n\\n\\nWe also observed the model was quite poor at assigning\\ncertain kinds of near-duplicates high similarity scores.\\n\\nWe\\nnoticed repeatedly that images with high-frequency textures\\n(such as fur or stripe patterns) pre-processed by different\\nresizing algorithms (nearest neighbor vs bi-linear) could\\nhave surprisingly low similarity.\\n\\nThis resulted in many false\\nnegatives.\\n\\n\\n\\nWe built our own near-duplicate detector to fix this issue.\\n\\n\\nWe created a synthetic data augmentation pipeline that com-\\nbined a variety of common image manipulations.\\n\\nThe aug-\\nmentation pipeline combines random cropping and zooming,\\naspect ratio distortion, downsizing and upscaling to different\\nresolutions, minor rotations, jpeg compression, and HSV\\ncolor jitter.\\n\\nThe pipeline also randomly selects from differ-\\nent interpolation algorithms for all relevant steps.\\n\\nWe then\\ntrained a model to maximize the similarity of an image and\\nits transformed variant while minimizing similarity to all\\nother images in a training batch.\\n\\nWe used the same n-pair /\\nInfoNCE loss as CLIP but with a fixed temperature of 0.07.\\n\\n\\n\\nWe selected a ResNet-50 as the model architecture.', 'We used the same n-pair /\\nInfoNCE loss as CLIP but with a fixed temperature of 0.07.\\n\\n\\n\\nWe selected a ResNet-50 as the model architecture.\\n\\nWe\\nmodified the base ResNet-50 with the anti-alias improve-\\nments from (Zhang, 2019) and used weight norm (Sali-\\nmans & Kingma, 2016) instead of batch norm (Ioffe &\\nSzegedy, 2015) to avoid leaking information about dupli-\\ncates via batch statistics - a problem previously noted in\\n(Henaff, 2020).\\n\\nWe also found the GELU activation func-\\ntion (Hendrycks & Gimpel, 2016) to perform better for this\\ntask.\\n\\nWe trained the model with a total batch size of 1,712\\nfor approximately 30 million images sampled from our pre-\\ntraining dataset.\\n\\nAt the end of training it achieves nearly\\n100% accuracy on its proxy training task.\\n\\n\\n\\nLinear Classifier Zero Shot\\nDataset YFCC WIT ∆ YFCC WIT ∆\\n\\nBirdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\\nCountry211 23.1 17.3 +5.8 5.2 5.3 +0.1\\nFlowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\\nGTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1\\nUCF101 69.2 74.9 −5.7 22.9 32.0 −9.1\\n\\n\\nStanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1\\n\\nImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7\\nDataset Average 65.5 66.6 −1.1 29.6 30.0\\n\\n−0.4\\nDataset “Wins” 10 15 −5 19 18 +1\\n\\nTable 12.\\n\\nCLIP performs similarly when trained on only\\nYFCC100M. Comparing a ResNet-50 trained on only\\nYFCC100M with a same sized subset of WIT shows simi-\\nlar average performance and number of wins on zero shot and\\nlinear classifier evals.\\n\\nHowever, large differences in dataset\\nspecific performance occur.\\n\\nWe include performance on the 3\\ndatasets where YFCC does best and worst compared to WIT\\naccording to a linear probe in order to highlight this as well as\\naggregate performance across all linear and zero-shot evals and\\nthe canonical ImageNet dataset.\\n\\n\\n\\nD. Dataset Ablation on YFCC100M\\nTo study whether our custom dataset is critical to the perfor-\\nmance of CLIP, we trained a model on a filtered subset of\\nthe YFCC100M dataset (details described in Section 2.2)\\nand compared its performance to the same model trained\\non an equally sized subset of WIT.\\n\\nWe train each model for\\n32 epochs at which point transfer performance begins to\\nplateau due to overfitting.\\n\\nResults are shown in Table 12.\\n\\n\\nAcross our whole eval suite, YFCC and WIT perform simi-\\nlarly on average for both zero-shot and linear probe settings.\\n\\n\\nHowever, performance on specific fine-grained classifica-\\ntion datasets can vary widely - sometimes by over 10%.\\n\\n\\nOur speculation is that these differences in performance re-\\nflect the relative density of relevant data in each pre-training\\ndataset.\\n\\nFor instance, pre-training on YFCC100M, which\\nmight contain many photos of birds and flowers (common\\nsubjects for photographers), results in better performance on\\nBirdsnap and Flowers102, while pre-training on WIT results\\nin better car and pet classifiers (which appear common in\\nour dataset).\\n\\n\\n\\nOverall, these results are encouraging as they suggest our\\napproach can use any reasonably filtered collection of paired\\n(text, image) data.\\n\\nThis mirrors recent work which reported\\npositive results using the same contrastive pre-training ob-\\njective on the relatively different domain of medical imaging\\n(Zhang et al., 2020).\\n\\nIt also is similar to the findings of noisy\\nstudent self-training which reported only slight improve-\\nments when using their JFT300M dataset over YFCC100M\\n(Xie et al., 2020).\\n\\nWe suspect the major advantage of our\\ndataset over the already existing YFCC100M is its much\\nlarger size.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 45\\n\\nFinally, we caution that WIT includes this filtered subset\\nof YFCC100M. This could result in our ablation under-\\nestimating the size of performance differences between\\nYFCC100M and the rest of WIT.\\n\\nWe do not think this is\\nlikely as YFCC100M is only 3.7% of the overall WIT data\\nblend and it did not noticeably change the performance of\\nmodels when it was added to the existing data blend during\\nthe creation of WIT.\\n\\n\\n\\nE. Selected Task and Dataset Results', 'E. Selected Task and Dataset Results\\n\\n\\nDue to the large variety of datasets and experiments consid-\\nered in this work, the main body focuses on summarizing\\nand analyzing overall results.\\n\\nIn the following subsections\\nwe report details of performance for specific groups of tasks,\\ndatasets, and evaluation settings.\\n\\n\\n\\nE.1.\\n\\nImage and Text Retrieval\\n\\nCLIP pre\\n\\n-trains for the task of image-text retrieval on our\\nnoisy web-scale dataset.\\n\\nAlthough the focus of this paper\\nis on representation learning and task learning for the pur-\\npose of transfer to a wide variety of downstream datasets,\\nvalidating that CLIP is able to achieve high transfer perfor-\\nmance transfer on exactly what it is pre-trained for is an\\nimportant sanity check / proof of concept.\\n\\nIn Table 13 we\\ncheck the zero-shot transfer performance of CLIP for both\\ntext and image retrieval on the Flickr30k and MSCOCO\\ndatsets.\\n\\nZero-shot CLIP matches or outperforms all prior\\nzero-shot results on these two datasets.\\n\\nZero-shot CLIP is\\nalso competitive with the current overall SOTA for the task\\nof text retrieval on Flickr30k.\\n\\nOn image retrieval, CLIP’s\\nperformance relative to the overall state of the art is notice-\\nably lower.\\n\\nHowever, zero-shot CLIP is still competitive\\nwith a fine-tuned Unicoder-VL.\\n\\nOn the larger MS-COCO\\ndataset fine-tuning improves performance significantly and\\nzero-shot CLIP is not competitive with the most recent work.\\n\\n\\nFor both these datasets we prepend the prompt “a photo\\nof” to the description of each image which we found boosts\\nCLIP’s zero-shot R@1 performance between 1 and 2 points.\\n\\n\\n\\nE.2.\\n\\nOptical Character Recognition\\n\\nAlthough visualizations have shown that ImageNet models\\ncontain features that respond to the presence of text in an\\nimage (Zeiler & Fergus, 2014), these representations are\\nnot sufficiently fine-grained to use for the task of optical\\ncharacter recognition (OCR).\\n\\nTo compensate, models are\\naugmented with the outputs of custom OCR engines and\\nfeatures to boost performance on tasks where this capability\\nis required (Singh et al., 2019; Yang et al., 2020).\\n\\nEarly dur-\\ning the development of CLIP, we noticed that CLIP began to\\nlearn primitive OCR capabilities which appeared to steadily\\nimprove over the course of the project.\\n\\nTo evaluate this\\nqualitatively noticed behavior, we measured performance\\n\\non 5 datasets requiring the direct and indirect use of OCR.\\n\\n\\nThree of these datasets MNIST (LeCun), SVHN (Netzer\\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\\nthe ability of a model to perform low-level character and\\nword recognition, while Hateful Memes (Kiela et al., 2020)\\nand SST-2 (Socher et al., 2013) check the ability of a model\\nto use OCR to perform a semantic task.\\n\\nResults are reported\\nin Table 14.\\n\\n\\n\\nCLIP’s performance is still highly variable and appears to\\nbe sensitive to some combination of the domain (rendered or\\nnatural images) and the type of text to be recognized (num-\\nbers or words).\\n\\nCLIP’s OCR performance is strongest Hate-\\nful Memes and SST-2 - datasets where the text is digitally\\nrendered and consists mostly of words.\\n\\nOn IIIT5K, which\\nis natural images of individually cropped words, zero-shot\\nCLIP performs a bit more respectively and its performance\\nis similar to Jaderberg et al.\\n\\n(2014) early work combining\\ndeep learning and structured prediction to perform open-\\nvocabulary OCR.\\n\\nHowever, performance is noticeably lower\\non two datasets involving recognition of hand written and\\nstreet view numbers.\\n\\nCLIP’s 51% accuracy on full number\\nSVHN is well below any published results.\\n\\nInspection sug-\\ngests CLIP struggles with repeated characters as well as the\\nlow resolution and blurry images of SVHN.\\n\\nCLIP’s zero-\\nshot MNIST performance is also poor and is outperformed\\nby supervised logistic regression on raw pixels, one of the\\nsimplest possible machine learning baselines.\\n\\n\\n\\nSST-2 is a sentence level NLP dataset which we render into\\nimages.', 'SST-2 is a sentence level NLP dataset which we render into\\nimages.\\n\\nWe include SST-2 in order to check whether CLIP\\nis able to convert low level OCR capability into a higher\\nlevel representation.\\n\\nFitting a linear classifier on CLIP’s rep-\\nresentation of rendered sentences achives 80.5% accuracy.\\n\\n\\nThis is on par with the 80% accuracy of a continuous bag\\nof words baseline using GloVe word vectors pre-trained on\\n840 billion tokens (Pennington et al., 2014).\\n\\nWhile this is a\\nsimple NLP baseline by today’s standard, and well below\\nthe 97.5% of the current SOTA, it is encouraging to see\\nthat CLIP is able to turn an image of rendered text into a\\nnon-trivial sentence level representation.\\n\\nFully supervised\\nCLIP is also surprisingly strong on Hateful Meme detec-\\ntion, where CLIP is only 0.7 points behind the current single\\nmodel SOTA and several points above the best baseline from\\nthe original paper.\\n\\nSimilar to SST-2, these other results on\\nHateful Memes use the ground truth text which CLIP does\\nnot have access to.\\n\\nFinally, we note that zero-shot CLIP\\noutperforms the best results using fully supervised linear\\nprobes across all other 56 models included in our evaluation\\nsuite.\\n\\nThis suggests CLIP’s OCR capability is at least some-\\nwhat unique compared to existing work on self-supervised\\nand supervised representation learning.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 46\\n\\nText Retrieval Image Retrieval\\n\\n\\nFlickr30k MSCOCO Flickr30k MSCOCO\\n\\nR@1 R@5 R@10\\n\\nR@1 R@5 R@10\\n\\nR@1 R@5 R@10\\n\\nR@1 R@5 R@10\\n\\nFi\\nne\\n\\ntu\\nne\\n\\nUnicoder-VLa 86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\\nUniterb 87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\\nVILLAc 87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\\nOscard - - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\\nERNIE-ViLe 88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -\\n\\nZ\\ner\\n\\no-\\nSh\\n\\not Visual N-Gramsf 15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\\nImageBERTg - - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\\nUnicoder-VLa 64.3\\n\\n86.8 92.3 - - - 48.4 76.0 85.2 - - -\\nUniterb 83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\\nCLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2\\n\\nTable\\n\\n13.\\n\\nCLIP improves zero-shot retrieval and is competitive with the best fine-tuned result on Flickr30k text retrieval.\\n\\nBold\\nindicates best overall performance while an underline indicates best in category performance (zero-shot or fine-tuned).\\n\\nFor all other\\nmodels, best results from the paper are reported regardless of model size / variant.\\n\\nMSCOCO performance is reported on the 5k test set.\\n\\n\\na(Li et al., 2020a) b(Chen\\n\\net al., 2019) c(Gan et al., 2020) d(Li et al., 2020b)\\n\\ne(Yu et al., 2020)\\n\\nf (Li et al., 2017) g(Qi et al., 2020)\\n\\nIIIT5K Hateful\\nMNIST SVHN 1k Memes SST-2\\n\\nFi\\nne\\n\\ntu\\nne SOTA 99.8a 96.4b 98.9c 78.0d 97.5e\\n\\nJOINTf - - 89.6 - -\\nCBoWg - - - - 80.0\\n\\nL\\nin\\n\\nea\\nr Raw Pixels 92.5 - - - -\\n\\nES Best 98.9h - - 58.6h 59.0i\\n\\nCLIP 99.2 - - 77.3 80.5\\n\\nZ\\nS\\n\\nCLIP 88.4 51.0 90.0 63.3 67.9\\n\\nTable 14.\\n\\nOCR performance on 5 datasets.\\n\\nAll metrics are accuracy\\non the test set except for Hateful Memes which reports ROC AUC\\non the dev set.\\n\\nSingle model SOTA reported to best of knowledge.\\n\\n\\nES Best reports the best performance across the 56 non-CLIP\\nmodels in our evaluation suite.\\n\\na(Assiri, 2020)\\n\\nb(Jaderberg et al.,\\n2015) c(Wang\\n\\net al., 2020) d(Lippe et al., 2020) f (Jaderberg et al.,\\n2014) g(Wang\\n\\net al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\\n2018)\\n\\nE.3.\\n\\nAction Recognition in Videos\\n\\nFor the purpose of learning, a potentially important aspect\\nof natural language is its ability to express, and therefore su-\\npervise, an extremely wide set of concepts.\\n\\nA CLIP model,\\nsince it is trained to pair semi-arbitrary text with images, is\\nlikely to receive supervision for a wide range of visual con-\\ncepts involving both common and proper nouns, verbs, and\\nadjectives.\\n\\nImageNet-1K, by contrast, only labels common\\nnouns.', 'ImageNet-1K, by contrast, only labels common\\nnouns.\\n\\nDoes the lack of broader supervision in ImageNet\\nresult in weaker transfer of ImageNet models to tasks involv-\\ning the recognition of visual concepts that are not nouns?\\n\\n\\n\\nTo investigate this, we measure and compare the perfor-\\nmance of CLIP and ImageNet models on several video\\n\\nUCF101 K700 RareAct\\nTop-1 AVG mWAP mWSAP\\n\\nFi\\nne\\n\\ntu\\nne R(2\\n\\n+1)D-BERTa 98.7 - - -\\n\\nNS ENet-L2b - 84.8 - -\\nHT100M S3Dd 91.3 - - -\\nBaseline I3De - 70.2 - -\\n\\nL\\nin\\n\\nea\\nr MMV FACf 91.8 - - -\\n\\nNS ENet-L2c 89.4c\\n\\n68.2c - -\\nCLIP 92.0 73.0 - -\\n\\nZ\\nS HT100M S3Dd - - 30.5 34.8\\n\\nCLIP 80.3 69.6 40.7 44.8\\n\\nTable 15.\\n\\nAction recognition performance on 3 video datasets.\\n\\nSin-\\ngle model SOTA reported to best of knowledge.\\n\\nNote that linear\\nCLIP and linear NS ENet-L2 are trained and evaluated on a single\\nframe subsampled version of each dataset and not directly compa-\\nrable to prior work.\\n\\nOn Kinetics-700, we report the ActivityNet\\ncompetition metric which is the average of top-1 and top-5 per-\\nformance.\\n\\na(Kalfaoglu et al., 2020)\\n\\nb(Lu et al., 2020) c(Xie et al.,\\n2020) d(Miech\\n\\net al., 2020b) e(Carreira et al., 2019)\\n\\nf (Alayrac\\net al., 2020)\\n\\naction classification datasets which measure the ability of a\\nmodel to recognize verbs.\\n\\nIn Table 15 we report results on\\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\\net al., 2019), two common datasets for the task.\\n\\nUnfortu-\\nnately, our CPU based linear classifier takes a prohibitively\\nlong time to evaluate on a video dataset due to the very large\\nnumber of training frames.\\n\\nTo deal with this, we aggres-\\nsively sub-sample each video to only a single center frame,\\neffectively turning it into an image classification dataset.\\n\\n\\nAs a result, our reported performance in a linear evaluation\\nsetting likely under estimates performance by a moderate\\namount.\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 47\\n\\nIN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\\nTop-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10\\n\\nNS EfficientNet-L2a 88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\\nFixResNeXt101-32x48d V2b 86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\\nLinear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\\nZero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5\\n\\nTable 16.\\n\\nDetailed ImageNet robustness performance.\\n\\nIN is used to abbreviate for ImageNet. a(Xie et al., 2020)\\n\\nb(Touvron et al., 2019)\\n\\nDespite this handicap, CLIP features transfer surprisingly\\nwell to this task.\\n\\nCLIP matches the best prior result on UCF-\\n101 in a linear probe evaluation setting and also outperforms\\nall other models in our evaluation suite.\\n\\nOn Kinetics-700,\\nCLIP also outperforms the fine-tuned I3D baseline from the\\noriginal paper.\\n\\nSince it does not require a training stage,\\nwe report CLIP’s zero-shot performance when averaging\\npredictions across all frames.\\n\\nCLIP also performs well in\\nthis setting and on Kinetics-700 its performance is within\\n1% of the fully supervised I3D baseline which is trained\\non 545000 labeled videos.\\n\\nEncouraged by these results, we\\nalso measure CLIP’s performance on the recently introduced\\nRareAct dataset (Miech et al., 2020a) which was designed\\nto measure zero-shot recognition of unusual actions like\\n“hammering a phone” and “drilling an egg”.\\n\\nCLIP improves\\nover the prior state of the art, a S3D model trained on auto-\\nmatically extracted captions from 100 million instructional\\nvideos, by 10 points.\\n\\n\\n\\nWhile CLIP has encouragingly strong performance on the\\ntask of action recognition, we note that there are many differ-\\nences between the models being compared beyond just their\\nform of supervision such as model architecture, training\\ndata distribution, dataset size, and compute used.', 'Further\\nwork is needed to more precisely determine what specific\\ndesign decisions contribute to achieving high performance\\non this task.\\n\\n1km 25km 200km 750km 2500km\\n\\nISNsa 16.9 43.0 51.9 66.7 80.2\\nCPlaNetb 16.5 37.1 46.4 62.0 78.5\\nCLIP 13.9 32.9 43.0 62.0 79.3\\nDeep-Ret+c 14.4 33.3 47.7 61.6 73.4\\nPlaNetd 8.4 24.5 37.6 53.6 71.3\\n\\nTable 17.\\n\\nGeolocalization performance on the IM2GPS test set.\\n\\n\\nMetric is percent of images localized within a given radius.\\n\\nModels\\nare ordered by average performance.\\n\\na(Muller-Budack et al., 2018)\\nb(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand\\n\\net al.,\\n2016)\\n\\nE.4.\\n\\nGeolocalization\\n\\nAnother behavior we noticed during the development of\\nCLIP was its ability to recognize many places and locations.\\n\\n\\nTo quantify this we created the Country211 dataset as de-\\nscribed in Appendix A and report results on it throughout\\nthe paper.\\n\\nHowever it is a new benchmark so to compare\\nwith prior work on geolocalization we also report results\\non the IM2GPS test set from Hays & Efros (2008) in Table\\n17.\\n\\nSince IM2GPS is a regression benchmark, we guess the\\nGPS coordinates of the nearest image in a set of reference\\nimages using CLIP’s embedding space.\\n\\nThis is not a zero-\\nshot result since it uses nearest-neighbor regression.\\n\\nDespite\\nquerying only 1 million images, which is much less than\\nprior work, CLIP performs similarly to several task specific\\nmodels.\\n\\nIt is not, however, competitive with the current state\\nof the art.\\n\\nE.5.\\n\\nRobustness to Distribution Shift\\n\\nSection 3.3 provides a high level summary and analysis of\\nImageNet-related robustness results.\\n\\nWe briefly provide\\nsome additional numerical details in this appendix.\\n\\nPer-\\nformance results per dataset are provided in Table 16 and\\ncompared with the current state of the art results reported\\nin Taori et al.\\n\\n(2020)’s evaluation suite.\\n\\nZero-shot CLIP im-\\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\\nBB.\\n\\nCLIP’s improvements are largest on ImageNet-Vid and\\nYoutube-BB due to its flexible zero-shot capability and on\\nImageNet-R, which likely reflects CLIP’s pre-training dis-\\ntribution including significant amounts of creative content.\\n\\n\\nA similar behavior has been documented for the Instagram\\npre-trained ResNeXt models as discussed in Taori et al.\\n(2020).\\n\\n\\n\\n\\n\\nLearning Transferable Visual Models From Natural Language Supervision 48\\n\\nF. Model Hyperparameters\\n\\nHyperparameter Value\\n\\nBatch size 32768\\nVocabulary size 49408\\nTraining epochs 32\\nMaximum temperature 100.0\\nWeight decay 0.2\\nWarm-up iterations 2000\\nAdam β1 0.9\\nAdam β2 0.999 (ResNet), 0.98 (ViT)\\n\\n\\nAdam ε 10−8 (ResNet), 10−6 (ViT)\\n\\n\\n\\nTable 18.\\n\\nCommon CLIP hyperparameters\\n\\nLearning Embedding Input ResNet Text Transformer\\nModel rate dimension resolution blocks width layers width\\n\\nheads\\n\\nRN50 5× 10−4 1024 224 (3, 4, 6, 3)\\n\\n2048 12 512 8\\nRN101 5×\\n\\n10−4 512 224 (3, 4, 23, 3) 2048 12 512 8\\nRN50x4\\n\\n5× 10−4 640 288 (4, 6, 10, 6) 2560 12 640 10\\nRN50x16 4× 10−4 768 384 (6, 8, 18, 8) 3072 12 768 12\\nRN50x64 3.6× 10−4 1024 448 (3, 15, 36, 10) 4096 12 1024 16\\n\\nTable 19.\\n\\nCLIP-ResNet hyperparameters\\n\\nLearning Embedding Input Vision Transformer Text Transformer\\nModel rate dimension resolution layers width heads layers width heads\\n\\nViT-B/32\\n\\n5× 10−4 512 224 12 768 12 12 512 8\\nViT-B/16 5× 10−4 512 224 12 768 12 12 512 8\\nViT-L/14\\n\\n4× 10−4 768 224 24 1024\\n\\n16 12 768 12\\nViT-L/14-336px 2×\\n\\n10−5 768 336 24 1024 16 12 768 12\\n\\nTable 20.\\n\\nCLIP-ViT hyperparameters']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "import langchain.text_splitter as ts\n",
    "\n",
    "# Initialize the SpacyTextSplitter\n",
    "spacy_text_splitter = ts.SpacyTextSplitter()\n",
    "\n",
    "# Split the text into sentences\n",
    "#text = \"This is the first sentence. This is the second sentence.\"\n",
    "for onesentence in sentences:\n",
    "    chunks = spacy_text_splitter.split_text(onesentence)\n",
    "    # Print the sentences\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "['Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] i; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays for Royal Challengers Bangalore in the IPL and Delhi in domestic cricket.\\n\\nConsidered to be one of the best cricketers in the world, he is widely regarded as one of the greatest batsmen in the history of the sport.[4] Nicknamed \"The King\", due to his dominant style of play and popularity, Kohli holds numerous records in his career across all formats.\\n\\nIn x2020, the International Cricket Council named him the male cricketer of the decade.\\n\\nKohli has also contributed to India\\'s successes, captaining the team from 2014 to 2022, and winning the 2011 World Cup and the 2013 Champions trophy.\\n\\nHe is among the only four Indian cricketers who have played over 500 matches for India.[5]\\n\\nBorn and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team.\\n\\nHe made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011.\\n\\nIn 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time.\\n\\nDuring 2014 T20 World Cup, he set a record for the most runs scored in the tournament.\\n\\nIn 2018, he achieved yet another milestone, becoming the world\\'s top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game.\\n\\nHis form continued in 2019, when he became the first player to score 20,000 international runs in a single decade.\\n\\nIn 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\\n\\n\\n\\nHe has received many accolades for his performances on the cricket field.\\n\\nHe was recognized as the ICC ODI Player of the Year in 2012 and has won the Sir Garfield Sobers Trophy, given to the ICC Cricketer of the Year, on two occasions, in 2017 and 2018 respectively.\\n\\nSubsequently, Kohli also won ICC Test Player of the Year and ICC ODI Player of the Year awards in 2018, becoming the first player to win both awards in the same year.\\n\\nAdditionally, he was named the Wisden Leading Cricketer in the World for three consecutive years, from 2016 to 2018.\\n\\nAt the national level, Kohli was honoured with the Arjuna Award in 2013, the Padma Shri under the sports category in 2017 and the Khel Ratna award, India\\'s highest sporting honour, in 2018.\\n\\n\\n\\nIn 2016, he was ranked as one of the world\\'s most famous athletes by ESPN, and one of the most valuable athlete brands by Forbes.\\n\\nIn 2018, Time magazine included him on its list of the 100 most influential people in the world.\\n\\nIn 2020, he was ranked 66th in Forbes list of the top 100 highest-paid athletes in the world for the year 2020 with estimated earnings of over $26 million.\\n\\nKohli has been deemed one of the most commercially viable cricketers, with estimated earnings of ₹165 crore (US$21 million) in the year 2022.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "\n",
      "\n",
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "\n",
      "Alec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1\n",
      "\n",
      "Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\n",
      "\n",
      "Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1\n",
      "\n",
      "Abstract\n",
      "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories.\n",
      "\n",
      "This restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept.\n",
      "\n",
      "Learning directly from raw\n",
      "text about images is a promising alternative which\n",
      "leverages a much broader source of supervision.\n",
      "\n",
      "\n",
      "We demonstrate that the simple pre-training task\n",
      "of predicting which caption goes with which im-\n",
      "age is an efficient and scalable way to learn SOTA\n",
      "image representations from scratch on a dataset\n",
      "of 400 million (image, text) pairs collected from\n",
      "the internet.\n",
      "\n",
      "After pre-training, natural language\n",
      "is used to reference learned visual concepts (or\n",
      "describe new ones) enabling zero-shot transfer\n",
      "of the model to downstream tasks.\n",
      "\n",
      "We study\n",
      "the performance of this approach by benchmark-\n",
      "ing on over 30 different existing computer vi-\n",
      "sion datasets, spanning tasks such as OCR, ac-\n",
      "tion recognition in videos, geo-localization, and\n",
      "many types of fine-grained object classification.\n",
      "\n",
      "\n",
      "The model transfers non-trivially to most tasks\n",
      "and is often competitive with a fully supervised\n",
      "baseline without the need for any dataset spe-\n",
      "cific training.\n",
      "\n",
      "For instance, we match the ac-\n",
      "curacy of the original ResNet-50 on ImageNet\n",
      "zero-shot without needing to use any of the 1.28\n",
      "million training examples it was trained on.\n",
      "\n",
      "We\n",
      "release our code and pre-trained model weights at\n",
      "https://github.com/OpenAI/CLIP.\n",
      "\n",
      "1.\n",
      "\n",
      "Introduction and Motivating Work\n",
      "Pre-training methods which learn directly from raw text\n",
      "have revolutionized NLP over the last few years (Dai &\n",
      "Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\n",
      "ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\n",
      "\n",
      "\n",
      "\n",
      "*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\n",
      "\n",
      "\n",
      "Correspondence to: <{alec, jongwook}@openai.com>.\n",
      "\n",
      "\n",
      "\n",
      "Task-agnostic objectives such as autoregressive and masked\n",
      "language modeling have scaled across many orders of mag-\n",
      "nitude in compute, model capacity, and data, steadily im-\n",
      "proving capabilities.\n",
      "\n",
      "The development of “text-to-text” as\n",
      "a standardized input-output interface (McCann et al., 2018;\n",
      "Radford et al., 2019; Raffel et al., 2019) has enabled task-\n",
      "agnostic architectures to zero-shot transfer to downstream\n",
      "datasets removing the need for specialized output heads or\n",
      "dataset specific customization.\n",
      "\n",
      "Flagship systems like GPT-3\n",
      "(Brown et al., 2020) are now competitive across many tasks\n",
      "with bespoke models while requiring little to no dataset\n",
      "specific training data.\n",
      "\n",
      "\n",
      "\n",
      "These results suggest that the aggregate supervision acces-\n",
      "sible to modern pre-training methods within web-scale col-\n",
      "lections of text surpasses that of high-quality crowd-labeled\n",
      "NLP datasets.\n",
      "\n",
      "However, in other fields such as computer\n",
      "vision it is still standard practice to pre-train models on\n",
      "crowd-labeled datasets such as ImageNet (Deng et al., 2009).\n",
      "\n",
      "\n",
      "Could scalable pre-training methods which learn directly\n",
      "from web text result in a similar breakthrough in computer\n",
      "vision?\n",
      "\n",
      "Prior work is encouraging.\n",
      "\n",
      "\n",
      "\n",
      "Over 20 years ago Mori et al.\n",
      "\n",
      "(1999) explored improving\n",
      "content based image retrieval by training a model to pre-\n",
      "dict the nouns and adjectives in text documents paired with\n",
      "images.\n",
      "\n",
      "Quattoni et al.\n",
      "\n",
      "(2007) demonstrated it was possi-\n",
      "ble to learn more data efficient image representations via\n",
      "manifold learning in the weight space of classifiers trained\n",
      "to predict words in captions associated with images.\n"
     ]
    }
   ],
   "source": [
    "print (chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning**:\n",
    "1. Notice the last four \"sentences\" are exceeding the token limits of the model (bert-base-uncased), which has a token limit of 512. In spite of that we got good results!\n",
    "\n",
    "2. (in other notebook) Fixed size paragraph chunking didn't work (it was character based. say a number.. 256) as the paragraphs could be longer than 256 \"characters\". \n",
    "\n",
    "Example look at the following:\n",
    "- Created a chunk of size 588, which is longer than the specified 256\n",
    "- Created a chunk of size 781, which is longer than the specified 256\n",
    "- Created a chunk of size 710, which is longer than the specified 256\n",
    "\n",
    "3. How about Naive splitting? \n",
    "    like text.split(\".\") #it splits into too many chunks. we lose the \"context\" in it. (there's no chunk overlapping in here)\n",
    "4. Recursive Chunking may be better (we used it in our code. )\n",
    "    - try out again here. \n",
    "5. maybe its better to do chunkin with strategy no. 1?\n",
    "6. Sapcy,NLTK with langchain also does semantic \"sentence based\" chunking.\n",
    "7. We don't yet have a solution for *.txt IEEE paper abstracts getting split as they are in small column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsentences = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt was the best of times, it was the worst of times, \\nit was the age of wisdom, it was the age of foolishness, \\nit was the epoch of belief, it was the epoch of incredulity, \\nit was the season of light, it was the season of darkness, \\nit was the spring of hope, it was the winter of despair, \\nwe had everything before us, we had nothing before us, \\nwe were all going direct to heaven, \\nwe were all going direct the other way–in short, \\nthe period was so far like the present period, \\nthat some of its noisiest authorities insisted on its being received, \\nfor good or for evil, in the superlative degree of comparison only.\\n'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 22:56:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 18389698 ms exceeds timeout 120000 ms\n",
      "23/09/20 22:56:25 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "23/09/20 22:56:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:56:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:57:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:58:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 22:59:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:00:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:01:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:02:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:03:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "23/09/20 23:04:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:04:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:05:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:06:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:06:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:06:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:06:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unknownf4c88a64f6a4.attlocal.net:35917\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "23/09/20 23:06:17 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststring = str(dfsentences.values[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt was the best of times, it was the worst of times, \\nit was the age of wisdom, it was the age of foolishness, \\nit was the epoch of belief, it was the epoch of incredulity, \\nit was the season of light, it was the season of darkness, \\nit was the spring of hope, it was the winter of despair, \\nwe had everything before us, we had nothing before us, \\nwe were all going direct to heaven, \\nwe were all going direct the other way–in short, \\nthe period was so far like the present period, \\nthat some of its noisiest authorities insisted on its being received, \\nfor good or for evil, in the superlative degree of comparison only.\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teststring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "#sents, vecs = process(str(dfsentences.values[0,0]))\n",
    "sents, vecs = process(teststring)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < 1500:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > 3000:\n",
    "        threshold = 0.6\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < 60 or div_len > 3000:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print (final_texts)\n",
    "# Print each chunk with an index and comment indicating its position in the original text\n",
    "for i, chunk in enumerate(final_texts):\n",
    "    print(f\"-----Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2023, 7775, 2433, 1997, 3565, 1011, 4432...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1006, 2289, 1007, 7645, 2009, 2001, 1343...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 2096, 24404, 8405, 3802, 2632, 1012, 100...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 2057, 17902, 2008, 2054, 2003, 2691, 240...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2035, 2122, 8107, 2024, 4083, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[101, 4083, 2013, 3019, 2653, 2038, 2195, 4022...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[101, 2009, 1521, 1055, 2172, 6082, 2000, 4094...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[101, 2612, 1010, 4725, 2029, 2147, 2006, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[101, 4083, 2013, 3019, 2653, 2036, 2038, 2019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[101, 1999, 1996, 2206, 4942, 29015, 2015, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[101, 4526, 1037, 12949, 2312, 2951, 13462, 44...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[101, 2096, 5796, 1011, 25033, 1998, 5107, 134...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[101, 2011, 7831, 1010, 2060, 3274, 4432, 3001...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[101, 1061, 11329, 2278, 18613, 2213, 1010, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[101, 2116, 4871, 2224, 8285, 18900, 2072, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[101, 1037, 2350, 14354, 2005, 3019, 2653, 104...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[101, 2144, 4493, 2951, 13462, 2015, 2079, 202...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[101, 2000, 4769, 2023, 1010, 2057, 3833, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[101, 1996, 4525, 2951, 13462, 2038, 1037, 271...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[101, 2119, 2122, 8107, 3745, 1037, 3145, 1440...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[101, 1999, 3275, 1017, 2057, 2421, 18404, 160...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[101, 2057, 2036, 5672, 1996, 3795, 2779, 4770...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[101, 1996, 3793, 4372, 16044, 2099, 2003, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[101, 2004, 1037, 2918, 2946, 2057, 2224, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[101, 2096, 3025, 3274, 4432, 2470, 2038, 2411...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[101, 2731, 2057, 3345, 1037, 2186, 1997, 1019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[101, 2027, 2024, 19537, 2004, 29300, 12376, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[101, 2005, 1996, 4432, 19081, 2057, 3345, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[101, 2057, 2224, 1996, 4205, 23569, 27605, 62...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[101, 23760, 1011, 11709, 2020, 2059, 5967, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[101, 1996, 4553, 3085, 4860, 16381, 1174, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[101, 2000, 2256, 3716, 1010, 5107, 1050, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[101, 1996, 2190, 12528, 2944, 24840, 10640, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[101, 5678, 1010, 1996, 2327, 1011, 1019, 1064...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[101, 1996, 3754, 2000, 2674, 1996, 2836, 1997...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[101, 3019, 2653, 10429, 1021, 1037, 17560, 95...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[101, 13599, 12528, 2000, 3188, 5717, 1011, 29...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[101, 12528, 24840, 2836, 2006, 2035, 2093, 29...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[101, 2023, 7620, 11138, 2116, 5966, 1999, 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[101, 12528, 2003, 1037, 3278, 3357, 2875, 123...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[101, 1996, 6565, 3484, 1997, 2951, 13462, 201...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[101, 2005, 2116, 2951, 13462, 2015, 1010, 205...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[101, 25732, 3330, 1998, 4372, 3366, 29256, 53...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[101, 2057, 2179, 2006, 2195, 2986, 1011, 8982...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[101, 4106, 1997, 5717, 1011, 2915, 12528, 283...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[101, 2004, 1037, 2034, 3160, 1010, 2057, 2298...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[101, 2000, 9530, 1011, 25304, 4697, 2023, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[101, 5717, 1011, 2915, 12528, 2041, 4842, 226...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[101, 5717, 1011, 2915, 12528, 2003, 6975, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[101, 2559, 2012, 3265, 2951, 13462, 2015, 765...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[101, 2006, 2048, 1997, 2122, 2951, 13462, 201...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[101, 2057, 8343, 2122, 4489, 2024, 3952, 2349...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[101, 2006, 1523, 2236, 1524, 4874, 2465, 1851...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[101, 2057, 28699, 9869, 2023, 2003, 2349, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[101, 2559, 2012, 2073, 5717, 1011, 2915, 1252...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[101, 5717, 1011, 2915, 12528, 2041, 4842, 226...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>[101, 1996, 2322, 2951, 13462, 2015, 2007, 201...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>[101, 2057, 2156, 2008, 5717, 1011, 2915, 1252...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>[101, 2122, 3463, 12944, 1996, 3532, 10673, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[101, 2011, 5688, 1010, 2512, 1011, 6739, 4286...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[101, 2034, 1010, 12528, 1521, 1055, 5717, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[101, 2011, 5688, 1010, 1523, 3671, 1524, 1358...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[101, 6123, 1011, 2625, 2742, 1011, 2241, 4553...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>[101, 2348, 1037, 5214, 4553, 2121, 2003, 2583...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>[101, 2057, 2424, 2008, 5717, 1011, 2915, 4651...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>[101, 1996, 2951, 8122, 1997, 5717, 1011, 2915...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[101, 5300, 2024, 4358, 2241, 2006, 8833, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>[101, 2836, 9783, 4235, 2013, 2145, 2104, 4842...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>[101, 2048, 2951, 13462, 2015, 1010, 4870, 107...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[101, 2431, 1997, 1996, 2951, 13462, 2015, 547...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[101, 2174, 1010, 1996, 2812, 4358, 2951, 8122...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[101, 2023, 2003, 2349, 2000, 1996, 2322, 1003...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[101, 2006, 3746, 7159, 1010, 5717, 1011, 2915...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>[101, 1996, 18198, 1010, 1061, 1027, 1060, 224...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[101, 5717, 1011, 2915, 2836, 2003, 23900, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[101, 2158, 3401, 1010, 9104, 2008, 12528, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[101, 2174, 1010, 5717, 1011, 2915, 12528, 206...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[101, 2023, 6083, 2008, 12528, 2089, 2022, 206...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[101, 2058, 1996, 2627, 2261, 2086, 1010, 1753...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[101, 1996, 14246, 2102, 2155, 1997, 4275, 203...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>[101, 2096, 1996, 3452, 9874, 2003, 5744, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[101, 6630, 4083, 2096, 2057, 2031, 8077, 1657...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>[101, 2000, 18478, 4989, 3896, 2008, 2071, 533...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>[101, 2122, 2235, 12528, 4275, 2036, 2104, 484...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>[101, 2057, 2036, 2424, 2008, 12528, 4432, 190...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[101, 2122, 3463, 24209, 11475, 27453, 2135, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>[101, 1006, 12609, 1007, 2029, 2988, 2008, 443...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>[101, 2256, 2190, 3452, 2944, 2003, 1037, 6819...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>[101, 2004, 3275, 2538, 24209, 11475, 27453, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[101, 2122, 8518, 2421, 20248, 1011, 2334, 398...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>[101, 3904, 1997, 2122, 8518, 2024, 7594, 1999...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>[101, 2000, 4769, 2023, 1010, 2057, 2036, 5468...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[101, 2023, 9312, 7621, 1010, 6851, 1999, 2252...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[101, 7399, 15113, 2836, 1997, 12528, 4275, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[101, 1006, 2157, 1007, 7644, 2024, 11398, 205...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[101, 2005, 6013, 1010, 2096, 21934, 20464, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[101, 1037, 2691, 4323, 1997, 3818, 17959, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[101, 2057, 14046, 2008, 1010, 2000, 3058, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[101, 21195, 1996, 8476, 1997, 6594, 1010, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[101, 2000, 2054, 3014, 2024, 2122, 15428, 201...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[101, 20216, 3089, 3802, 2632, 1012, 1006, 126...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>[101, 2027, 5468, 2836, 2006, 1037, 2275, 1997...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[101, 2027, 16599, 2023, 7835, 2138, 1999, 211...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[101, 1037, 24501, 7159, 1011, 7886, 3084, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[101, 11434, 2135, 2174, 1010, 20216, 3089, 38...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[101, 4621, 15873, 2791, 5761, 8377, 1999, 106...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[101, 20216, 3089, 3802, 2632, 1012, 1006, 126...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[101, 2947, 2009, 2003, 9608, 2000, 5987, 5717...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>[101, 5717, 1011, 2915, 12528, 2003, 2172, 206...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[101, 2005, 1996, 2711, 2465, 1999, 7858, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>[101, 12528, 4275, 5967, 2000, 3746, 7159, 203...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>[101, 7831, 2000, 2529, 2836, 2129, 2515, 1252...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>[101, 2000, 2131, 1037, 2488, 4824, 1997, 2129...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>[101, 2057, 2359, 2000, 2131, 1037, 3168, 1997...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>[101, 2023, 2064, 2393, 2149, 2000, 12826, 470...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>[101, 2057, 2018, 2274, 2367, 4286, 2298, 2012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>[101, 1999, 1996, 5717, 1011, 2915, 2553, 1996...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>[101, 2261, 1011, 2915, 12528, 2036, 7457, 462...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[101, 7163, 4328, 6774, 1996, 3815, 1997, 3746...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[101, 1996, 5114, 1999, 10640, 2183, 2013, 571...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[101, 2445, 2023, 1010, 2009, 3849, 2008, 2096...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[101, 2138, 2122, 2261, 1011, 2915, 9312, 2015...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[101, 2000, 2256, 3716, 1010, 2478, 1037, 7399...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[101, 2004, 1999, 2380, 4048, 3802, 2632, 1012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[101, 2087, 1997, 1996, 5114, 1999, 2836, 2043...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[101, 1523, 3984, 2229, 1524, 5218, 2000, 2699...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[101, 3737, 3653, 1011, 4738, 2944, 2003, 2379...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[101, 2065, 2057, 5436, 2529, 10640, 5443, 125...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[101, 2000, 1996, 6698, 2008, 10697, 2024, 833...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[101, 1019, 1012, 2951, 17702, 4106, 1037, 514...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>[101, 2028, 5724, 2000, 4652, 2023, 2003, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>[101, 2096, 2023, 21586, 7316, 2995, 2907, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>[101, 2023, 2038, 1996, 12482, 5178, 1997, 148...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>[101, 5815, 1037, 2047, 9312, 2052, 5478, 2019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>[101, 1037, 12654, 1997, 2023, 4106, 2003, 359...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>[101, 2261, 7778, 2135, 3278, 8377, 1999, 1064...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>[101, 1006, 2187, 1007, 2096, 2195, 2951, 1346...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>[101, 1006, 2157, 1007, 2144, 1996, 7017, 1997...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>[101, 2566, 11365, 2098, 26163, 1997, 1037, 73...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>[101, 3278, 2147, 2003, 2145, 2734, 2000, 5335...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>[101, 2096, 25169, 2038, 2061, 2521, 11328, 53...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>[101, 2023, 2003, 1999, 7959, 21369, 3468, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>[101, 4106, 1999, 2930, 1017, 1012, 1015, 2179...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[101, 2057, 2024, 9657, 2008, 2045, 2024, 2145...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>[101, 2174, 1010, 12528, 2069, 6162, 2015, 607...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>[101, 2019, 16436, 2135, 3722, 26163, 1997, 88...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>[101, 2023, 6083, 12528, 2515, 2210, 2000, 476...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[101, 2023, 2003, 1037, 15743, 11213, 2008, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>[101, 2348, 12528, 2064, 23951, 17296, 9699, 5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>[101, 6854, 1010, 2004, 2649, 1999, 10819, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>[101, 1037, 3722, 2801, 4276, 2667, 2003, 4101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>[101, 2612, 12528, 19079, 2015, 2011, 2478, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>[101, 11566, 12528, 2007, 2969, 1011, 10429, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>[101, 2023, 10673, 13999, 7860, 2714, 2000, 22...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>[101, 2005, 2742, 1010, 2009, 2064, 2424, 7882...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>[101, 2116, 1997, 12528, 1521, 1055, 9859, 202...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>[101, 2445, 2049, 2591, 13494, 1010, 2057, 476...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[101, 2057, 2031, 2036, 4912, 2000, 2839, 4697...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>[101, 2256, 13827, 5852, 5050, 2256, 3988, 407...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>[101, 3867, 10640, 2006, 5907, 5579, 1997, 487...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>[101, 3867, 1997, 4871, 6219, 2046, 4126, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[101, 1996, 3125, 1997, 2023, 7551, 2001, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>[101, 2057, 2179, 2008, 2023, 21040, 4359, 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>[101, 2057, 2036, 3344, 2041, 7885, 2714, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[101, 2057, 3344, 2041, 2093, 7885, 1011, 2057...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>[101, 2057, 2034, 3432, 2246, 2046, 5907, 1754...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>[101, 2023, 2003, 3621, 2488, 2836, 2084, 1996...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[101, 2057, 1044, 22571, 14573, 2229, 4697, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>[101, 1999, 2344, 2000, 2817, 2129, 1996, 1382...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[101, 2057, 2179, 2008, 1996, 2896, 11207, 241...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>[101, 2174, 1010, 2130, 1996, 16965, 20611, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>[101, 2005, 2742, 1010, 2057, 2424, 2008, 2104...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>[101, 2023, 2685, 2000, 5907, 2098, 8924, 2714...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>[101, 2012, 1996, 3020, 1018, 1003, 11207, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[101, 9867, 2057, 2279, 4912, 2000, 2839, 4697...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>[101, 12528, 2836, 2006, 2266, 1997, 3519, 487...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>[101, 1996, 2322, 2087, 5907, 2098, 10873, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>[101, 6963, 19090, 1996, 7017, 1997, 4871, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[101, 2256, 10502, 1997, 9867, 2003, 2025, 383...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>[101, 2057, 5468, 1996, 2944, 1521, 1055, 2836...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>[101, 2057, 2109, 1996, 6819, 8609, 2951, 1346...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>[101, 2445, 12528, 1521, 1055, 12379, 2465, 28...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[101, 20392, 5579, 3223, 1996, 2944, 2000, 111...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>[101, 5678, 1010, 2057, 3344, 2041, 1037, 1520...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>[101, 2057, 2179, 2008, 1996, 2944, 2018, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>[101, 3602, 2008, 2023, 7551, 2001, 9416, 2069...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>[101, 2057, 2036, 7718, 12528, 1521, 1055, 571...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>[101, 2023, 1022, 22074, 1024, 1996, 8292, 257...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>[101, 2349, 2000, 1996, 3267, 1997, 1996, 2951...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>[101, 3141, 2147, 2151, 2944, 2008, 21155, 201...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[101, 2023, 2003, 2019, 4914, 2135, 5186, 5041...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[101, 2009, 2036, 2950, 2172, 1997, 1996, 1236...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[101, 13764, 8649, 2241, 4083, 1006, 12755, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[101, 12528, 2003, 2019, 2742, 1997, 2478, 301...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[101, 1999, 2023, 6123, 1010, 1996, 5700, 2224...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[101, 12528, 1521, 1055, 3653, 1011, 2731, 470...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>[101, 2023, 2752, 1997, 2470, 5246, 2067, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>[101, 2096, 3988, 4073, 4208, 3952, 2006, 3653...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>[101, 2023, 2240, 1997, 2147, 10861, 5134, 374...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>[101, 7091, 2057, 2031, 10847, 3251, 2009, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>[101, 20934, 6030, 2213, 10105, 2072, 1010, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>[101, 12385, 21302, 1010, 1046, 1012, 1010, 13...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>[101, 8802, 1010, 1049, 1012, 1010, 10958, 209...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>[101, 8802, 1010, 1056, 1012, 1010, 15990, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>[101, 8802, 1010, 1056, 1012, 1010, 12849, 682...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>[101, 8802, 1010, 1056, 1012, 1010, 12849, 682...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>[101, 8802, 1010, 1060, 1012, 1998, 20512, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>[101, 8802, 1010, 1061, 1012, 1011, 1039, 1012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>[101, 15898, 1010, 1043, 1012, 1010, 7658, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>[101, 5435, 2229, 1010, 1037, 1012, 1010, 1283...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[101, 9152, 4523, 2418, 25569, 1010, 2418, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>[101, 24559, 2683, 1516, 24232, 2581, 1010, 23...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>[101, 2104, 13102, 8586, 9031, 7534, 7860, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>[101, 26957, 1010, 1046, 1012, 1010, 15214, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>[101, 24471, 2140, 8299, 1024, 1013, 1013, 747...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>[101, 24389, 1010, 1046, 1012, 1010, 11132, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>[101, 28144, 8486, 13476, 1010, 1052, 1012, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>[101, 24398, 2549, 1516, 25191, 2487, 1010, 22...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>[101, 13849, 1010, 1038, 1012, 1010, 3288, 580...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>[101, 13849, 1010, 1038, 1012, 1010, 8945, 263...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>[101, 5671, 1010, 1039, 1012, 1054, 1012, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>[101, 2184, 1012, 9800, 2620, 1013, 1055, 2363...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>[101, 2002, 1010, 1047, 1012, 1010, 9327, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>[101, 2002, 1010, 1047, 1012, 1010, 9327, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>[101, 2002, 1010, 1047, 1012, 1010, 5470, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>[101, 2002, 20850, 2121, 1010, 1052, 1012, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>[101, 21863, 21190, 10603, 1010, 1040, 1012, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[101, 21863, 21190, 10603, 1010, 1040, 1012, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[101, 21863, 21190, 10603, 1010, 1040, 1012, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[101, 21863, 21190, 10603, 1010, 1040, 1012, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[101, 1996, 2116, 5344, 1997, 15873, 2791, 102...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[101, 21863, 21190, 10603, 1010, 1040, 1012, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>[101, 2940, 1010, 1042, 1012, 1010, 10437, 218...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>[101, 4922, 1010, 1046, 1012, 1998, 12726, 209...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>[101, 25833, 1516, 4293, 2629, 1012, 17481, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>[101, 6291, 1010, 1037, 1012, 1010, 24040, 253...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[101, 4720, 2620, 2487, 1013, 16729, 7716, 208...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>[101, 18816, 1010, 1040, 1012, 1010, 10514, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>[101, 1015, 1516, 1022, 1012, 15368, 1010, 228...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>[101, 11948, 2072, 1010, 1045, 1012, 1040, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>[101, 16770, 1024, 1013, 1013, 9193, 1012, 891...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>[101, 8040, 5369, 13094, 2386, 1010, 1049, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>[101, 2019, 9312, 1997, 5907, 2465, 18513, 205...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>[101, 8931, 1997, 1996, 9353, 2213, 2006, 2529...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>[101, 12411, 16118, 7033, 1010, 1054, 1012, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>[101, 27084, 5886, 1010, 1054, 1012, 1010, 239...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>[101, 1999, 8931, 1997, 1996, 2286, 3034, 2006...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>[101, 27084, 5886, 1010, 1054, 1012, 1010, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>[101, 2061, 7295, 1010, 1047, 1012, 5301, 2784...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>[101, 14017, 4886, 2386, 1010, 1045, 1012, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>[101, 16729, 7716, 2080, 1010, 10476, 1012, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>[101, 5185, 12044, 2696, 3567, 1010, 1050, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>[101, 13498, 27052, 2361, 1010, 1046, 1012, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>[101, 1055, 4371, 5999, 2100, 1010, 1039, 1012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>[101, 4083, 2892, 1011, 16913, 23732, 4372, 16...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>[101, 9092, 1010, 1049, 1012, 1998, 3393, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>[101, 1048, 1012, 9854, 15873, 2791, 2000, 301...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>[101, 4806, 1997, 1996, 9353, 2213, 1010, 5354...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>[101, 23401, 1010, 1061, 1012, 1010, 7418, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>[101, 6819, 13320, 10224, 1010, 1052, 1012, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>[101, 4261, 1516, 4583, 1012, 17481, 1010, 235...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>[101, 12098, 9048, 2615, 17463, 6657, 2102, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>[101, 2023, 2003, 1037, 4800, 5302, 9305, 2944...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>[101, 11071, 5688, 1006, 9587, 3597, 1007, 205...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[101, 3463, 1996, 3265, 7399, 15113, 7644, 202...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>[101, 2057, 3602, 2008, 1010, 2005, 1996, 5055...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>[101, 7399, 15113, 2836, 27347, 2005, 2169, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 4724, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 1016, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 1016, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 2277, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 29145, 1012, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2382, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 10165, 1012, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2322, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 2566, 29278, 438...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1016, 1013, 4700, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 19557, 7874, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 5835, 6178, 1012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 12104, 614...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 9118, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2260, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[101, 2304, 5413, 7228, 20364, 9001, 1014, 232...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1018, 1013, 3156, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 12686, 3899, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 3387, 1997, 2222...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 9402, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>[101, 1037, 6302, 1997, 2675, 5613, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 6352, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 6358, 11877, 725...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 5824, 2509...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>[101, 5871, 13425, 1997, 6840, 1012, 5871, 134...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 3429, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>[101, 1045, 2165, 1999, 2413, 23568, 1012, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1019, 1013, 19235, 614...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1016, 1013, 2531, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 11304, 2911, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 4086, 11565, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2531, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 3263, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 16216, 7389, 696...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 9402, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 2711, 5376, 8164...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 7886, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 8699, 2559, 2227...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1019, 1013, 1021, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1018, 5200, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 4937, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2184, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 12256, 3995, 923...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1019, 1013, 3263, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 4743, 1012, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 2603, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>[101, 1037, 6302, 1997, 1037, 14502, 7169, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 4464, 2581...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>[101, 1037, 6302, 1997, 8292, 12933, 2063, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>[101, 6149, 4635, 1024, 1015, 1013, 7886, 6149...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>[101, 5107, 3989, 1997, 20932, 2013, 4029, 125...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>[101, 2035, 4973, 2024, 6721, 2007, 1996, 6453...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>[101, 2043, 2062, 2084, 2028, 23561, 2003, 210...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>[101, 1996, 2598, 3606, 3830, 2003, 6910, 2665...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>[101, 1042, 2080, 1051, 2094, 2184, 1015, 1039...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>[101, 10047, 12943, 4372, 3802, 1039, 1048, 12...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>[101, 7886, 9402, 4293, 3770, 5594, 3938, 9353...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>[101, 2096, 2009, 2003, 29202, 2000, 2224, 103...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>[101, 2057, 2179, 1996, 2944, 1521, 1055, 3444...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>[101, 2116, 6270, 3893, 2015, 4158, 2349, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>[101, 2057, 2036, 5159, 1996, 2944, 2001, 3243...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>[101, 2057, 4384, 8385, 2008, 4871, 2007, 2152...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>[101, 2057, 2328, 2256, 2219, 2379, 1011, 2447...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>[101, 2057, 2580, 1037, 12553, 2951, 15476, 36...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>[101, 1996, 15476, 1011, 2273, 12516, 13117, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>[101, 1996, 13117, 2036, 18154, 27034, 2013, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>[101, 2057, 2059, 4738, 1037, 2944, 2000, 2584...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>[101, 2057, 6310, 1996, 2918, 24501, 7159, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>[101, 2408, 2256, 2878, 9345, 2140, 7621, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>[101, 9380, 2839, 5038, 2348, 5107, 22318, 203...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>[101, 2220, 4241, 2099, 1011, 13749, 1996, 245...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>[101, 2093, 1997, 2122, 2951, 13462, 2015, 240...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>[101, 12528, 1521, 1055, 2836, 2003, 2145, 381...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>[101, 1006, 2297, 1007, 2220, 2147, 11566, 278...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>[101, 7020, 2102, 1011, 1016, 2003, 1037, 6251...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>[101, 2057, 2421, 7020, 2102, 1011, 1016, 1999...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>[101, 2096, 2023, 2003, 1037, 3722, 17953, 236...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>[101, 2714, 2000, 7020, 2102, 1011, 1016, 1010...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>[101, 2633, 1010, 2057, 3602, 2008, 5717, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>[101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>[101, 1054, 1030, 1015, 1054, 1030, 1019, 1054...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[101, 20248, 4135, 9289, 3989, 2178, 5248, 205...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [101, 2023, 7775, 2433, 1997, 3565, 1011, 4432...   \n",
       "1    [101, 1006, 2289, 1007, 7645, 2009, 2001, 1343...   \n",
       "2    [101, 2096, 24404, 8405, 3802, 2632, 1012, 100...   \n",
       "3    [101, 2057, 17902, 2008, 2054, 2003, 2691, 240...   \n",
       "4    [101, 2035, 2122, 8107, 2024, 4083, 2013, 3019...   \n",
       "5    [101, 4083, 2013, 3019, 2653, 2038, 2195, 4022...   \n",
       "6    [101, 2009, 1521, 1055, 2172, 6082, 2000, 4094...   \n",
       "7    [101, 2612, 1010, 4725, 2029, 2147, 2006, 3019...   \n",
       "8    [101, 4083, 2013, 3019, 2653, 2036, 2038, 2019...   \n",
       "9    [101, 1999, 1996, 2206, 4942, 29015, 2015, 101...   \n",
       "10   [101, 4526, 1037, 12949, 2312, 2951, 13462, 44...   \n",
       "11   [101, 2096, 5796, 1011, 25033, 1998, 5107, 134...   \n",
       "12   [101, 2011, 7831, 1010, 2060, 3274, 4432, 3001...   \n",
       "13   [101, 1061, 11329, 2278, 18613, 2213, 1010, 20...   \n",
       "14   [101, 2116, 4871, 2224, 8285, 18900, 2072, 101...   \n",
       "15   [101, 1037, 2350, 14354, 2005, 3019, 2653, 104...   \n",
       "16   [101, 2144, 4493, 2951, 13462, 2015, 2079, 202...   \n",
       "17   [101, 2000, 4769, 2023, 1010, 2057, 3833, 1037...   \n",
       "18   [101, 1996, 4525, 2951, 13462, 2038, 1037, 271...   \n",
       "19   [101, 2119, 2122, 8107, 3745, 1037, 3145, 1440...   \n",
       "20   [101, 1999, 3275, 1017, 2057, 2421, 18404, 160...   \n",
       "21   [101, 2057, 2036, 5672, 1996, 3795, 2779, 4770...   \n",
       "22   [101, 1996, 3793, 4372, 16044, 2099, 2003, 103...   \n",
       "23   [101, 2004, 1037, 2918, 2946, 2057, 2224, 1037...   \n",
       "24   [101, 2096, 3025, 3274, 4432, 2470, 2038, 2411...   \n",
       "25   [101, 2731, 2057, 3345, 1037, 2186, 1997, 1019...   \n",
       "26   [101, 2027, 2024, 19537, 2004, 29300, 12376, 2...   \n",
       "27   [101, 2005, 1996, 4432, 19081, 2057, 3345, 103...   \n",
       "28   [101, 2057, 2224, 1996, 4205, 23569, 27605, 62...   \n",
       "29   [101, 23760, 1011, 11709, 2020, 2059, 5967, 20...   \n",
       "30   [101, 1996, 4553, 3085, 4860, 16381, 1174, 200...   \n",
       "31   [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "32   [101, 2000, 2256, 3716, 1010, 5107, 1050, 1011...   \n",
       "33   [101, 1996, 2190, 12528, 2944, 24840, 10640, 2...   \n",
       "34   [101, 5678, 1010, 1996, 2327, 1011, 1019, 1064...   \n",
       "35   [101, 1996, 3754, 2000, 2674, 1996, 2836, 1997...   \n",
       "36   [101, 3019, 2653, 10429, 1021, 1037, 17560, 95...   \n",
       "37   [101, 13599, 12528, 2000, 3188, 5717, 1011, 29...   \n",
       "38   [101, 12528, 24840, 2836, 2006, 2035, 2093, 29...   \n",
       "39   [101, 2023, 7620, 11138, 2116, 5966, 1999, 199...   \n",
       "40   [101, 12528, 2003, 1037, 3278, 3357, 2875, 123...   \n",
       "41   [101, 1996, 6565, 3484, 1997, 2951, 13462, 201...   \n",
       "42   [101, 2005, 2116, 2951, 13462, 2015, 1010, 205...   \n",
       "43   [101, 25732, 3330, 1998, 4372, 3366, 29256, 53...   \n",
       "44   [101, 2057, 2179, 2006, 2195, 2986, 1011, 8982...   \n",
       "45   [101, 4106, 1997, 5717, 1011, 2915, 12528, 283...   \n",
       "46   [101, 2004, 1037, 2034, 3160, 1010, 2057, 2298...   \n",
       "47   [101, 2000, 9530, 1011, 25304, 4697, 2023, 101...   \n",
       "48   [101, 5717, 1011, 2915, 12528, 2041, 4842, 226...   \n",
       "49   [101, 5717, 1011, 2915, 12528, 2003, 6975, 200...   \n",
       "50   [101, 2559, 2012, 3265, 2951, 13462, 2015, 765...   \n",
       "51   [101, 2006, 2048, 1997, 2122, 2951, 13462, 201...   \n",
       "52   [101, 2057, 8343, 2122, 4489, 2024, 3952, 2349...   \n",
       "53   [101, 2006, 1523, 2236, 1524, 4874, 2465, 1851...   \n",
       "54   [101, 2057, 28699, 9869, 2023, 2003, 2349, 200...   \n",
       "55   [101, 2559, 2012, 2073, 5717, 1011, 2915, 1252...   \n",
       "56   [101, 5717, 1011, 2915, 12528, 2041, 4842, 226...   \n",
       "57   [101, 1996, 2322, 2951, 13462, 2015, 2007, 201...   \n",
       "58   [101, 2057, 2156, 2008, 5717, 1011, 2915, 1252...   \n",
       "59   [101, 2122, 3463, 12944, 1996, 3532, 10673, 19...   \n",
       "60   [101, 2011, 5688, 1010, 2512, 1011, 6739, 4286...   \n",
       "61   [101, 2034, 1010, 12528, 1521, 1055, 5717, 101...   \n",
       "62   [101, 2011, 5688, 1010, 1523, 3671, 1524, 1358...   \n",
       "63   [101, 6123, 1011, 2625, 2742, 1011, 2241, 4553...   \n",
       "64   [101, 2348, 1037, 5214, 4553, 2121, 2003, 2583...   \n",
       "65   [101, 2057, 2424, 2008, 5717, 1011, 2915, 4651...   \n",
       "66   [101, 1996, 2951, 8122, 1997, 5717, 1011, 2915...   \n",
       "67   [101, 5300, 2024, 4358, 2241, 2006, 8833, 1011...   \n",
       "68   [101, 2836, 9783, 4235, 2013, 2145, 2104, 4842...   \n",
       "69   [101, 2048, 2951, 13462, 2015, 1010, 4870, 107...   \n",
       "70   [101, 2431, 1997, 1996, 2951, 13462, 2015, 547...   \n",
       "71   [101, 2174, 1010, 1996, 2812, 4358, 2951, 8122...   \n",
       "72   [101, 2023, 2003, 2349, 2000, 1996, 2322, 1003...   \n",
       "73   [101, 2006, 3746, 7159, 1010, 5717, 1011, 2915...   \n",
       "74   [101, 1996, 18198, 1010, 1061, 1027, 1060, 224...   \n",
       "75   [101, 5717, 1011, 2915, 2836, 2003, 23900, 200...   \n",
       "76   [101, 2158, 3401, 1010, 9104, 2008, 12528, 200...   \n",
       "77   [101, 2174, 1010, 5717, 1011, 2915, 12528, 206...   \n",
       "78   [101, 2023, 6083, 2008, 12528, 2089, 2022, 206...   \n",
       "79   [101, 2058, 1996, 2627, 2261, 2086, 1010, 1753...   \n",
       "80   [101, 1996, 14246, 2102, 2155, 1997, 4275, 203...   \n",
       "81   [101, 2096, 1996, 3452, 9874, 2003, 5744, 1010...   \n",
       "82   [101, 6630, 4083, 2096, 2057, 2031, 8077, 1657...   \n",
       "83   [101, 2000, 18478, 4989, 3896, 2008, 2071, 533...   \n",
       "84   [101, 2122, 2235, 12528, 4275, 2036, 2104, 484...   \n",
       "85   [101, 2057, 2036, 2424, 2008, 12528, 4432, 190...   \n",
       "86   [101, 2122, 3463, 24209, 11475, 27453, 2135, 2...   \n",
       "87   [101, 1006, 12609, 1007, 2029, 2988, 2008, 443...   \n",
       "88   [101, 2256, 2190, 3452, 2944, 2003, 1037, 6819...   \n",
       "89   [101, 2004, 3275, 2538, 24209, 11475, 27453, 2...   \n",
       "90   [101, 2122, 8518, 2421, 20248, 1011, 2334, 398...   \n",
       "91   [101, 3904, 1997, 2122, 8518, 2024, 7594, 1999...   \n",
       "92   [101, 2000, 4769, 2023, 1010, 2057, 2036, 5468...   \n",
       "93   [101, 2023, 9312, 7621, 1010, 6851, 1999, 2252...   \n",
       "94   [101, 7399, 15113, 2836, 1997, 12528, 4275, 19...   \n",
       "95   [101, 1006, 2157, 1007, 7644, 2024, 11398, 205...   \n",
       "96   [101, 2005, 6013, 1010, 2096, 21934, 20464, 20...   \n",
       "97   [101, 1037, 2691, 4323, 1997, 3818, 17959, 200...   \n",
       "98   [101, 2057, 14046, 2008, 1010, 2000, 3058, 101...   \n",
       "99   [101, 21195, 1996, 8476, 1997, 6594, 1010, 200...   \n",
       "100  [101, 2000, 2054, 3014, 2024, 2122, 15428, 201...   \n",
       "101  [101, 20216, 3089, 3802, 2632, 1012, 1006, 126...   \n",
       "102  [101, 2027, 5468, 2836, 2006, 1037, 2275, 1997...   \n",
       "103  [101, 2027, 16599, 2023, 7835, 2138, 1999, 211...   \n",
       "104  [101, 1037, 24501, 7159, 1011, 7886, 3084, 101...   \n",
       "105  [101, 11434, 2135, 2174, 1010, 20216, 3089, 38...   \n",
       "106  [101, 4621, 15873, 2791, 5761, 8377, 1999, 106...   \n",
       "107  [101, 20216, 3089, 3802, 2632, 1012, 1006, 126...   \n",
       "108  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "109  [101, 2947, 2009, 2003, 9608, 2000, 5987, 5717...   \n",
       "110  [101, 5717, 1011, 2915, 12528, 2003, 2172, 206...   \n",
       "111  [101, 2005, 1996, 2711, 2465, 1999, 7858, 1011...   \n",
       "112  [101, 12528, 4275, 5967, 2000, 3746, 7159, 203...   \n",
       "113  [101, 7831, 2000, 2529, 2836, 2129, 2515, 1252...   \n",
       "114  [101, 2000, 2131, 1037, 2488, 4824, 1997, 2129...   \n",
       "115  [101, 2057, 2359, 2000, 2131, 1037, 3168, 1997...   \n",
       "116  [101, 2023, 2064, 2393, 2149, 2000, 12826, 470...   \n",
       "117  [101, 2057, 2018, 2274, 2367, 4286, 2298, 2012...   \n",
       "118  [101, 1999, 1996, 5717, 1011, 2915, 2553, 1996...   \n",
       "119  [101, 2261, 1011, 2915, 12528, 2036, 7457, 462...   \n",
       "120  [101, 7163, 4328, 6774, 1996, 3815, 1997, 3746...   \n",
       "121  [101, 1996, 5114, 1999, 10640, 2183, 2013, 571...   \n",
       "122  [101, 2445, 2023, 1010, 2009, 3849, 2008, 2096...   \n",
       "123  [101, 2138, 2122, 2261, 1011, 2915, 9312, 2015...   \n",
       "124  [101, 2000, 2256, 3716, 1010, 2478, 1037, 7399...   \n",
       "125  [101, 2004, 1999, 2380, 4048, 3802, 2632, 1012...   \n",
       "126  [101, 2087, 1997, 1996, 5114, 1999, 2836, 2043...   \n",
       "127  [101, 1523, 3984, 2229, 1524, 5218, 2000, 2699...   \n",
       "128  [101, 3737, 3653, 1011, 4738, 2944, 2003, 2379...   \n",
       "129  [101, 2065, 2057, 5436, 2529, 10640, 5443, 125...   \n",
       "130  [101, 2000, 1996, 6698, 2008, 10697, 2024, 833...   \n",
       "131  [101, 1019, 1012, 2951, 17702, 4106, 1037, 514...   \n",
       "132  [101, 2028, 5724, 2000, 4652, 2023, 2003, 2000...   \n",
       "133  [101, 2096, 2023, 21586, 7316, 2995, 2907, 101...   \n",
       "134  [101, 2023, 2038, 1996, 12482, 5178, 1997, 148...   \n",
       "135  [101, 5815, 1037, 2047, 9312, 2052, 5478, 2019...   \n",
       "136  [101, 1037, 12654, 1997, 2023, 4106, 2003, 359...   \n",
       "137  [101, 2261, 7778, 2135, 3278, 8377, 1999, 1064...   \n",
       "138  [101, 1006, 2187, 1007, 2096, 2195, 2951, 1346...   \n",
       "139  [101, 1006, 2157, 1007, 2144, 1996, 7017, 1997...   \n",
       "140  [101, 2566, 11365, 2098, 26163, 1997, 1037, 73...   \n",
       "141  [101, 3278, 2147, 2003, 2145, 2734, 2000, 5335...   \n",
       "142  [101, 2096, 25169, 2038, 2061, 2521, 11328, 53...   \n",
       "143  [101, 2023, 2003, 1999, 7959, 21369, 3468, 200...   \n",
       "144  [101, 4106, 1999, 2930, 1017, 1012, 1015, 2179...   \n",
       "145  [101, 2057, 2024, 9657, 2008, 2045, 2024, 2145...   \n",
       "146  [101, 2174, 1010, 12528, 2069, 6162, 2015, 607...   \n",
       "147  [101, 2019, 16436, 2135, 3722, 26163, 1997, 88...   \n",
       "148  [101, 2023, 6083, 12528, 2515, 2210, 2000, 476...   \n",
       "149  [101, 2023, 2003, 1037, 15743, 11213, 2008, 10...   \n",
       "150  [101, 2348, 12528, 2064, 23951, 17296, 9699, 5...   \n",
       "151  [101, 6854, 1010, 2004, 2649, 1999, 10819, 101...   \n",
       "152  [101, 1037, 3722, 2801, 4276, 2667, 2003, 4101...   \n",
       "153  [101, 2612, 12528, 19079, 2015, 2011, 2478, 10...   \n",
       "154  [101, 11566, 12528, 2007, 2969, 1011, 10429, 1...   \n",
       "155  [101, 2023, 10673, 13999, 7860, 2714, 2000, 22...   \n",
       "156  [101, 2005, 2742, 1010, 2009, 2064, 2424, 7882...   \n",
       "157  [101, 2116, 1997, 12528, 1521, 1055, 9859, 202...   \n",
       "158  [101, 2445, 2049, 2591, 13494, 1010, 2057, 476...   \n",
       "159  [101, 2057, 2031, 2036, 4912, 2000, 2839, 4697...   \n",
       "160  [101, 2256, 13827, 5852, 5050, 2256, 3988, 407...   \n",
       "161  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "162  [101, 3867, 10640, 2006, 5907, 5579, 1997, 487...   \n",
       "163  [101, 3867, 1997, 4871, 6219, 2046, 4126, 1011...   \n",
       "164  [101, 1996, 3125, 1997, 2023, 7551, 2001, 2000...   \n",
       "165  [101, 2057, 2179, 2008, 2023, 21040, 4359, 199...   \n",
       "166  [101, 2057, 2036, 3344, 2041, 7885, 2714, 2000...   \n",
       "167  [101, 2057, 3344, 2041, 2093, 7885, 1011, 2057...   \n",
       "168  [101, 2057, 2034, 3432, 2246, 2046, 5907, 1754...   \n",
       "169  [101, 2023, 2003, 3621, 2488, 2836, 2084, 1996...   \n",
       "170  [101, 2057, 1044, 22571, 14573, 2229, 4697, 20...   \n",
       "171  [101, 1999, 2344, 2000, 2817, 2129, 1996, 1382...   \n",
       "172  [101, 2057, 2179, 2008, 1996, 2896, 11207, 241...   \n",
       "173  [101, 2174, 1010, 2130, 1996, 16965, 20611, 19...   \n",
       "174  [101, 2005, 2742, 1010, 2057, 2424, 2008, 2104...   \n",
       "175  [101, 2023, 2685, 2000, 5907, 2098, 8924, 2714...   \n",
       "176  [101, 2012, 1996, 3020, 1018, 1003, 11207, 101...   \n",
       "177  [101, 9867, 2057, 2279, 4912, 2000, 2839, 4697...   \n",
       "178  [101, 12528, 2836, 2006, 2266, 1997, 3519, 487...   \n",
       "179  [101, 1996, 2322, 2087, 5907, 2098, 10873, 200...   \n",
       "180  [101, 6963, 19090, 1996, 7017, 1997, 4871, 200...   \n",
       "181  [101, 2256, 10502, 1997, 9867, 2003, 2025, 383...   \n",
       "182  [101, 2057, 5468, 1996, 2944, 1521, 1055, 2836...   \n",
       "183  [101, 2057, 2109, 1996, 6819, 8609, 2951, 1346...   \n",
       "184  [101, 2445, 12528, 1521, 1055, 12379, 2465, 28...   \n",
       "185  [101, 20392, 5579, 3223, 1996, 2944, 2000, 111...   \n",
       "186  [101, 5678, 1010, 2057, 3344, 2041, 1037, 1520...   \n",
       "187  [101, 2057, 2179, 2008, 1996, 2944, 2018, 1037...   \n",
       "188  [101, 3602, 2008, 2023, 7551, 2001, 9416, 2069...   \n",
       "189  [101, 2057, 2036, 7718, 12528, 1521, 1055, 571...   \n",
       "190  [101, 2023, 1022, 22074, 1024, 1996, 8292, 257...   \n",
       "191  [101, 2349, 2000, 1996, 3267, 1997, 1996, 2951...   \n",
       "192  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "193  [101, 3141, 2147, 2151, 2944, 2008, 21155, 201...   \n",
       "194  [101, 2023, 2003, 2019, 4914, 2135, 5186, 5041...   \n",
       "195  [101, 2009, 2036, 2950, 2172, 1997, 1996, 1236...   \n",
       "196  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "197  [101, 13764, 8649, 2241, 4083, 1006, 12755, 10...   \n",
       "198  [101, 12528, 2003, 2019, 2742, 1997, 2478, 301...   \n",
       "199  [101, 1999, 2023, 6123, 1010, 1996, 5700, 2224...   \n",
       "200  [101, 12528, 1521, 1055, 3653, 1011, 2731, 470...   \n",
       "201  [101, 2023, 2752, 1997, 2470, 5246, 2067, 2000...   \n",
       "202  [101, 2096, 3988, 4073, 4208, 3952, 2006, 3653...   \n",
       "203  [101, 2023, 2240, 1997, 2147, 10861, 5134, 374...   \n",
       "204  [101, 7091, 2057, 2031, 10847, 3251, 2009, 200...   \n",
       "205  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "206  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "207  [101, 20934, 6030, 2213, 10105, 2072, 1010, 10...   \n",
       "208  [101, 12385, 21302, 1010, 1046, 1012, 1010, 13...   \n",
       "209  [101, 8802, 1010, 1049, 1012, 1010, 10958, 209...   \n",
       "210  [101, 8802, 1010, 1056, 1012, 1010, 15990, 101...   \n",
       "211  [101, 8802, 1010, 1056, 1012, 1010, 12849, 682...   \n",
       "212  [101, 8802, 1010, 1056, 1012, 1010, 12849, 682...   \n",
       "213  [101, 8802, 1010, 1060, 1012, 1998, 20512, 101...   \n",
       "214  [101, 8802, 1010, 1061, 1012, 1011, 1039, 1012...   \n",
       "215  [101, 15898, 1010, 1043, 1012, 1010, 7658, 101...   \n",
       "216  [101, 5435, 2229, 1010, 1037, 1012, 1010, 1283...   \n",
       "217  [101, 9152, 4523, 2418, 25569, 1010, 2418, 101...   \n",
       "218  [101, 24559, 2683, 1516, 24232, 2581, 1010, 23...   \n",
       "219  [101, 2104, 13102, 8586, 9031, 7534, 7860, 200...   \n",
       "220  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "221  [101, 26957, 1010, 1046, 1012, 1010, 15214, 10...   \n",
       "222  [101, 24471, 2140, 8299, 1024, 1013, 1013, 747...   \n",
       "223  [101, 24389, 1010, 1046, 1012, 1010, 11132, 10...   \n",
       "224  [101, 28144, 8486, 13476, 1010, 1052, 1012, 10...   \n",
       "225  [101, 24398, 2549, 1516, 25191, 2487, 1010, 22...   \n",
       "226  [101, 13849, 1010, 1038, 1012, 1010, 3288, 580...   \n",
       "227  [101, 13849, 1010, 1038, 1012, 1010, 8945, 263...   \n",
       "228  [101, 5671, 1010, 1039, 1012, 1054, 1012, 1010...   \n",
       "229  [101, 2184, 1012, 9800, 2620, 1013, 1055, 2363...   \n",
       "230  [101, 2002, 1010, 1047, 1012, 1010, 9327, 1010...   \n",
       "231  [101, 2002, 1010, 1047, 1012, 1010, 9327, 1010...   \n",
       "232  [101, 2002, 1010, 1047, 1012, 1010, 5470, 1010...   \n",
       "233  [101, 2002, 20850, 2121, 1010, 1052, 1012, 101...   \n",
       "234  [101, 21863, 21190, 10603, 1010, 1040, 1012, 1...   \n",
       "235  [101, 21863, 21190, 10603, 1010, 1040, 1012, 1...   \n",
       "236  [101, 21863, 21190, 10603, 1010, 1040, 1012, 1...   \n",
       "237  [101, 21863, 21190, 10603, 1010, 1040, 1012, 1...   \n",
       "238  [101, 1996, 2116, 5344, 1997, 15873, 2791, 102...   \n",
       "239  [101, 21863, 21190, 10603, 1010, 1040, 1012, 1...   \n",
       "240  [101, 2940, 1010, 1042, 1012, 1010, 10437, 218...   \n",
       "241  [101, 4922, 1010, 1046, 1012, 1998, 12726, 209...   \n",
       "242  [101, 25833, 1516, 4293, 2629, 1012, 17481, 10...   \n",
       "243  [101, 6291, 1010, 1037, 1012, 1010, 24040, 253...   \n",
       "244  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "245  [101, 4720, 2620, 2487, 1013, 16729, 7716, 208...   \n",
       "246  [101, 18816, 1010, 1040, 1012, 1010, 10514, 10...   \n",
       "247  [101, 1015, 1516, 1022, 1012, 15368, 1010, 228...   \n",
       "248  [101, 11948, 2072, 1010, 1045, 1012, 1040, 101...   \n",
       "249  [101, 16770, 1024, 1013, 1013, 9193, 1012, 891...   \n",
       "250  [101, 8040, 5369, 13094, 2386, 1010, 1049, 101...   \n",
       "251  [101, 2019, 9312, 1997, 5907, 2465, 18513, 205...   \n",
       "252  [101, 8931, 1997, 1996, 9353, 2213, 2006, 2529...   \n",
       "253  [101, 12411, 16118, 7033, 1010, 1054, 1012, 10...   \n",
       "254  [101, 27084, 5886, 1010, 1054, 1012, 1010, 239...   \n",
       "255  [101, 1999, 8931, 1997, 1996, 2286, 3034, 2006...   \n",
       "256  [101, 27084, 5886, 1010, 1054, 1012, 1010, 105...   \n",
       "257  [101, 2061, 7295, 1010, 1047, 1012, 5301, 2784...   \n",
       "258  [101, 14017, 4886, 2386, 1010, 1045, 1012, 101...   \n",
       "259  [101, 16729, 7716, 2080, 1010, 10476, 1012, 24...   \n",
       "260  [101, 5185, 12044, 2696, 3567, 1010, 1050, 101...   \n",
       "261  [101, 13498, 27052, 2361, 1010, 1046, 1012, 10...   \n",
       "262  [101, 1055, 4371, 5999, 2100, 1010, 1039, 1012...   \n",
       "263  [101, 4083, 2892, 1011, 16913, 23732, 4372, 16...   \n",
       "264  [101, 9092, 1010, 1049, 1012, 1998, 3393, 1010...   \n",
       "265  [101, 1048, 1012, 9854, 15873, 2791, 2000, 301...   \n",
       "266  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "267  [101, 4806, 1997, 1996, 9353, 2213, 1010, 5354...   \n",
       "268  [101, 23401, 1010, 1061, 1012, 1010, 7418, 101...   \n",
       "269  [101, 6819, 13320, 10224, 1010, 1052, 1012, 10...   \n",
       "270  [101, 4261, 1516, 4583, 1012, 17481, 1010, 235...   \n",
       "271  [101, 12098, 9048, 2615, 17463, 6657, 2102, 12...   \n",
       "272  [101, 2023, 2003, 1037, 4800, 5302, 9305, 2944...   \n",
       "273  [101, 11071, 5688, 1006, 9587, 3597, 1007, 205...   \n",
       "274  [101, 3463, 1996, 3265, 7399, 15113, 7644, 202...   \n",
       "275  [101, 2057, 3602, 2008, 1010, 2005, 1996, 5055...   \n",
       "276  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "277  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "278  [101, 7399, 15113, 2836, 27347, 2005, 2169, 19...   \n",
       "279  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "280  [101, 6149, 4635, 1024, 1015, 1013, 4724, 6149...   \n",
       "281  [101, 6149, 4635, 1024, 1015, 1013, 1016, 6149...   \n",
       "282  [101, 6149, 4635, 1024, 1015, 1013, 1016, 6149...   \n",
       "283  [101, 1037, 6302, 1997, 1037, 2277, 1012, 1037...   \n",
       "284  [101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...   \n",
       "285  [101, 1037, 6302, 1997, 1037, 29145, 1012, 103...   \n",
       "286  [101, 6149, 4635, 1024, 1015, 1013, 2382, 6149...   \n",
       "287  [101, 1037, 6302, 1997, 1037, 10165, 1012, 103...   \n",
       "288  [101, 6149, 4635, 1024, 1015, 1013, 2322, 6149...   \n",
       "289  [101, 1037, 6302, 1997, 1037, 2566, 29278, 438...   \n",
       "290  [101, 6149, 4635, 1024, 1016, 1013, 4700, 6149...   \n",
       "291  [101, 1037, 6302, 1997, 1037, 19557, 7874, 101...   \n",
       "292  [101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...   \n",
       "293  [101, 1037, 6302, 1997, 1037, 5835, 6178, 1012...   \n",
       "294  [101, 6149, 4635, 1024, 1015, 1013, 12104, 614...   \n",
       "295  [101, 1037, 6302, 1997, 1037, 9118, 1012, 1037...   \n",
       "296  [101, 6149, 4635, 1024, 1015, 1013, 2260, 6149...   \n",
       "297  [101, 2304, 5413, 7228, 20364, 9001, 1014, 232...   \n",
       "298  [101, 6149, 4635, 1024, 1018, 1013, 3156, 6149...   \n",
       "299  [101, 1037, 6302, 1997, 1037, 12686, 3899, 101...   \n",
       "300  [101, 1037, 6302, 1997, 1037, 3387, 1997, 2222...   \n",
       "301  [101, 6149, 4635, 1024, 1015, 1013, 9402, 6149...   \n",
       "302  [101, 1037, 6302, 1997, 2675, 5613, 1012, 1037...   \n",
       "303  [101, 6149, 4635, 1024, 1015, 1013, 6352, 6149...   \n",
       "304  [101, 1037, 6302, 1997, 1037, 6358, 11877, 725...   \n",
       "305  [101, 6149, 4635, 1024, 1015, 1013, 5824, 2509...   \n",
       "306  [101, 5871, 13425, 1997, 6840, 1012, 5871, 134...   \n",
       "307  [101, 6149, 4635, 1024, 1015, 1013, 3429, 6149...   \n",
       "308  [101, 1045, 2165, 1999, 2413, 23568, 1012, 103...   \n",
       "309  [101, 6149, 4635, 1024, 1019, 1013, 19235, 614...   \n",
       "310  [101, 6149, 4635, 1024, 1016, 1013, 2531, 6149...   \n",
       "311  [101, 1037, 6302, 1997, 1037, 11304, 2911, 101...   \n",
       "312  [101, 6149, 4635, 1024, 1015, 1013, 6694, 6149...   \n",
       "313  [101, 1037, 6302, 1997, 1037, 4086, 11565, 101...   \n",
       "314  [101, 6149, 4635, 1024, 1015, 1013, 2531, 6149...   \n",
       "315  [101, 6149, 4635, 1024, 1015, 1013, 3263, 6149...   \n",
       "316  [101, 1037, 6302, 1997, 1037, 16216, 7389, 696...   \n",
       "317  [101, 6149, 4635, 1024, 1015, 1013, 9402, 6149...   \n",
       "318  [101, 1037, 6302, 1997, 1037, 2711, 5376, 8164...   \n",
       "319  [101, 6149, 4635, 1024, 1015, 1013, 7886, 6149...   \n",
       "320  [101, 1037, 6302, 1997, 1037, 8699, 2559, 2227...   \n",
       "321  [101, 6149, 4635, 1024, 1019, 1013, 1021, 6149...   \n",
       "322  [101, 1037, 6302, 1997, 1018, 5200, 1012, 1037...   \n",
       "323  [101, 1037, 6302, 1997, 1037, 4937, 1012, 1037...   \n",
       "324  [101, 6149, 4635, 1024, 1015, 1013, 2184, 6149...   \n",
       "325  [101, 1037, 6302, 1997, 1037, 12256, 3995, 923...   \n",
       "326  [101, 6149, 4635, 1024, 1019, 1013, 3263, 6149...   \n",
       "327  [101, 1037, 6302, 1997, 1037, 4743, 1012, 1037...   \n",
       "328  [101, 6149, 4635, 1024, 1015, 1013, 2603, 6149...   \n",
       "329  [101, 1037, 6302, 1997, 1037, 14502, 7169, 101...   \n",
       "330  [101, 6149, 4635, 1024, 1015, 1013, 4464, 2581...   \n",
       "331  [101, 1037, 6302, 1997, 8292, 12933, 2063, 101...   \n",
       "332  [101, 6149, 4635, 1024, 1015, 1013, 7886, 6149...   \n",
       "333  [101, 5107, 3989, 1997, 20932, 2013, 4029, 125...   \n",
       "334  [101, 2035, 4973, 2024, 6721, 2007, 1996, 6453...   \n",
       "335  [101, 2043, 2062, 2084, 2028, 23561, 2003, 210...   \n",
       "336  [101, 1996, 2598, 3606, 3830, 2003, 6910, 2665...   \n",
       "337  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "338  [101, 1042, 2080, 1051, 2094, 2184, 1015, 1039...   \n",
       "339  [101, 10047, 12943, 4372, 3802, 1039, 1048, 12...   \n",
       "340  [101, 7886, 9402, 4293, 3770, 5594, 3938, 9353...   \n",
       "341  [101, 2096, 2009, 2003, 29202, 2000, 2224, 103...   \n",
       "342  [101, 2057, 2179, 1996, 2944, 1521, 1055, 3444...   \n",
       "343  [101, 2116, 6270, 3893, 2015, 4158, 2349, 2000...   \n",
       "344  [101, 2057, 2036, 5159, 1996, 2944, 2001, 3243...   \n",
       "345  [101, 2057, 4384, 8385, 2008, 4871, 2007, 2152...   \n",
       "346  [101, 2057, 2328, 2256, 2219, 2379, 1011, 2447...   \n",
       "347  [101, 2057, 2580, 1037, 12553, 2951, 15476, 36...   \n",
       "348  [101, 1996, 15476, 1011, 2273, 12516, 13117, 1...   \n",
       "349  [101, 1996, 13117, 2036, 18154, 27034, 2013, 1...   \n",
       "350  [101, 2057, 2059, 4738, 1037, 2944, 2000, 2584...   \n",
       "351  [101, 2057, 6310, 1996, 2918, 24501, 7159, 101...   \n",
       "352  [101, 2408, 2256, 2878, 9345, 2140, 7621, 1010...   \n",
       "353  [101, 9380, 2839, 5038, 2348, 5107, 22318, 203...   \n",
       "354  [101, 2220, 4241, 2099, 1011, 13749, 1996, 245...   \n",
       "355  [101, 2093, 1997, 2122, 2951, 13462, 2015, 240...   \n",
       "356  [101, 12528, 1521, 1055, 2836, 2003, 2145, 381...   \n",
       "357  [101, 1006, 2297, 1007, 2220, 2147, 11566, 278...   \n",
       "358  [101, 7020, 2102, 1011, 1016, 2003, 1037, 6251...   \n",
       "359  [101, 2057, 2421, 7020, 2102, 1011, 1016, 1999...   \n",
       "360  [101, 2096, 2023, 2003, 1037, 3722, 17953, 236...   \n",
       "361  [101, 2714, 2000, 7020, 2102, 1011, 1016, 1010...   \n",
       "362  [101, 2633, 1010, 2057, 3602, 2008, 5717, 1011...   \n",
       "363  [101, 4083, 4651, 3085, 5107, 4275, 2013, 3019...   \n",
       "364  [101, 1054, 1030, 1015, 1054, 1030, 1019, 1054...   \n",
       "365  [101, 20248, 4135, 9289, 3989, 2178, 5248, 205...   \n",
       "\n",
       "                                        token_type_ids  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "31   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "33   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "35   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "36   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "39   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "42   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "45   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "47   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "48   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "52   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "54   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "55   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "56   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "57   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "59   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "61   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "63   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "65   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "66   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "67   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "68   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "69   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "70   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "72   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "73   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "74   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "75   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "76   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "77   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "78   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "79   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "80   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "81   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "82   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "83   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "84   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "85   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "86   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "87   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "88   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "89   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "90   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "91   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "92   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "93   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "94   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "95   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "96   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "97   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "98   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "99   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "100  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "101  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "102  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "103  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "104  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "105  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "108  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "109  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "110  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "111  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "112  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "113  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "114  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "115  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "116  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "117  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "118  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "119  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "120  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "121  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "122  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "123  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "124  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "127  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "129  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "130  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "132  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "134  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "135  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "137      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "138  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "139  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "140  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "141  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "142  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "143  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "144  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "145  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "146  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "147  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "148  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "149  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "150  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "151  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "152  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "153  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "154  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "155  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "156  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "158  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "159  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "160  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "161  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "162  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "163  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "164  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "165  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "166  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "167  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "168  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "169  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "170  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "171  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "172      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "173  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "174  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "175  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "176  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "177  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "178  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "179  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "180  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "181  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "182  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "183  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "184  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "185  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "186  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "187  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "188  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "189  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "190  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "191  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "192  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "193  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "194  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "195  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "196  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "197  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "198  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "199  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "200  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "201  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "202  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "203  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "204  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "205  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "206  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "207  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "208  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "209  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "210  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "211  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "212  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "213  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "214  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "215  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "216  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "217  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "218  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "219      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "220  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "221  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "222  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "223  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "224  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "225  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "226  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "227  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "228  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "229  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "230  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "231  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "232  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "233  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "234  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "235  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "236  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "237  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "238  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "239  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "240  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "241  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "242  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "243  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "244  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "245  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "246  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "247  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "248  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "249  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "251  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "252  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "253  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "254  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "255  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "256  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "257  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "258  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "259  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "260  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "261  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "262  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "263         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "264  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "265  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "266  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "267  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "268  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "269  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "270  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "271  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "272  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "273  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "274  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "275  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "276  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "277  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "278  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "279  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "280  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "281  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "282  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "283  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "284  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "285  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "286  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "287  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "288  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "289  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "290  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "291  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "292  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "293  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "294  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "295  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "296  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "300  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "301  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "302  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "303  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "304  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "305  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "306  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "307  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "308  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "309  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "310  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "311  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "312  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "314  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "315  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "316  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "317  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "318  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "319  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "320  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "321  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "322  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "323  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "325  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "326  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "327  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "328  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "329  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "330  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "331  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "332  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "333      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "334  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "335   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "336  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "337               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "338  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "339  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "340  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "341  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "342  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "344  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "345  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "346      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "347  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "348  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "349  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "350  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "351  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "352  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "353  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "355  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "356  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "357  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "358  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "359  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "360  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "361  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "362  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "363  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "364  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "365  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        attention_mask  Length  \n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     405  \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     509  \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     394  \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      69  \n",
       "5        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "6    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      57  \n",
       "7    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "8    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      51  \n",
       "9    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "10   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      57  \n",
       "11   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      34  \n",
       "12   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      31  \n",
       "13   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      31  \n",
       "14   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      74  \n",
       "15   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "16   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "17   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     136  \n",
       "18   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "19   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     398  \n",
       "20   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     528  \n",
       "21   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     493  \n",
       "22   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "23   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     200  \n",
       "24   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     199  \n",
       "25   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      67  \n",
       "26   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "27   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "28   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "29   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      25  \n",
       "30   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     156  \n",
       "31   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     357  \n",
       "32   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     677  \n",
       "33   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      56  \n",
       "34   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "35   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "36   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      38  \n",
       "37   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "38    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      16  \n",
       "39   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "40   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     378  \n",
       "41   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      77  \n",
       "42   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     121  \n",
       "43   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     111  \n",
       "44   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     395  \n",
       "45   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      70  \n",
       "46   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "47   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      48  \n",
       "48   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     248  \n",
       "49   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      73  \n",
       "50   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "51   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      65  \n",
       "52   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "53   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     159  \n",
       "54   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "55   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      85  \n",
       "56   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      76  \n",
       "57   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "58   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      99  \n",
       "59   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "60   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     254  \n",
       "61   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "62   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "63   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "64   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     485  \n",
       "65   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     204  \n",
       "66   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      59  \n",
       "67   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "68   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "69   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "70   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "71   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      17  \n",
       "72   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "73   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     116  \n",
       "74   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     253  \n",
       "75   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      70  \n",
       "76   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      27  \n",
       "77   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      67  \n",
       "78   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      98  \n",
       "79   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      52  \n",
       "80   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      97  \n",
       "81   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "82   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     472  \n",
       "83   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      99  \n",
       "84   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      74  \n",
       "85   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      34  \n",
       "86   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "87   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "88   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      60  \n",
       "89   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      44  \n",
       "90   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "91   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      62  \n",
       "92   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "93   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     196  \n",
       "94   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     175  \n",
       "95   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "96   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     756  \n",
       "97   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      73  \n",
       "98   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "99   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "100  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      64  \n",
       "101  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      56  \n",
       "102  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     193  \n",
       "103  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     141  \n",
       "104  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      32  \n",
       "105  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      68  \n",
       "106  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      46  \n",
       "107  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      69  \n",
       "108  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     141  \n",
       "109  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     630  \n",
       "110  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     433  \n",
       "111  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     687  \n",
       "112  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     323  \n",
       "113  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      17  \n",
       "114  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "115  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      38  \n",
       "116  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "117  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      70  \n",
       "118  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     278  \n",
       "119  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      27  \n",
       "120  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     113  \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      58  \n",
       "122  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      98  \n",
       "123  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      53  \n",
       "124  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "125  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "126  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      32  \n",
       "127  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      47  \n",
       "128  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      52  \n",
       "129  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "130  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "131  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      78  \n",
       "132  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "133  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "134  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "135  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      27  \n",
       "136  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     365  \n",
       "137      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "138  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      58  \n",
       "139  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      74  \n",
       "140  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      44  \n",
       "141  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "142  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "143  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "144  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     138  \n",
       "145  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     137  \n",
       "146  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "147  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      55  \n",
       "148  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      54  \n",
       "149  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "150  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      65  \n",
       "151  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      34  \n",
       "152  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     102  \n",
       "153  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      66  \n",
       "154  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      56  \n",
       "155  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     100  \n",
       "156  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     142  \n",
       "157  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      78  \n",
       "158  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "159  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "160  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "161  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     447  \n",
       "162  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     537  \n",
       "163  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     431  \n",
       "164  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     376  \n",
       "165  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     167  \n",
       "166  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      73  \n",
       "167  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      79  \n",
       "168  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      58  \n",
       "169  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "170  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      49  \n",
       "171  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      44  \n",
       "172      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "173  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "174  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      46  \n",
       "175  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      51  \n",
       "176  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     409  \n",
       "177  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     171  \n",
       "178  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "179  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      27  \n",
       "180  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "181  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      47  \n",
       "182  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      54  \n",
       "183  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      47  \n",
       "184  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      40  \n",
       "185  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     122  \n",
       "186  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      55  \n",
       "187  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      80  \n",
       "188  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "189  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     137  \n",
       "190  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "191  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "192  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     521  \n",
       "193  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      42  \n",
       "194  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      83  \n",
       "195  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     105  \n",
       "196  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "197  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     126  \n",
       "198  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      23  \n",
       "199  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     293  \n",
       "200  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "201  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "202  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     134  \n",
       "203  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     500  \n",
       "204  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     714  \n",
       "205  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     584  \n",
       "206  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      69  \n",
       "207  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "208  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      41  \n",
       "209  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      64  \n",
       "210  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "211  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      43  \n",
       "212  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      47  \n",
       "213  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      95  \n",
       "214  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      59  \n",
       "215  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "216  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      53  \n",
       "217  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      53  \n",
       "218  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      78  \n",
       "219      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "220  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      67  \n",
       "221  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      48  \n",
       "222  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      71  \n",
       "223  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      46  \n",
       "224  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      44  \n",
       "225  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     771  \n",
       "226  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      42  \n",
       "227  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      40  \n",
       "228  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     260  \n",
       "229  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      64  \n",
       "230  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     108  \n",
       "231  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      46  \n",
       "232  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     181  \n",
       "233  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     115  \n",
       "234  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      34  \n",
       "235  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      45  \n",
       "236  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      38  \n",
       "237  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      65  \n",
       "238  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "239  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     138  \n",
       "240  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     152  \n",
       "241  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     699  \n",
       "242  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     547  \n",
       "243  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     100  \n",
       "244  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "245  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     465  \n",
       "246  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     116  \n",
       "247  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     172  \n",
       "248  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     169  \n",
       "249  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     161  \n",
       "250  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "252  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      87  \n",
       "253  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      83  \n",
       "254  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      61  \n",
       "255  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      18  \n",
       "256  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      64  \n",
       "257  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "258  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     149  \n",
       "259  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "260  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      94  \n",
       "261  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     114  \n",
       "262  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      48  \n",
       "263         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      14  \n",
       "264  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      79  \n",
       "265  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      19  \n",
       "266  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      76  \n",
       "267  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      94  \n",
       "268  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     231  \n",
       "269  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     733  \n",
       "270  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     827  \n",
       "271  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     597  \n",
       "272  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     515  \n",
       "273  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     449  \n",
       "274  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     508  \n",
       "275  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "276  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     131  \n",
       "277  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     674  \n",
       "278  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      22  \n",
       "279  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     143  \n",
       "280  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      43  \n",
       "281  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      41  \n",
       "282  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "283  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "284  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      41  \n",
       "285  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "286  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     204  \n",
       "287  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      27  \n",
       "288  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      40  \n",
       "289  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "290  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      44  \n",
       "291  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "292  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "293  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      29  \n",
       "294  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      36  \n",
       "295  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "296  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "297  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      85  \n",
       "298  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      40  \n",
       "299  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      76  \n",
       "300  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      58  \n",
       "301  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "302  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "303  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      39  \n",
       "304  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      33  \n",
       "305  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     113  \n",
       "306  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      25  \n",
       "307  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      31  \n",
       "308  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      36  \n",
       "309  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     104  \n",
       "310  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      25  \n",
       "311  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      32  \n",
       "312  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "313  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "314  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     159  \n",
       "315  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      38  \n",
       "316  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "317  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      42  \n",
       "318  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      35  \n",
       "319  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      37  \n",
       "320  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      34  \n",
       "321  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      41  \n",
       "322  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      59  \n",
       "323  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      26  \n",
       "324  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      36  \n",
       "325  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "326  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     177  \n",
       "327  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      28  \n",
       "328  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      36  \n",
       "329  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "330  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      45  \n",
       "331  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      50  \n",
       "332  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "333      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "334  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      42  \n",
       "335   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      16  \n",
       "336  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      17  \n",
       "337               [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      12  \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     141  \n",
       "339  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     785  \n",
       "340  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     580  \n",
       "341  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "342  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "343  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      36  \n",
       "344  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "345  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      42  \n",
       "346      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      15  \n",
       "347  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      24  \n",
       "348  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      45  \n",
       "349  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      21  \n",
       "350  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      55  \n",
       "351  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     596  \n",
       "352  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     390  \n",
       "353  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     101  \n",
       "354  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      63  \n",
       "355  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     103  \n",
       "356  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     117  \n",
       "357  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     121  \n",
       "358  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      20  \n",
       "359  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      92  \n",
       "360  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     100  \n",
       "361  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      30  \n",
       "362  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      62  \n",
       "363  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      55  \n",
       "364  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     342  \n",
       "365  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     487  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(final_texts)\n",
    "df=pd.DataFrame(dict(tokens))\n",
    "#df['input_ids'].iloc[0]\n",
    "df['Length']=df['input_ids'].apply(len)\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Print the DataFrame to a CSV file\n",
    "df.to_csv('AdjacentSequenceClustering.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
