{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2f8212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:49.294037Z",
     "start_time": "2023-04-08T08:19:47.533476Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "       \n",
       "    \n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"NLP with Transformers\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "           \n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "    \n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "   \n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "    \n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "<center><img src=\"https://d4x5p7s4.rocketcdn.me/wp-content/uploads/2016/03/logo-poster-smaller.png\"/> </center>\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85541636",
   "metadata": {},
   "source": [
    "## Pretrained tokenizers\n",
    "\n",
    "\n",
    "\n",
    "In this lab, we will delve into the tokenizers. A tokenizer transforms a piece of text into tokens so that they can form an input to the subsequent tasks such as a transformer that will use the tokens for a classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70557460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:50.221375Z",
     "start_time": "2023-04-08T08:19:50.217991Z"
    }
   },
   "outputs": [],
   "source": [
    "example = \"The tokenizer does tokenization. It does this to have fun with tokens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e13e249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:50.576624Z",
     "start_time": "2023-04-08T08:19:50.405119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8c7e89613e47f289091249ab5404ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea1d3ee6acc40b9ac593ac7e9353697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1dd91031c44e44b218a1820f574431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0efe8f872cf4987bc67d12d38b7ad10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ba4d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:50.646698Z",
     "start_time": "2023-04-08T08:19:50.638803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 19204, 17629, 2515, 19204, 3989, 1012, 2009, 2515, 2023, 2000, 2031, 4569, 2007, 19204, 2015, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d995465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:50.859268Z",
     "start_time": "2023-04-08T08:19:50.848604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17629</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2515</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2031</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4569</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input_ids  token_type_ids  attention_mask\n",
       "0         101               0               1\n",
       "1        1996               0               1\n",
       "2       19204               0               1\n",
       "3       17629               0               1\n",
       "4        2515               0               1\n",
       "5       19204               0               1\n",
       "6        3989               0               1\n",
       "7        1012               0               1\n",
       "8        2009               0               1\n",
       "9        2515               0               1\n",
       "10       2023               0               1\n",
       "11       2000               0               1\n",
       "12       2031               0               1\n",
       "13       4569               0               1\n",
       "14       2007               0               1\n",
       "15      19204               0               1\n",
       "16       2015               0               1\n",
       "17       1012               0               1\n",
       "18        102               0               1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36e7f8",
   "metadata": {},
   "source": [
    " There are some interesting observations: first, the `example` string has on 12 words and two puntuation (end of sentence periods). But the tokenizer has broken it into 19 tokens! Can you guess why?\n",
    "\n",
    "Let us investigate it by reversing the tokenization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea5f3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:51.669551Z",
     "start_time": "2023-04-08T08:19:51.665515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'token', '##izer', 'does', 'token', '##ization', '.', 'it', 'does', 'this', 'to', 'have', 'fun', 'with', 'token', '##s', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "recovered_tokens = tokenizer.convert_ids_to_tokens(tokens.input_ids)\n",
    "print(recovered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294beea6",
   "metadata": {},
   "source": [
    "Intuitively, we would have expected the tokens to be the words and punctuations. In other words, we would have exptected something like:\n",
    "\n",
    "`['The', 'tokenizer', 'does', 'tokenization', '.',  'It', 'does', 'this', 'to', 'have', 'fun', 'with', 'tokens', '.']`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79177b6",
   "metadata": {},
   "source": [
    "The tokenizer has converted everything to lower case! It turns out that the tokenizer `bert-based-uncased` does that as a pre-processing step. \n",
    "\n",
    "#### Special tokens\n",
    "\n",
    "Next, we observe that there is a `[CLS]` token at the beginning, and a `[SEP]` token at the end. The `[CLS]` is a special token, that `BERT` transformer always prepends to the beginning of a segment (here the sentence). And `BERT` appends each segment with `[SEP]`, to separate it from the next segment.\n",
    "\n",
    "Recall that we covered this while carefully studying the BERT architecture in a previous session.\n",
    "\n",
    "`BERT` expects two segments, but we are passing only one segment. Each token is tagged with a segment identifier. In this case, the `tokens.token_type_ids` field captures the segment identifier. Since the whole sentence form a segment, all the tokens belong to the first (and only) segment. So the `tokens.token_type_ids` is `0`.\n",
    "\n",
    "### Word segments\n",
    "\n",
    "The english language has a rather vast vocabulary. If we were to consider such a large vocabulary, say a vocabulary of 1-million words, then we would have a problem:\n",
    "\n",
    "Recall that categorical variables have to be hot-encoded into a hot-encoding vector, and the dimensionality of the vector is equal to the cardinality of the classes in the categorical variable. Now, for us, the categorical variable here is the token, and if it takes a value in a large vocabulary, then it will get hot-encoded into a very large vector. \n",
    "\n",
    "Very large vectors are inefficient to train; they need far more computational power, and also more data.\n",
    "\n",
    "So most tokenizers take a much smaller vocabulary, say of 20K tokens. But these tokens do not always correspond to the words. Look carefully at the `example` text:\n",
    "\n",
    ">The tokenizer does tokenization. It does this to have fun with tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3280ba",
   "metadata": {},
   "source": [
    "Do we see something interestings? Some of the words are composites: they are made of `token` and some other word-piece. For example, `tokenizer` is `token` + `##izer`. The suffixing word-pieces start with the special characters `##` signifying that they complete the previous token. But we realize that there are lots of words in the vocabulary that the end with `izer`. Also, suffixing many words with an `s` makes them the plural of the original word. \n",
    "\n",
    "\n",
    "Some examples are:\n",
    "\n",
    ">Finalizer\n",
    "Finalizers\n",
    "Visualizer\n",
    "Visualizers\n",
    "Equalizer\n",
    "Equalizers\n",
    "Humanizer\n",
    "Humanizers\n",
    "Harmonizer\n",
    "Harmonizers\n",
    "Patronizer\n",
    "Patronizers\n",
    "Vaporizer\n",
    "Vaporizers\n",
    "\n",
    "\n",
    "We have deliberately chosen words that end with `izer`, and that themselves do not look like composites with the `izer`.\n",
    "\n",
    "Let us see what happens if we pass these words through the tokenizer, and then inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187f64bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:53.576837Z",
     "start_time": "2023-04-08T08:19:53.572773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'final', '##izer', 'final', '##izer', '##s', 'visual', '##izer', 'visual', '##izer', '##s', 'equal', '##izer', 'equal', '##izer', '##s', 'human', '##izer', 'human', '##izer', '##s', 'harmon', '##izer', 'harmon', '##izer', '##s', 'patron', '##izer', 'patron', '##izer', '##s', 'vapor', '##izer', 'vapor', '##izer', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "izers = \"\"\"\n",
    "Finalizer\n",
    "Finalizers\n",
    "Visualizer\n",
    "Visualizers\n",
    "Equalizer\n",
    "Equalizers\n",
    "Humanizer\n",
    "Humanizers\n",
    "Harmonizer\n",
    "Harmonizers\n",
    "Patronizer\n",
    "Patronizers\n",
    "Vaporizer\n",
    "Vaporizers\"\"\"\n",
    "izer_tokens = tokenizer(izers)\n",
    "izer_recovered_tokens = tokenizer.convert_ids_to_tokens(izer_tokens.input_ids)\n",
    "print(izer_recovered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a80401",
   "metadata": {},
   "source": [
    "Notice how a word like `Visualizers` gets broken into word-segments: `visual`, `##izer` and `##s`.\n",
    "\n",
    "\n",
    "This gives us an interesting insight: **one way to abbreviate the language vocabulary is to use these word-segments as the building blocks of the vocabulary, and not the words themselves.** As an added benefit, the number of unknown words get reduced, since they can now be split into smaller word-segments that are part of the vocabulary.\n",
    "\n",
    "There are agglutinative languages such as Sanskrit, Finish, Turkish, Japanese which tend to have a lot of joining of words, so that a word can be arbitrarily long, composed of many sub-words. In all such cases, the advantage of using a smaller vocabulary of subwords comes in very handy.\n",
    "\n",
    "There are many such approaches to tokenize sentences. Some popular ones are:\n",
    "\n",
    "* **WordPiece**, as shown above, that BERT tokenizers uses\n",
    "* **SentencePiece** or **Unigram**, that multilingual models often use\n",
    "* **Byte-level BPE**, that GPT-2 uses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19133aee",
   "metadata": {},
   "source": [
    "The `bert-base-uncased` tokenizer does exactly this. There are other tokenizers, with small variations in how they tokenize, but they all essentially follow similar ideas.\n",
    "\n",
    "#### Attention mask\n",
    "\n",
    "Now, consider what happens if we give the tokenizer a batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c165367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:23:33.115407Z",
     "start_time": "2023-04-08T08:23:33.103841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2000, 9413, 2099, 2003, 2529, 1010, 2000...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2000, 4553, 2003, 2000, 2444, 1012, 102,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 2000, 9413, 2099, 2003, 2529, 1010, 2000...   \n",
       "1  [101, 2000, 4553, 2003, 2000, 2444, 1012, 102,...   \n",
       "\n",
       "                      token_type_ids                     attention_mask  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "err = \"To err is human, to forgive divine\"\n",
    "learn = \"To learn is to live.\"\n",
    "sentences = [ err, learn]\n",
    "\n",
    "tokens = tokenizer(sentences, padding=True)\n",
    "pd.DataFrame(dict(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9eaf3",
   "metadata": {},
   "source": [
    "Notice that the first sentence is longer than the second sentence by three tokens. So the attention mask of the second sentence is padded with three zeros. If we do not give `padding=True`, then the attention mask will be a shorter list of `1`s.\n",
    "\n",
    "This explains the purpose of the attention mask -- it tells the downstream models which tokens to ignore or not focus attention on, since they are simply padding tokens.\n",
    "\n",
    "We can see this more explicitly below. Notice the presence of the special token `[PAD]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6386f33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:46:54.221686Z",
     "start_time": "2023-04-08T08:46:54.218043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'to', 'er', '##r', 'is', 'human', ',', 'to', 'forgive', 'divine', '[SEP]']\n",
      "['[CLS]', 'to', 'learn', 'is', 'to', 'live', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ids in tokens.input_ids:\n",
    "    recovered = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print (recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3168ba",
   "metadata": {},
   "source": [
    "#### Unknown words\n",
    "\n",
    "As we mentioned, the english vocabulary is vast. But the tokenizers tend to work with a much smaller vocabulary, and any word they do not recognize, they assign it a special `token id`, `[UNK]`. With the use of `WordPiece` tokenizer in `BERT`, these show up far less often, since the unknown words are often broken down into known sub-word pieces. Let's see this below, where `brillig` is a nonsensical word from the jabberwocky poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2f29a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:07:56.262721Z",
     "start_time": "2023-04-08T09:07:56.258075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2009, 2001, 7987, 8591, 8004, 1998, 1996, 18036, 10536, 2000, 6961, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jabberwocky = 'It was brillig and the slithy toves'\n",
    "tokens = tokenizer(jabberwocky)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11212dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:07:58.159109Z",
     "start_time": "2023-04-08T09:07:58.155412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'it', 'was', 'br', '##ill', '##ig', 'and', 'the', 'slit', '##hy', 'to', '##ves', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "recovered_tokens = tokenizer.convert_ids_to_tokens(tokens.input_ids)\n",
    "print(recovered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f93bb",
   "metadata": {},
   "source": [
    "### DECODING\n",
    "\n",
    "Decoding is the restoration of the text back from the token identifiers. Let us look into this, with the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6a8a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:09:19.693463Z",
     "start_time": "2023-04-08T09:09:18.188054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] it was brillig and the slithy toves [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab9e60",
   "metadata": {},
   "source": [
    "Observe, however, that the words are in lower case. Also, there is the presence of the `[CLS]` and `[SEP]` tokens.\n",
    "\n",
    "We could have avoided this, with a slightly different api, calling the `tokenize()` method of the `tokenizer`. In this case, it does not pad it with `[CLS]` and `[SEP]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86898c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:14:50.431322Z",
     "start_time": "2023-04-08T09:14:50.427754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'br', '##ill', '##ig', 'and', 'the', 'slit', '##hy', 'to', '##ves']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(jabberwocky)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401babb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:13:36.556552Z",
     "start_time": "2023-04-08T09:13:36.538635Z"
    }
   },
   "source": [
    "To explicitly get the input tokens ids, we need to call:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e19f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:15:08.446501Z",
     "start_time": "2023-04-08T09:15:08.441657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009, 2001, 7987, 8591, 8004, 1998, 1996, 18036, 10536, 2000, 6961]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dc5c7",
   "metadata": {},
   "source": [
    "And finally, we can decode it back the usual way; note the absence of the special tokens in the result, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6231019d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:15:56.607110Z",
     "start_time": "2023-04-08T09:15:56.602847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was brillig and the slithy toves'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf6935",
   "metadata": {},
   "source": [
    "### Explicitly calling a tokenizer class\n",
    "\n",
    "An alternate syntax is to use the explicit class `BertTokenizer` to load the relevant, pretrained tokenizer checkpoint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3303ee50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T09:20:28.227792Z",
     "start_time": "2023-04-08T09:20:28.025454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad1c63",
   "metadata": {},
   "source": [
    "### Sentence-Piece tokenizer\n",
    "\n",
    "Let us now see how a `SentencePiece` tokenizer would tokenize. We will use XLM-R (`xlm-roberta-base`) for this below. Notice the small variations in the output format. The segment start is given by: `<s>\n",
    "`, and the end by `</s>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f82746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T11:09:44.214154Z",
     "start_time": "2023-04-08T11:09:43.122969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 581, 47, 1098, 52825, 14602, 47, 1098, 47691, 5, 1650, 14602, 903, 47, 765, 7477, 678, 47, 84694, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "tokens = tokenizer(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06462c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T11:09:57.871382Z",
     "start_time": "2023-04-08T11:09:57.867546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁The', '▁to', 'ken', 'izer', '▁does', '▁to', 'ken', 'ization', '.', '▁It', '▁does', '▁this', '▁to', '▁have', '▁fun', '▁with', '▁to', 'kens', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf9455",
   "metadata": {},
   "source": [
    "### Anatomy of a tokenizer\n",
    "\n",
    "A tokenizer internally is a **pipeline** of four logical operations; these are depicted in the figure below.\n",
    "\n",
    "<img src=\"images/tokenizer-pipeline.png\" />"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
